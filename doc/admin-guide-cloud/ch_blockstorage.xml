<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="managing-volumes">
    <?dbhtml stop-chunking?>
    <title>Block Storage</title>
    <para>
        OpenStack Block Storage allows you to add block-level storage to your
        OpenStack Compute instances. It is similar in function to the Amazon
        EC2 Elastic Block Storage (EBS) offering.
    </para>
    <para>
        The OpenStack Block Storage service uses a series of daemon processes.
        Each process will have the prefix
        <systemitem class="service">cinder-</systemitem>, and they will all
        run persistently on the host. The binaries can all be run on from a 
        single node, or they can be spread across multiple nodes. They can 
        also be run on the same node as other OpenStack services.
    </para>
    <para>
        The default OpenStack Block Storage service implementation is an iSCSI
        solution that uses Logical Volume Manager (LVM) for Linux. It also
        provides drivers that allow you to use a back-end storage device from
        a different vendor, in addition to or instead of the base LVM 
        implementation.
    </para>
    <note>
        <para>
            The OpenStack Block Storage service is not a shared storage
            solution like a Storage Area Network (SAN) of NFS volumes, where
            you can attach a volume to multiple servers. With the OpenStack
            Block Storage service, you can attach a volume to only one 
            instance at a time.
        </para>
    </note>
    <para>
        This chapter uses a simple example to demonstrate Block Storage. In 
        this example, one cloud controller runs the
        <systemitem class="service">nova-api</systemitem>,
        <systemitem class="service">nova-scheduler</systemitem>,
        <systemitem class="service">nova-objectstore</systemitem>,
        <literal>nova-network</literal> and <literal>cinder-*</literal>
        services. There are two additional compute nodes running
        <systemitem class="service">nova-compute</systemitem>.
    </para>
    <para>
        The example in this chapter uses a custom partitioning scheme that
        uses 60GB of space and labels it as a Logical Volume (LV). The network
        uses <literal>FlatManager</literal> as the
        <literal>NetworkManager</literal> setting for OpenStack Compute.
    </para>
    <para>
        The network mode does not interfere with the way Block Storage works,
        but networking must be set up. For more information on networking, see
        <xref linkend="ch_networking"/>.
    </para>

    <section xml:id="section_manage-volumes">
    <title>Manage volumes</title>
    <para>
        To set up Compute to use volumes, you must have
        <systemitem class="service">lvm2</systemitem> installed.
    </para>
        <para>
            This procedure creates and attaches a volume to a server instance.
        </para>
        <procedure>
            <step>
                <para>
                    The <command>cinder create</command> command creates a
                    logical volume (LV) in the volume group (VG)
                    <parameter>cinder-volumes</parameter>.
                </para>
                <para>
                    Create a volume:
                </para>
                <programlisting>$ cinder create</programlisting>
            </step>
            <step>
                <para>
                    The <command>nova volume-attach</command> command creates
                    a unique iSCSI IQN that is exposed to the compute node.
                </para>
                <para>
                    Attach the LV to an instance:
                </para>
                <programlisting>$ nova volume-attach</programlisting>
            </step>
            <step>
                <para>
                    The OpenStack Block Storage service and OpenStack Compute
                    can now be configured using the
                    <filename>cinder.conf</filename> configuration file.
                </para>
            </step>
        </procedure>
            <para>
                The compute node, which runs the instance, now has an active
                iSCSI session. It will also have a new local disk, usually a
                <filename>/dev/sdX</filename> disk. This local storage is
                used by libvirt as storage for the instance. The instance
                itself will usually get a separate new disk, usually a
                <filename>/dev/vdX</filename> disk.
            </para>
            <para>
                In some cases, instances can be stored and run from within
                volumes. For more information, see the 
                <link xlink:href="http://docs.openstack.org/user-guide/content/boot_from_volume.html">
                Launch an instance from a volume</link> section in the
                <link xlink:href="http://docs.openstack.org/user-guide/content/">
                <citetitle>OpenStack End User Guide</citetitle></link>.
            </para>
    <xi:include href="section_multi_backend.xml"/>
    <xi:include href="section_backup-block-storage-disks.xml"/>
    <xi:include href="section_volume-migration.xml"/>
    </section>
    <section xml:id="troubleshooting-cinder-install">
       <title>Troubleshoot your installation</title>
       <para>
            This section contains troubleshooting information for Block 
            Storage.
        </para>
        <xi:include href="section_ts_cinder_config.xml"/>
        <xi:include href="section_ts_multipath_warn.xml"/>
        <xi:include href="section_ts_vol_attach_miss_sg_scan.xml"/>
        <xi:include href="section_ts_HTTP_bad_req_in_cinder_vol_log.xml"/>
        <xi:include href="section_ts_attach_vol_fail_not_JSON.xml"/>
        <xi:include href="section_ts_duplicate_3par_host.xml"/>
        <xi:include href="section_ts_failed_attach_vol_after_detach.xml"/>
        <xi:include href="section_ts_failed_attach_vol_no_sysfsutils.xml"/>
        <xi:include href="section_ts_failed_connect_vol_FC_SAN.xml"/>
        <xi:include href="section_ts_failed_sched_create_vol.xml"/>
        <xi:include href="section_ts_no_emulator_x86_64.xml"/>
        <xi:include href="section_ts_non_existent_host.xml"/>
        <xi:include href="section_ts_non_existent_vlun.xml"/>
    </section>
</chapter>
