<?xml version="1.0" encoding="UTF-8"?>
<section xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  version="5.0"
  xml:id="technical-considerations-network-focus">
    <?dbhtml stop-chunking?>
    <title>Technical Considerations</title>
    <para>Designing an OpenStack network architecture involves a
        combination of layer 2 and layer 3 considerations. Layer 2
        decisions involve those made at the data-link layer, such as
        the decision to use Ethernet versus Token Ring. Layer 3
        involve those made about the protocol layer and the point at
        which IP comes into the picture. As an example, a completely
        internal OpenStack network can exist at layer 2 and ignore
        layer 3 however, in order for any traffic to go outside of
        that cloud, to another network, or to the Internet, a layer 3
        router or switch must be involved.</para>
    <para>The past few years have seen two competing trends in
        networking. There has been a trend towards building data
        center network architectures based on layer 2 networking and
        simultaneously another network architecture approach is to
        treat the cloud environment essentially as a miniature version
        of the Internet. This represents a radically different
        approach to the network architecture from what is currently
        installed in the staging environment because the Internet is
        based entirely on layer 3 routing rather than layer 2
        switching.</para>
    <para>In the data center context, there are advantages of
        designing the network on layer 2 protocols rather than layer
        3. In spite of the difficulties of using a bridge to perform
        the network role of a router, many vendors, customers, and
        service providers are attracted to the idea of using Ethernet
        in as many parts of their networks as possible. The benefits
        of selecting a layer 2 design are:</para>
    <itemizedlist>
        <listitem>
            <para>Ethernet frames contain all the essentials for
                networking. These include, but are not limited to,
                globally unique source addresses, globally unique
                destination addresses, and error control.</para>
        </listitem>
        <listitem>
            <para>Ethernet frames can carry any kind of packet.
                Networking at layer 2 is independent of the layer 3
                protocol.</para>
        </listitem>
        <listitem>
            <para>More layers added to the Ethernet frame only slow
                the networking process down. This is known as 'nodal
                processing delay'.</para>
        </listitem>
        <listitem>
            <para>Adjunct networking features, for example class of
                service (CoS) or multicasting, can be added to
                Ethernet as readily as IP networks.</para>
        </listitem>
        <listitem>
            <para>VLANs are an easy mechanism for isolating
                networks.</para>
        </listitem>
    </itemizedlist>
    <para>Most information starts and ends inside Ethernet frames.
        Today this applies to data, voice (for example, VoIP) and
        video (for example, web cameras). The concept is that, if more
        of the end-to-end transfer of information from a source to a
        destination can be done in the form of Ethernet frames, more
        of the benefits of Ethernet can be realized on the network.
        Though it is not a substitute for IP networking, networking at
        layer 2 can be a powerful adjunct to IP networking.</para>
    <para>The basic reasoning behind using layer 2 Ethernet over layer
        3 IP networks is the speed, the reduced overhead of the IP
        hierarchy, and the lack of requirement to keep track of IP
        address configuration as systems are moved around. Whereas the
        simplicity of layer 2 protocols might work well in a data
        center with hundreds of physical machines, cloud data centers
        have the additional burden of needing to keep track of all
        virtual machine addresses and networks. In these data centers,
        it is not uncommon for one physical node to support 30-40
        instances.</para>
    <para>Important Note: Networking at the frame level says nothing
        about the presence or absence of IP addresses at the packet
        level. Almost all ports, links, and devices on a network of
        LAN switches still have IP addresses, as do all the source and
        destination hosts. There are many reasons for the continued
        need for IP addressing. The largest one is the need to manage
        the network. A device or link without an IP address is usually
        invisible to most management applications. Utilities including
        remote access for diagnostics, file transfer of configurations
        and software, and similar applications cannot run without IP
        addresses as well as MAC addresses.</para>
    <section xml:id="layer-2-arch-limitations"><title>Layer 2 Architecture Limitations</title>
    <para>Outside of the traditional data center the limitations of
        layer 2 network architectures become more obvious.</para>
    <itemizedlist>
        <listitem>
            <para>Number of VLANs is limited to 4096.</para>
        </listitem>
        <listitem>
            <para>The number of MACs stored in switch tables is
                limited.</para>
        </listitem>
        <listitem>
            <para>The need to maintain a set of layer 4 devices to
                handle traffic control must be accommodated.</para>
        </listitem>
        <listitem>
            <para>MLAG, often used for switch redundancy, is a
                proprietary solution that does not scale beyond two
                devices and forces vendor lock-in.</para>
        </listitem>
        <listitem>
            <para>It can be difficult to troubleshoot a network
                without IP addresses and ICMP.</para>
        </listitem>
        <listitem>
            <para>Configuring ARP is considered complicated on large
                layer 2 networks.</para>
        </listitem>
        <listitem>
            <para>All network devices need to be aware of all MACs,
                even instance MACs, so there is constant churn in MAC
                tables and network state changes as instances are
                started or stopped.</para>
        </listitem>
        <listitem>
            <para>Migrating MACs (instance migration) to different
                physical locations are a potential problem if ARP
                table timeouts are not set properly.</para>
        </listitem>
    </itemizedlist>
    <para>It is important to know that layer 2 has a very limited set
        of network management tools. It is very difficult to control
        traffic, as it does not have mechanisms to manage the network
        or shape the traffic, and network troubleshooting is very
        difficult. One reason for this difficulty is network devices
        have no IP addresses. As a result, there is no reasonable way
        to check network delay in a layer 2 network.</para>
    <para>On large layer 2 networks, configuring ARP learning can also
        be complicated. The setting for the MAC address timer on
        switches is critical and, if set incorrectly, can cause
        significant performance problems. As an example, the Cisco
        default MAC address timer is extremely long. Migrating MACs to
        different physical locations to support instance migration can
        be a significant problem. In this case, the network
        information maintained in the switches could be out of sync
        with the new location of the instance.</para>
    <para>In a layer 2 network, all devices are aware of all MACs,
        even those that belong to instances. The network state
        information in the backbone changes whenever an instance is
        started or stopped. As a result there is far too much churn in
        the MAC tables on the backbone switches.</para></section>
    <section xml:id="layer-3-arch-advantages"><title>Layer 3 Architecture Advantages</title>
    <para>In the layer 3 case, there is no churn in the routing tables
        due to instances starting and stopping. The only time there
        would be a routing state change would be in the case of a Top
        of Rack (ToR) switch failure or a link failure in the backbone
        itself. Other advantages of using a layer 3 architecture
        include:</para>
    <itemizedlist>
        <listitem>
            <para>layer 3 networks provide the same level of
                resiliency and scalability as the Internet.</para>
        </listitem>
        <listitem>
            <para>Controlling traffic with routing metrics is
                straightforward.</para>
        </listitem>
        <listitem>
            <para>layer 3 can be configured to use BGP confederation
                for scalability so core routers have state
                proportional to number of racks, not to the number of
                servers or instances.</para>
        </listitem>
        <listitem>
            <para>Routing ensures that instance MAC and IP addresses
                out of the network core reducing state churn. Routing
                state changes only occur in the case of a ToR switch
                failure or backbone link failure.</para>
        </listitem>
        <listitem>
            <para>There are a variety of well tested tools, for
                example ICMP, to monitor and manage traffic.</para>
        </listitem>
        <listitem>
            <para>layer 3 architectures allow for the use of Quality
                of Service (QoS) to manage network performance.</para>
        </listitem>
    </itemizedlist>
    <section xml:id="layer-3-arch-limitations"><title>Layer 3 Architecture Limitations</title>
    <para>The main limitation of layer 3 is that there is no built-in
        isolation mechanism comparable to the VLANs in layer 2
        networks. Furthermore, the hierarchical nature of IP addresses
        means that an instance will also be on the same subnet as its
        physical host. This means that it cannot be migrated outside
        of the subnet easily. For these reasons, network
        virtualization needs to use IP encapsulation and software at
        the end hosts for both isolation, as well as for separation of
        the addressing in the virtual layer from addressing in the
        physical layer. Other potential disadvantages of layer 3
        include the need to design an IP addressing scheme rather than
        relying on the switches to automatically keep track of the MAC
        addresses and to configure the interior gateway routing
        protocol in the switches.</para></section></section>
    <section xml:id="network-recommendations-overview">
        <title>Network Recommendations Overview</title>
    <para>OpenStack has complex networking requirements for several
        reasons. Many components interact at different levels of the
        system stack that adds complexity. Data flows are complex.
        Data in an OpenStack cloud moves both between instances across
        the network (also known as East-West), as well as in and out
        of the system (also known as North-South). Physical server
        nodes have network requirements that are independent of those
        used by instances which need to be isolated from the core
        network to account for scalability. It is also recommended to
        functionally separate the networks for security purposes and
        tune performance through traffic shaping.</para>
    <para>A number of important general technical and business factors
        need to be taken into consideration when planning and
        designing an OpenStack network. They include:</para>
    <itemizedlist>
        <listitem>
            <para>A requirement for vendor independence. To avoid
                hardware or software vendor lock-in, the design should
                not rely on specific features of a vendor’s router or
                switch.</para>
        </listitem>
        <listitem>
            <para>A requirement to massively scale the ecosystem to
                support millions of end users.</para>
        </listitem>
        <listitem>
            <para>A requirement to support indeterminate platforms and
                applications.</para>
        </listitem>
        <listitem>
            <para>A requirement to design for cost efficient
                operations to take advantage of massive scale.</para>
        </listitem>
        <listitem>
            <para>A requirement to ensure that there is no single
                point of failure in the cloud ecosystem.</para>
        </listitem>
        <listitem>
            <para>A requirement for high availability architecture to
                meet customer SLA requirements.</para>
        </listitem>
        <listitem>
            <para>A requirement to be tolerant of rack level
                failure.</para>
        </listitem>
        <listitem>
            <para>A requirement to maximize flexibility to architect
                future production environments.</para>
        </listitem>
    </itemizedlist>
    <para>Keeping all of these in mind, the following network design
        recommendations can be made:</para>
    <itemizedlist>
        <listitem>
            <para>Layer 3 designs are preferred over layer 2
                architectures.</para>
        </listitem>
        <listitem>
            <para>Design a dense multi-path network core to support
                multi-directional scaling and flexibility.</para>
        </listitem>
        <listitem>
            <para>Use hierarchical addressing because it is the only
                viable option to scale network ecosystem.</para>
        </listitem>
        <listitem>
            <para>Use virtual networking to isolate instance service
                network traffic from the management and internal
                network traffic.</para>
        </listitem>
        <listitem>
            <para>Isolate virtual networks using encapsulation
                technologies.</para>
        </listitem>
        <listitem>
            <para>Use traffic shaping for performance tuning.</para>
        </listitem>
        <listitem>
            <para>Use eBGP to connect to the Internet up-link.</para>
        </listitem>
        <listitem>
            <para>Use iBGP to flatten the internal traffic on the
                layer 3 mesh.</para>
        </listitem>
        <listitem>
            <para>Determine the most effective configuration for block
                storage network.</para>
        </listitem>
    </itemizedlist></section>
    <section xml:id="additional-considerations-network-focus"><title>Additional Considerations</title>
    <para>There are numerous topics to consider when designing a
        network-focused OpenStack cloud.</para>
    <section xml:id="openstack-networking-versus-nova-network"><title>OpenStack Networking versus Nova Network
        Considerations</title>
    <para>Selecting the type of networking technology to implement
        depends on many factors. OpenStack Networking (Neutron) and
        Nova Network both have their advantages and disadvantages.
        They are both valid and supported options that fit different
        use cases as described in the following table.</para></section>
    <section xml:id="redundant-networking-tor-switch-ha"><title>Redundant Networking: ToR Switch High Availability
        Risk Analysis</title>
    <para>A technical consideration of networking is the idea that
        switching gear in the data center that should be installed
        with backup switches in case of hardware failure.</para>
    <para>Research into the mean time between failures (MTBF) on
        switches is between 100,000 and 200,000 hours. This number is
        dependent on the ambient temperature of the switch in the data
        center. When properly cooled and maintained, this translates
        to between 11 and 22 years before failure. Even in the worst
        case of poor ventilation and high ambient temperatures in the
        data center, the MTBF is still 2-3 years. This is based on
        published research found at
        http://www.garrettcom.com/techsupport/papers/ethernet_switch_reliability.pdf
        and http://www.n-tron.com/pdf/network_availability.pdf</para>
    <para>In most cases, it is much more economical to only use a
        single switch with a small pool of spare switches to replace
        failed units than it is to outfit an entire data center with
        redundant switches. Applications should also be able to
        tolerate rack level outages without affecting normal
        operations since network and compute resources are easily
        provisioned and plentiful.</para></section>
    <section xml:id="preparing-for-future-ipv6-support"><title>Preparing for the future: IPv6 Support</title>
    <para>One of the most important networking topics today is the
        impending exhaustion of IPv4 addresses. In early 2014, ICANN
        announced that they started allocating the final IPv4 address
        blocks to the Regional Internet Registries
        http://www.internetsociety.org/deploy360/blog/2014/05/goodbye-ipv4-iana-starts-allocating-final-address-blocks/.
        This means the IPv4 address space is close to being fully
        allocated. As a result, it will soon become difficult to
        allocate more IPv4 addresses to an application that has
        experienced growth, or is expected to scale out, due to the
        lack of unallocated IPv4 address blocks.</para>
    <para>For network focused applications the future is the IPv6
        protocol. IPv6 increases the address space significantly,
        fixes long standing issues in the IPv4 protocol, and will
        become an essential for network focused applications in the
        future.</para>
    <para>Neutron supports IPv6 when configured to take advantage of
        the feature. To enable it, simply create an IPv6 subnet in
        OpenStack Neutron and use IPv6 prefixes when creating security
        groups.</para></section>
    <section xml:id="asymetric-links"><title>Asymmetric Links</title>
    <para>When designing a network architecture, the traffic patterns
        of an application will heavily influence the allocation of
        total bandwidth and the number of links that are used to send
        and receive traffic. Applications that provide file storage
        for customers will allocate bandwidth and links to favor
        incoming traffic, whereas video streaming applications will
        allocate bandwidth and links to favor outgoing traffic.</para></section>
    <section xml:id="performance-network-focus"><title>Performance</title>
    <para>It is important to analyze the applications' tolerance for
        latency and jitter when designing an environment to support
        network focused applications. Certain applications, for
        example VoIP, are less tolerant of latency and jitter. Where
        latency and jitter are concerned, certain applications may
        require tuning of QoS parameters and network device queues to
        ensure that they are queued for transmit immediately or
        guaranteed minimum bandwidth. Since OpenStack currently does
        not support these functions, some considerations may need to
        be made for the network plug-in selected.</para>
    <para>The location of a service may also impact the application or
        consumer experience. If an application is designed to serve
        differing content to differing users it will need to be
        designed to properly direct connections to those specific
        locations. Use a multi-site installation for these situations,
        where appropriate.</para>
    <para>OpenStack networking can be implemented in two separate
        ways. The legacy nova-network provides a flat DHCP network
        with a single broadcast domain. This implementation does not
        support tenant isolation networks or advanced plug-ins, but it
        is currently the only way to implement a distributed layer 3
        agent using the multi_host configuration. Neutron is the
        official current implementation of OpenStack Networking. It
        provides a pluggable architecture that supports a large
        variety of network methods. Some of these include a layer 2
        only provider network model, external device plug-ins, or even
        OpenFlow controllers.</para>
    <para>Networking at large scales becomes a set of boundary
        questions. The determination of how large a layer 2 domain
        needs to be is based on the amount of nodes within the domain
        and the amount of broadcast traffic that passes between
        instances. Breaking layer 2 boundaries may require the
        implementation of overlay networks and tunnels. This decision
        is a balancing act between the need for a smaller overhead or
        a need for a smaller domain.</para>
    <para>When selecting network devices, be aware that making this
        decision based on largest port density often comes with a
        drawback. Aggregation switches and routers have not all kept
        pace with Top of Rack switches and may induce bottlenecks on
        north-south traffic. As a result, it may be possible for
        massive amounts of downstream network utilization to impact
        upstream network devices, impacting service to the cloud.
        Since OpenStack does not currently provide a mechanism for
        traffic shaping or rate limiting, it is necessary to implement
        these features at the network hardware level.</para></section></section>
</section>