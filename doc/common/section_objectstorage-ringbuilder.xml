<?xml version="1.0" encoding="utf-8"?>
<section xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    version="5.0"
    xml:id="section_objectstorage-ringbuilder">
    <!-- ... Old module003-ch005-the-ring edited, renamed, and stored in doc/common for use by both Cloud Admin and Operator Training Guides... -->
    <title>Ring-builder</title>
    <para>Rings are built and managed manually by a utility called the ring-builder. The
        ring-builder assigns partitions to devices and writes an optimized Python structure to a
        gzipped, serialized file on disk for shipping out to the servers. The server processes just
        check the modification time of the file occasionally and reload their in-memory copies of
        the ring structure as needed. Because of how the ring-builder manages changes to the ring,
        using a slightly older ring usually just means one of the three replicas for a subset of the
        partitions will be incorrect, which can be easily worked around.</para>
    <para>The ring-builder also keeps its own builder file with the ring information and additional
        data required to build future rings. It is very important to keep multiple backup copies of
        these builder files. One option is to copy the builder files out to every server while
        copying the ring files themselves. Another is to upload the builder files into the cluster
        itself. If you lose the builder file, you have to create a new ring from scratch. Nearly all
        partitions would be assigned to different devices and, therefore, nearly all of the stored
        data would have to be replicated to new locations. So, recovery from a builder file loss is
        possible, but data would be unreachable for an extended time.</para>
    <section xml:id="section_ring-data-structure">
        <title>Ring data structure</title>
        <para>The ring data structure consists of three top level
            fields: a list of devices in the cluster, a list of lists
            of device ids indicating partition to device assignments,
            and an integer indicating the number of bits to shift an
            MD5 hash to calculate the partition for the hash.</para>
    </section>
    <section xml:id="section_partition-assignment">
            <title>Partition assignment list</title>
        <para>This is a list of <literal>array(‘H’)</literal> of devices ids. The
            outermost list contains an <literal>array(‘H’)</literal> for each
            replica. Each <literal>array(‘H’)</literal> has a length equal to the
                partition count for the ring. Each integer in the
                <literal>array(‘H’)</literal> is an index into the above list of devices.
                The partition list is known internally to the Ring
                class as <literal>_replica2part2dev_id</literal>.</para>
            <para>So, to create a list of device dictionaries assigned to a partition, the Python
            code would look like:
            <programlisting>devices = [self.devs[part2dev_id[partition]] for
part2dev_id in self._replica2part2dev_id]</programlisting></para>
            <para>That code is a little simplistic, as it does not account for the removal of
            duplicate devices. If a ring has more replicas than devices, then a partition will have
            more than one replica on one device.</para>
        <para><literal>array(‘H’)</literal> is used for memory conservation as there
                may be millions of partitions.</para>
    </section>
    <section xml:id="section_fractional-replicas">
            <title>Fractional replicas</title>
            <para>A ring is not restricted to having an integer number
                of replicas. In order to support the gradual changing
                of replica counts, the ring is able to have a real
                number of replicas.</para>
            <para>When the number of replicas is not an integer, then the last element of
                <literal>_replica2part2dev_id</literal> will have a length that is less than the
            partition count for the ring. This means that some partitions will have more replicas
            than others. For example, if a ring has 3.25 replicas, then 25 percent of its partitions
            will have four replicas, while the remaining 75 percent will have just three.</para>
    </section>
<section xml:id="section_partition-shift-value">
            <title>Partition shift value</title>
            <para>The partition shift value is known internally to the
                Ring class as <literal>_part_shift</literal>. This value used to shift an
                MD5 hash to calculate the partition on which the data
                for that hash should reside. Only the top four bytes
                of the hash is used in this process. For example, to
                compute the partition for the path
                /account/container/object the Python code might look
                like:
<programlisting>partition = unpack_from('&gt;I',
md5('/account/container/object').digest())[0] &gt;&gt;
self._part_shift</programlisting></para>
            <para>For a ring generated with part_power P, the
                partition shift value is <literal>32 - P</literal>.</para>
</section>
    <section xml:id="section_build-ring">
        <title>Build the ring</title>
        <para>The initial building of the ring first calculates the
            number of partitions that should ideally be assigned to
            each device based the device’s weight. For example, given
            a partition power of 20, the ring will have 1,048,576
            partitions. If there are 1,000 devices of equal weight
            they will each desire 1,048.576 partitions. The devices
            are then sorted by the number of partitions they desire
            and kept in order throughout the initialization
            process.</para>
        <note><para>Each device is also assigned a random tiebreaker
            value that is used when two devices desire the same number
            of partitions. This tiebreaker is not stored on disk
            anywhere, and so two different rings created with the same
            parameters will have different partition assignments. For
            repeatable partition assignments, <literal>RingBuilder.rebalance()</literal>
            takes an optional seed value that will be used to seed
            Python’s pseudo-random number generator.</para></note>
        <para>Then, the ring builder assigns each replica of each partition to the device that
            requires most partitions at that point while keeping it as far away as possible from
            other replicas. The ring builder prefers to assign a replica to a device in a region
            does not already have a replica. If no such region is available, the ring builder tries
            to find a device in a different zone. If that's not possible, it will look on a
            different server. If it doesn't find one there, it will just look for a device that has
            no replicas. Finally, if all of the other options are exhausted, the ring builder
            assigns the replica to the device that has the fewest replicas already assigned. Note
            that assignment of multiple replicas to one device will only happen if the ring has
            fewer devices than it has replicas.</para>
        <para>When building a new ring based on an old ring, the desired number of partitions each
            device wants is recalculated. Next, the partitions to be reassigned are gathered up. Any
            removed devices have all their assigned partitions unassigned and added to the gathered
            list. Any partition replicas that (due to the addition of new devices) can be spread out
            for better durability are unassigned and added to the gathered list. Any devices that
            have more partitions than they now need have random partitions unassigned from them and
            added to the gathered list. Lastly, the gathered partitions are then reassigned to
            devices using a similar method as in the initial assignment described above.</para>
        <para>Whenever a partition has a replica reassigned, the time of the reassignment is
            recorded. This is taken into account when gathering partitions to reassign so that no
            partition is moved twice in a configurable amount of time. This configurable amount of
            time is known internally to the RingBuilder class as <literal>min_part_hours</literal>.
            This restriction is ignored for replicas of partitions on devices that have been removed
            since removing a device only happens on device failure and reasssignment is the only
            choice.</para>
        <para>The above processes don’t always perfectly rebalance a ring due to the random nature
            of gathering partitions for reassignment. To help reach a more balanced ring, the
            rebalance process is repeated until near perfect (less than 1 percent off) or when the
            balance doesn’t improve by at least 1 percent (indicating we probably can’t get perfect
            balance due to wildly imbalanced zones or too many partitions recently moved).</para>
    </section>
</section>
