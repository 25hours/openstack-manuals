<?xml version="1.0" encoding="UTF-8"?>
        <!-- Warning: Do not edit this file. It is automatically
             generated and your changes will be overwritten.
             The tool to do so lives in the tools directory of this
             repository -->
        <para xmlns="http://docbook.org/ns/docbook" version="5.0">
        <table rules="all">
          <caption>Description of configuration options for storage</caption>
           <col width="50%"/>
           <col width="50%"/>
           <thead>
              <tr>
                  <td>Configuration option=Default value</td>
                  <td>(Type) Description</td>
              </tr>
          </thead>
          <tbody>
              <tr>
                       <td>available_devices=</td>
                       <td>(ListOpt) List of all available devices</td>
              </tr>
              <tr>
                       <td>backend=sqlalchemy</td>
                       <td>(StrOpt) The backend to use for db</td>
              </tr>
              <tr>
                       <td>bindir=$pybasedir/bin</td>
                       <td>(StrOpt) Directory where cinder binaries are installed</td>
              </tr>
              <tr>
                       <td>capacity_weight_multiplier=1.0</td>
                       <td>(FloatOpt) Multiplier used for weighing volume capacity. Negative numbers mean to stack vs spread.</td>
              </tr>
              <tr>
                       <td>cinder_huawei_conf_file=/etc/cinder/cinder_huawei_conf.xml</td>
                       <td>(StrOpt) config data for cinder huawei plugin</td>
              </tr>
              <tr>
                       <td>coraid_esm_address=</td>
                       <td>(StrOpt) IP address of Coraid ESM</td>
              </tr>
              <tr>
                       <td>coraid_group=admin</td>
                       <td>(StrOpt) Name of group on Coraid ESM to which coraid_user belongs (must have admin privilege)</td>
              </tr>
              <tr>
                       <td>coraid_password=password</td>
                       <td>(StrOpt) Password to connect to Coraid ESM</td>
              </tr>
              <tr>
                       <td>coraid_repository_key=coraid_repository</td>
                       <td>(StrOpt) Volume Type key name to store ESM Repository Name</td>
              </tr>
              <tr>
                       <td>coraid_user=admin</td>
                       <td>(StrOpt) User name to connect to Coraid ESM</td>
              </tr>
              <tr>
                       <td>enabled_backends=None</td>
                       <td>(ListOpt) A list of backend names to use. These backend names should be backed by a unique [CONFIG] group with its options</td>
              </tr>
              <tr>
                       <td>glusterfs_disk_util=df</td>
                       <td>(StrOpt) Use du or df for free space calculation</td>
              </tr>
              <tr>
                       <td>glusterfs_mount_point_base=$state_path/mnt</td>
                       <td>(StrOpt) Base dir containing mount points for gluster shares</td>
              </tr>
              <tr>
                       <td>glusterfs_shares_config=/etc/cinder/glusterfs_shares</td>
                       <td>(StrOpt) File with the list of available gluster shares</td>
              </tr>
              <tr>
                       <td>glusterfs_sparsed_volumes=True</td>
                       <td>(BoolOpt) Create volumes as sparsed files which take no space.If set to False volume is created as regular file.In such case volume creation takes a lot of time.</td>
              </tr>
              <tr>
                       <td>hds_cinder_config_file=/opt/hds/hus/cinder_hus_conf.xml</td>
                       <td>(StrOpt) configuration file for HDS cinder plugin for HUS</td>
              </tr>
              <tr>
                       <td>iscsi_helper=tgtadm</td>
                       <td>(StrOpt) iscsi target user-land tool to use</td>
              </tr>
              <tr>
                       <td>iscsi_iotype=fileio</td>
                       <td>(StrOpt) Sets the behavior of the iSCSI target to either perform blockio or fileio optionally, auto can be set and Cinder will autodetect type of backing device</td>
              </tr>
              <tr>
                       <td>iscsi_ip_address=$my_ip</td>
                       <td>(StrOpt) The port that the iSCSI daemon is listening on</td>
              </tr>
              <tr>
                       <td>iscsi_num_targets=100</td>
                       <td>(IntOpt) Number of iscsi target ids per host</td>
              </tr>
              <tr>
                       <td>iscsi_port=3260</td>
                       <td>(IntOpt) The port that the iSCSI daemon is listening on</td>
              </tr>
              <tr>
                       <td>iscsi_target_prefix=iqn.2010-10.org.openstack:</td>
                       <td>(StrOpt) prefix for iscsi volumes</td>
              </tr>
              <tr>
                       <td>lvm_mirrors=0</td>
                       <td>(IntOpt) If set, create lvms with multiple mirrors. Note that this requires lvm_mirrors + 2 pvs with available space</td>
              </tr>
              <tr>
                       <td>max_age=0</td>
                       <td>(IntOpt) number of seconds between subsequent usage refreshes</td>
              </tr>
              <tr>
                       <td>max_gigabytes=10000</td>
                       <td>(IntOpt) maximum number of volume gigabytes to allow per host</td>
              </tr>
              <tr>
                       <td>max_overflow=None</td>
                       <td>(IntOpt) If set, use this value for max_overflow with sqlalchemy</td>
              </tr>
              <tr>
                       <td>max_pool_size=5</td>
                       <td>(IntOpt) Maximum number of SQL connections to keep open in a pool</td>
              </tr>
              <tr>
                       <td>max_retries=10</td>
                       <td>(IntOpt) maximum db connection retries during startup. (setting -1 implies an infinite retry count)</td>
              </tr>
              <tr>
                       <td>memcached_servers=None</td>
                       <td>(ListOpt) Memcached servers or None for in process cache.</td>
              </tr>
              <tr>
                       <td>migration_create_volume_timeout_secs=300</td>
                       <td>(IntOpt) Timeout for creating the volume to migrate to when performing volume migration (seconds)</td>
              </tr>
              <tr>
                       <td>min_pool_size=1</td>
                       <td>(IntOpt) Minimum number of SQL connections to keep open in a pool</td>
              </tr>
              <tr>
                       <td>netapp_login=None</td>
                       <td>(StrOpt) User name for the storage controller</td>
              </tr>
              <tr>
                       <td>netapp_password=None</td>
                       <td>(StrOpt) Password for the storage controller</td>
              </tr>
              <tr>
                       <td>netapp_server_hostname=None</td>
                       <td>(StrOpt) Host name for the storage controller</td>
              </tr>
              <tr>
                       <td>netapp_server_port=80</td>
                       <td>(IntOpt) Port number for the storage controller</td>
              </tr>
              <tr>
                       <td>netapp_size_multiplier=1.2</td>
                       <td>(FloatOpt) Volume size multiplier to ensure while creation</td>
              </tr>
              <tr>
                       <td>netapp_storage_family=ontap_cluster</td>
                       <td>(StrOpt) Storage family type.</td>
              </tr>
              <tr>
                       <td>netapp_storage_protocol=None</td>
                       <td>(StrOpt) Storage protocol type.</td>
              </tr>
              <tr>
                       <td>netapp_transport_type=http</td>
                       <td>(StrOpt) Transport type protocol</td>
              </tr>
              <tr>
                       <td>netapp_vfiler=None</td>
                       <td>(StrOpt) Vfiler to use for provisioning</td>
              </tr>
              <tr>
                       <td>netapp_volume_list=None</td>
                       <td>(StrOpt) Comma separated volumes to be used for provisioning</td>
              </tr>
              <tr>
                       <td>netapp_vserver=openstack</td>
                       <td>(StrOpt) Cluster vserver to use for provisioning</td>
              </tr>
              <tr>
                       <td>nexenta_blocksize=</td>
                       <td>(StrOpt) block size for volumes (blank=default,8KB)</td>
              </tr>
              <tr>
                       <td>nexenta_host=</td>
                       <td>(StrOpt) IP address of Nexenta SA</td>
              </tr>
              <tr>
                       <td>nexenta_iscsi_target_portal_port=3260</td>
                       <td>(IntOpt) Nexenta target portal port</td>
              </tr>
              <tr>
                       <td>nexenta_password=nexenta</td>
                       <td>(StrOpt) Password to connect to Nexenta SA</td>
              </tr>
              <tr>
                       <td>nexenta_rest_port=2000</td>
                       <td>(IntOpt) HTTP port to connect to Nexenta REST API server</td>
              </tr>
              <tr>
                       <td>nexenta_rest_protocol=auto</td>
                       <td>(StrOpt) Use http or https for REST connection (default auto)</td>
              </tr>
              <tr>
                       <td>nexenta_sparse=False</td>
                       <td>(BoolOpt) flag to create sparse volumes</td>
              </tr>
              <tr>
                       <td>nexenta_target_group_prefix=cinder/</td>
                       <td>(StrOpt) prefix for iSCSI target groups on SA</td>
              </tr>
              <tr>
                       <td>nexenta_target_prefix=iqn.1986-03.com.sun:02:cinder-</td>
                       <td>(StrOpt) IQN prefix for iSCSI targets</td>
              </tr>
              <tr>
                       <td>nexenta_user=admin</td>
                       <td>(StrOpt) User name to connect to Nexenta SA</td>
              </tr>
              <tr>
                       <td>nexenta_volume=cinder</td>
                       <td>(StrOpt) pool on SA that will hold all volumes</td>
              </tr>
              <tr>
                       <td>nfs_mount_options=None</td>
                       <td>(StrOpt) Mount options passed to the nfs client. See section of the nfs man page for details</td>
              </tr>
              <tr>
                       <td>nfs_mount_point_base=$state_path/mnt</td>
                       <td>(StrOpt) Base dir containing mount points for nfs shares</td>
              </tr>
              <tr>
                       <td>nfs_oversub_ratio=1.0</td>
                       <td>(FloatOpt) This will compare the allocated to available space on the volume destination.  If the ratio exceeds this number, the destination will no longer be valid.</td>
              </tr>
              <tr>
                       <td>nfs_shares_config=/etc/cinder/nfs_shares</td>
                       <td>(StrOpt) File with the list of available nfs shares</td>
              </tr>
              <tr>
                       <td>nfs_sparsed_volumes=True</td>
                       <td>(BoolOpt) Create volumes as sparsed files which take no space.If set to False volume is created as regular file.In such case volume creation takes a lot of time.</td>
              </tr>
              <tr>
                       <td>nfs_used_ratio=0.95</td>
                       <td>(FloatOpt) Percent of ACTUAL usage of the underlying volume before no new volumes can be allocated to the volume destination.</td>
              </tr>
              <tr>
                       <td>rbd_ceph_conf=</td>
                       <td>(StrOpt) path to the ceph configuration file to use</td>
              </tr>
              <tr>
                       <td>rbd_flatten_volume_from_snapshot=False</td>
                       <td>(BoolOpt) flatten volumes created from snapshots to remove dependency</td>
              </tr>
              <tr>
                       <td>rbd_pool=rbd</td>
                       <td>(StrOpt) the RADOS pool in which rbd volumes are stored</td>
              </tr>
              <tr>
                       <td>rbd_secret_uuid=None</td>
                       <td>(StrOpt) the libvirt uuid of the secret for the rbd_uservolumes</td>
              </tr>
              <tr>
                       <td>rbd_user=None</td>
                       <td>(StrOpt) the RADOS client name for accessing rbd volumes - only set when using cephx authentication</td>
              </tr>
              <tr>
                       <td>san_clustername=</td>
                       <td>(StrOpt) Cluster name to use for creating volumes</td>
              </tr>
              <tr>
                       <td>san_ip=</td>
                       <td>(StrOpt) IP address of SAN controller</td>
              </tr>
              <tr>
                       <td>san_is_local=False</td>
                       <td>(BoolOpt) Execute commands locally instead of over SSH; use if the volume service is running on the SAN device</td>
              </tr>
              <tr>
                       <td>san_login=admin</td>
                       <td>(StrOpt) Username for SAN controller</td>
              </tr>
              <tr>
                       <td>san_password=</td>
                       <td>(StrOpt) Password for SAN controller</td>
              </tr>
              <tr>
                       <td>san_private_key=</td>
                       <td>(StrOpt) Filename of private key to use for SSH authentication</td>
              </tr>
              <tr>
                       <td>san_ssh_port=22</td>
                       <td>(IntOpt) SSH port to use with SAN</td>
              </tr>
              <tr>
                       <td>san_thin_provision=True</td>
                       <td>(BoolOpt) Use thin provisioning for SAN volumes?</td>
              </tr>
              <tr>
                       <td>san_zfs_volume_base=rpool/</td>
                       <td>(StrOpt) The ZFS path under which to create zvols for volumes.</td>
              </tr>
              <tr>
                       <td>scality_sofs_config=None</td>
                       <td>(StrOpt) Path or URL to Scality SOFS configuration file</td>
              </tr>
              <tr>
                       <td>scality_sofs_mount_point=$state_path/scality</td>
                       <td>(StrOpt) Base dir where Scality SOFS shall be mounted</td>
              </tr>
              <tr>
                       <td>scality_sofs_volume_dir=cinder/volumes</td>
                       <td>(StrOpt) Path from Scality SOFS root to volume dir</td>
              </tr>
              <tr>
                       <td>sf_account_prefix=autodoc</td>
                       <td>(StrOpt) Create SolidFire accounts with this prefix</td>
              </tr>
              <tr>
                       <td>sf_allow_tenant_qos=False</td>
                       <td>(BoolOpt) Allow tenants to specify QOS on create</td>
              </tr>
              <tr>
                       <td>sf_emulate_512=True</td>
                       <td>(BoolOpt) Set 512 byte emulation on volume creation; </td>
              </tr>
              <tr>
                       <td>storwize_svc_connection_protocol=iSCSI</td>
                       <td>(StrOpt) Connection protocol (iSCSI/FC)</td>
              </tr>
              <tr>
                       <td>storwize_svc_flashcopy_timeout=120</td>
                       <td>(IntOpt) Maximum number of seconds to wait for FlashCopy to be prepared. Maximum value is 600 seconds (10 minutes).</td>
              </tr>
              <tr>
                       <td>storwize_svc_multihostmap_enabled=True</td>
                       <td>(BoolOpt) Allows vdisk to multi host mapping</td>
              </tr>
              <tr>
                       <td>storwize_svc_multipath_enabled=False</td>
                       <td>(BoolOpt) Connect with multipath (currently FC-only)</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_autoexpand=True</td>
                       <td>(BoolOpt) Storage system autoexpand parameter for volumes (True/False)</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_compression=False</td>
                       <td>(BoolOpt) Storage system compression option for volumes</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_easytier=True</td>
                       <td>(BoolOpt) Enable Easy Tier for volumes</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_grainsize=256</td>
                       <td>(IntOpt) Storage system grain size parameter for volumes (32/64/128/256)</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_rsize=2</td>
                       <td>(IntOpt) Storage system space-efficiency parameter for volumes (percentage)</td>
              </tr>
              <tr>
                       <td>storwize_svc_vol_warning=0</td>
                       <td>(IntOpt) Storage system threshold for volume capacity warnings (percentage)</td>
              </tr>
              <tr>
                       <td>storwize_svc_volpool_name=volpool</td>
                       <td>(StrOpt) Storage system storage pool for volumes</td>
              </tr>
              <tr>
                       <td>volume_backend_name=None</td>
                       <td>(StrOpt) The backend name for a given driver implementation</td>
              </tr>
              <tr>
                       <td>volume_clear=zero</td>
                       <td>(StrOpt) Method used to wipe old volumes (valid options are: none, zero, shred)</td>
              </tr>
              <tr>
                       <td>volume_clear_size=0</td>
                       <td>(IntOpt) Size in MiB to wipe at start of old volumes. 0 =&gt; all</td>
              </tr>
              <tr>
                       <td>volume_dd_blocksize=1M</td>
                       <td>(StrOpt) The default block size used when copying/clearing volumes</td>
              </tr>
              <tr>
                       <td>volume_driver=cinder.volume.drivers.lvm.LVMISCSIDriver</td>
                       <td>(StrOpt) Driver to use for volume creation</td>
              </tr>
              <tr>
                       <td>volume_group=cinder-volumes</td>
                       <td>(StrOpt) Name for the VG that will contain exported volumes</td>
              </tr>
              <tr>
                       <td>volume_manager=cinder.volume.manager.VolumeManager</td>
                       <td>(StrOpt) full class name for the Manager for volume</td>
              </tr>
              <tr>
                       <td>volume_name_template=volume-%s</td>
                       <td>(StrOpt) Template string to be used to generate volume names</td>
              </tr>
              <tr>
                       <td>volume_tmp_dir=None</td>
                       <td>(StrOpt) where to store temporary image files if the volume driver does not write them directly to the volume</td>
              </tr>
              <tr>
                       <td>volume_topic=cinder-volume</td>
                       <td>(StrOpt) the topic volume nodes listen on</td>
              </tr>
              <tr>
                       <td>volume_transfer_key_length=16</td>
                       <td>(IntOpt) The number of characters in the autogenerated auth key.</td>
              </tr>
              <tr>
                       <td>volume_transfer_salt_length=8</td>
                       <td>(IntOpt) The number of characters in the salt.</td>
              </tr>
              <tr>
                       <td>volume_usage_audit_period=month</td>
                       <td>(StrOpt) time period to generate volume usages for.  Time period must be hour, day, month or year</td>
              </tr>
              <tr>
                       <td>volumes_dir=$state_path/volumes</td>
                       <td>(StrOpt) Volume configuration file storage directory</td>
              </tr>
              <tr>
                       <td>windows_iscsi_lun_path=C:\iSCSIVirtualDisks</td>
                       <td>(StrOpt) Path to store VHD backed volumes</td>
              </tr>
              <tr>
                       <td>xiv_proxy=xiv_openstack.nova_proxy.XIVNovaProxy</td>
                       <td>(StrOpt) Proxy driver</td>
              </tr>
              <tr>
                       <td>zadara_default_cache_policy=write-through</td>
                       <td>(StrOpt) Default cache policy for volumes</td>
              </tr>
              <tr>
                       <td>zadara_default_encryption=NO</td>
                       <td>(StrOpt) Default encryption policy for volumes</td>
              </tr>
              <tr>
                       <td>zadara_default_stripesize=64</td>
                       <td>(StrOpt) Default stripe size for volumes</td>
              </tr>
              <tr>
                       <td>zadara_default_striping_mode=simple</td>
                       <td>(StrOpt) Default striping mode for volumes</td>
              </tr>
              <tr>
                       <td>zadara_password=None</td>
                       <td>(StrOpt) Password for the VPSA</td>
              </tr>
              <tr>
                       <td>zadara_user=None</td>
                       <td>(StrOpt) User name for the VPSA</td>
              </tr>
              <tr>
                       <td>zadara_vol_name_template=OS_%s</td>
                       <td>(StrOpt) Default template for VPSA volume names</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_allow_nonexistent_delete=True</td>
                       <td>(BoolOpt) Don't halt on deletion of non-existing volumes</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_auto_detach_on_delete=True</td>
                       <td>(BoolOpt) Automatically detach from servers on volume delete</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_ip=None</td>
                       <td>(StrOpt) Management IP of Zadara VPSA</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_poolname=None</td>
                       <td>(StrOpt) Name of VPSA storage pool for volumes</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_port=None</td>
                       <td>(StrOpt) Zadara VPSA port number</td>
              </tr>
              <tr>
                       <td>zadara_vpsa_use_ssl=False</td>
                       <td>(BoolOpt) Use SSL connection</td>
              </tr>
       </tbody>
        </table>
        </para>