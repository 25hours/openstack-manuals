<?xml version='1.0' encoding='UTF-8'?>
<para xmlns="http://docbook.org/ns/docbook" version="5.0">
  <!-- Warning: Do not edit this file. It is automatically
     generated and your changes will be overwritten.
     The tool to do so lives in openstack-doc-tools repository. -->
  <table rules="all" xml:id="config_table_nova_volumes">
    <caption>Description of volumes configuration options</caption>
    <col width="50%"/>
    <col width="50%"/>
    <thead>
      <tr>
        <th>Configuration option = Default value</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <th colspan="2">[DEFAULT]</th>
      </tr>
      <tr>
        <td>block_device_allocate_retries = 60</td>
        <td>(IntOpt) Number of times to retry block device allocation on failures</td>
      </tr>
      <tr>
        <td>block_device_allocate_retries_interval = 3</td>
        <td>(IntOpt) Waiting time interval (seconds) between block device allocation retries on failures</td>
      </tr>
      <tr>
        <td>volume_api_class = nova.volume.cinder.API</td>
        <td>(StrOpt) The full class name of the volume API class to use</td>
      </tr>
      <tr>
        <td>volume_usage_poll_interval = 0</td>
        <td>(IntOpt) Interval in seconds for gathering volume usages</td>
      </tr>
      <tr>
        <th colspan="2">[baremetal]</th>
      </tr>
      <tr>
        <td>iscsi_iqn_prefix = iqn.2010-10.org.openstack.baremetal</td>
        <td>(StrOpt) The iSCSI IQN prefix used in baremetal volume connections.</td>
      </tr>
      <tr>
        <td>volume_driver = nova.virt.baremetal.volume_driver.LibvirtVolumeDriver</td>
        <td>(StrOpt) Baremetal volume driver.</td>
      </tr>
      <tr>
        <th colspan="2">[cinder]</th>
      </tr>
      <tr>
        <td>api_insecure = False</td>
        <td>(BoolOpt) Allow to perform insecure SSL requests to cinder</td>
      </tr>
      <tr>
        <td>ca_certificates_file = None</td>
        <td>(StrOpt) Location of ca certificates file to use for cinder client requests.</td>
      </tr>
      <tr>
        <td>catalog_info = volume:cinder:publicURL</td>
        <td>(StrOpt) Info to match when looking for cinder in the service catalog. Format is: separated values of the form: &lt;service_type&gt;:&lt;service_name&gt;:&lt;endpoint_type&gt;</td>
      </tr>
      <tr>
        <td>cross_az_attach = True</td>
        <td>(BoolOpt) Allow attach between instance and volume in different availability zones.</td>
      </tr>
      <tr>
        <td>endpoint_template = None</td>
        <td>(StrOpt) Override service catalog lookup with template for cinder endpoint e.g. http://localhost:8776/v1/%(project_id)s</td>
      </tr>
      <tr>
        <td>http_retries = 3</td>
        <td>(IntOpt) Number of cinderclient retries on failed http calls</td>
      </tr>
      <tr>
        <td>http_timeout = None</td>
        <td>(IntOpt) HTTP inactivity timeout (in seconds)</td>
      </tr>
      <tr>
        <td>os_region_name = None</td>
        <td>(StrOpt) Region name of this node</td>
      </tr>
      <tr>
        <th colspan="2">[hyperv]</th>
      </tr>
      <tr>
        <td>force_volumeutils_v1 = False</td>
        <td>(BoolOpt) Force V1 volume utility class</td>
      </tr>
      <tr>
        <td>volume_attach_retry_count = 10</td>
        <td>(IntOpt) The number of times to retry to attach a volume</td>
      </tr>
      <tr>
        <td>volume_attach_retry_interval = 5</td>
        <td>(IntOpt) Interval between volume attachment attempts, in seconds</td>
      </tr>
      <tr>
        <th colspan="2">[libvirt]</th>
      </tr>
      <tr>
        <td>glusterfs_mount_point_base = $state_path/mnt</td>
        <td>(StrOpt) Directory where the glusterfs volume is mounted on the compute node</td>
      </tr>
      <tr>
        <td>nfs_mount_options = None</td>
        <td>(StrOpt) Mount options passedf to the NFS client. See section of the nfs man page for details</td>
      </tr>
      <tr>
        <td>nfs_mount_point_base = $state_path/mnt</td>
        <td>(StrOpt) Directory where the NFS volume is mounted on the compute node</td>
      </tr>
      <tr>
        <td>num_aoe_discover_tries = 3</td>
        <td>(IntOpt) Number of times to rediscover AoE target to find volume</td>
      </tr>
      <tr>
        <td>num_iscsi_scan_tries = 5</td>
        <td>(IntOpt) Number of times to rescan iSCSI target to find volume</td>
      </tr>
      <tr>
        <td>num_iser_scan_tries = 5</td>
        <td>(IntOpt) Number of times to rescan iSER target to find volume</td>
      </tr>
      <tr>
        <td>qemu_allowed_storage_drivers = </td>
        <td>(ListOpt) Protocols listed here will be accessed directly from QEMU. Currently supported protocols: [gluster]</td>
      </tr>
      <tr>
        <td>rbd_secret_uuid = None</td>
        <td>(StrOpt) The libvirt UUID of the secret for the rbd_uservolumes</td>
      </tr>
      <tr>
        <td>rbd_user = None</td>
        <td>(StrOpt) The RADOS client name for accessing rbd volumes</td>
      </tr>
      <tr>
        <td>scality_sofs_config = None</td>
        <td>(StrOpt) Path or URL to Scality SOFS configuration file</td>
      </tr>
      <tr>
        <td>scality_sofs_mount_point = $state_path/scality</td>
        <td>(StrOpt) Base dir where Scality SOFS shall be mounted</td>
      </tr>
      <tr>
        <th colspan="2">[xenserver]</th>
      </tr>
      <tr>
        <td>block_device_creation_timeout = 10</td>
        <td>(IntOpt) Time to wait for a block device to be created</td>
      </tr>
    </tbody>
  </table>
</para>
