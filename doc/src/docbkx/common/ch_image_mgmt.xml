<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_image_mgmt">
    <title>Image Management</title>
    <para>You can use OpenStack Image Services for discovering,
        registering, and retrieving virtual machine images. The
        service includes a RESTful API that allows users to query VM
        image metadata and retrieve the actual image with HTTP
        requests, or you can use a client class in your Python code to
        accomplish the same tasks. </para>
    <para> VM images made available through OpenStack Image Service
        can be stored in a variety of locations from simple file
        systems to object-storage systems like the OpenStack Object
        Storage project, or even use S3 storage either on its own or
        through an OpenStack Object Storage S3 interface.</para>
    <para>The backend stores that OpenStack Image Service can work
        with are as follows:</para>
    <itemizedlist>
        <listitem>
            <para>OpenStack Object Storage - OpenStack Object Storage
                is the highly-available object storage project in
                OpenStack.</para>
        </listitem>

        <listitem>
            <para>Filesystem - The default backend that OpenStack
                Image Service uses to store virtual machine images is
                the filesystem backend. This simple backend writes
                image files to the local filesystem.</para>
        </listitem>

        <listitem>
            <para>S3 - This backend allows OpenStack Image Service to
                store virtual machine images in Amazon’s S3
                service.</para>
        </listitem>

        <listitem>
            <para>HTTP - OpenStack Image Service can read virtual
                machine images that are available via HTTP somewhere
                on the Internet. This store is readonly.</para>
        </listitem>
    </itemizedlist>

    <para>This chapter assumes you have a working installation of the
        Image Service, with a working endpoint and users created in
        the Identity service, plus you have sourced the environment
        variables required by the nova client and glance
        client.</para>
    <xi:include href="tenant-specific-image-storage.xml"/>
    <xi:include href="adding-images.xml"/>
    <section xml:id="starting-images">
        <title>Getting virtual machine images</title>
        <?dbhtml stop-chunking?>
        <section xml:id="cirros-images">
            <title>CirrOS (test) images</title>
            <para>Scott Moser maintains a set of small virtual machine
                images that are designed for testing. These images use
                    <literal>cirros</literal> as the login user. They
                are hosted under the CirrOS project on Launchpad
                    and<link
                    xlink:href="https://launchpad.net/cirros/+download"
                    >are available for download</link>. </para>
            <para> If your deployment uses QEMU or KVM, we recommend
                using the images in QCOW2 format. The most recent
                64-bit QCOW2 image as of this writing is <link
                    xlink:href="https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img"
                    >cirros-0.3.0-x86_64-disk.img</link>
            </para>
        </section>

        <section xml:id="ubuntu-images">
            <title>Ubuntu images</title>
            <para>Canonical maintains an <link
                    xlink:href="http://uec-images.ubuntu.com">official
                    set of Ubuntu-based images</link> These accounts
                use <literal>ubuntu</literal> as the login
                user.</para>
            <para>If your deployment uses QEMU or KVM, we recommend
                using the images in QCOW2 format. The most recent
                version of the 64-bit QCOW2 image for Ubuntu 12.04 is
                    <link
                    xlink:href="http://uec-images.ubuntu.com/precise/current/precise-server-cloudimg-amd64-disk1.img"
                    >precise-server-cloudimg-amd64-disk1.img</link>.</para>
        </section>

        <section xml:id="fedora-images">
            <title>Fedora images</title>
            <para>The Fedora project maintains prebuilt Fedora JEOS
                (Just Enough OS) images for download at <link
                    xlink:href="http://berrange.fedorapeople.org/images"
                    >http://berrange.fedorapeople.org/images
                </link>.</para>
            <para>A 64-bit QCOW2 image for Fedora 16, <link
                    xlink:href="http://berrange.fedorapeople.org/images/2012-02-29/f16-x86_64-openstack-sda.qcow2"
                    > f16-x86_64-openstack-sda.qcow2</link>, is
                available for download. </para>
        </section>
        <section xml:id="suse-sles-images">
            <title>openSUSE and SLES 11 images</title>
            <para><link xlink:href="http://susestudio.com">SUSE
                    Studio</link> is an easy way to build virtual
                appliances for openSUSE and SLES 11 (SUSE Linux
                Enterprise Server) that are compatible with OpenStack.
                Free registration is required to download or build
                images.</para>

            <para>For example, Christian Berendt used openSUSE to
                create <link
                    xlink:href="http://susestudio.com/a/YRUrwO/testing-instance-for-openstack-opensuse-121"
                    >a test openSUSE 12.1 (JeOS) image</link>.</para>
        </section>
        <section xml:id="rcb-images">
            <title>Rackspace Cloud Builders (multiple distros)
                images</title>
            <para>Rackspace Cloud Builders maintains a list of
                pre-built images from various distributions (RedHat,
                CentOS, Fedora, Ubuntu) at <link
                    xlink:href="https://github.com/rackerjoe/oz-image-build"
                    >rackerjoe/oz-image-build on Github</link>.</para>
        </section>
    </section>
    <section xml:id="tool-support-creating-new-images">
        <?dbhtml stop-chunking?>
        <title>Tool support for creating images</title>
        <para>There are several open-source third-party tools
            available that simplify the task of creating new virtual
            machine images.</para>
        <section xml:id="oz">
            <title>Oz (KVM)</title>
            <para><link xlink:href="http://aeolusproject.org/oz.html"
                    >Oz</link> is a command-line tool that has the
                ability to create images for common Linux
                distributions. Rackspace Cloud Builders uses Oz to
                create virtual machines, see <link
                    xlink:href="https://github.com/rackerjoe/oz-image-build"
                    >rackerjoe/oz-image-build on Github</link> for
                their Oz templates. For an example from the Fedora
                Project wiki, see <link
                    xlink:href="https://fedoraproject.org/wiki/Getting_started_with_OpenStack_Nova#Building_an_Image_With_Oz"
                    > Building an image with Oz</link>. </para>
        </section>
        <section xml:id="ubuntu-vm-builder">
            <title>VMBuilder (KVM, Xen)</title>
            <para><link xlink:href="https://launchpad.net/vmbuilder"
                    >VMBuilder</link> can be used to create virtual
                machine images for different hypervisors.</para>
            <para>The <link
                    xlink:href="https://help.ubuntu.com/12.04/serverguide/jeos-and-vmbuilder.html"
                    > Ubuntu 12.04 server guide</link> has
                documentation on how to use VMBuilder.</para>
        </section>
        <section xml:id="boxgrinder">
            <title>BoxGrinder (KVM, Xen, VMWare)</title>
            <para><link xlink:href="http://boxgrinder.org"
                    >BoxGrinder</link> is another tool for creating
                virtual machine images, which it calls appliances.
                BoxGrinder can create Fedora, Red Hat Enterprise
                Linux, or CentOS images. BoxGrinder is currently only
                supported on Fedora. </para>
        </section>
        <section xml:id="veewee">
            <title>VeeWee (KVM)</title>
            <para><link
                    xlink:href="https://github.com/jedi4ever/veewee">
                    VeeWee</link> is often used to build <link
                    xlink:href="http://vagrantup.com">Vagrant</link>
                boxes, but it can also be used to build KVM
                images.</para>
            <para>See the <link
                    xlink:href="https://github.com/jedi4ever/veewee/blob/master/doc/definition.md"
                    >doc/definition.md</link> and <link
                    xlink:href="https://github.com/jedi4ever/veewee/blob/master/doc/template.md"
                    >doc/template.md</link> VeeWee documentation files
                for more details. </para>
        </section>
        <section xml:id="imagefactory">
            <title>imagefactory</title>
            <para><link xlink:href="http://imgfac.org/"
                    >imagefactory</link> is a new tool from the <link
                    xlink:href="http://www.aeolusproject.org/"
                    >Aeolus</link> project designed to automate the
                building, converting, and uploading images to
                different cloud providers. It includes support for
                OpenStack-based clouds.</para>
        </section>

    </section>
    <section xml:id="image-customizing-what-you-need-to-know">
        <?dbhtml stop-chunking?>
        <title>Customizing an image for OpenStack</title>
        <para>This section describes what customizations you should to
            your image to maximize compatibility with
            OpenStack.</para>
        <section xml:id="support-metadata-or-config-drive">
            <title>Support metadata service or config drive</title>
            <para/>
            <!--<para>An image needs to be able to retrieve information from OpenStack, such as the ssh public key
                and <link linkend="user-data">user data</link> that the user submitted when
                requesting the image. This information is accessible via the <link
                    linkend="metadata-service">metadata service</link> or the <link
                    linkend="config-drive">config drive</link>. The easiest way to support this is
                to install the <link xlink:href="http://launchpad.net/cloud-init">cloud-init</link>
                package into your image.</para>-->
        </section>
        <section xml:id="support-resizing">
            <title>Support resizing</title>
            <!--<para>The size of the disk in a virtual machine image is determined when you initially
                create the image. However, OpenStack lets you launch instances with different size
                drives by specifying different <link linkend="instance-building-blocks-flavors"
                    >flavors</link>. For example, if your image was created with a 5 GB disk, and
                you launch an instance with a flavor of <literal>m1.small</literal>, the resulting
                virtual machine instance will have a primary disk of 10GB. When an instance's disk
                is resized up, zeros are just added to the end.</para>-->
            <para>Your image needs to be able to resize its partitions
                on boot to match the size requested by the user.
                Otherwise, after the instance boots, you will need to
                manually resize the partitions if you want to access
                the additional storage you have access to when the
                disk size associated with the flavor exceeds the disk
                size your image was created with. </para>
            <para>Your image must be configured to deal with two issues:<itemizedlist>
                    <listitem>
                        <para>The image's partition table describes
                            the original size of the image</para>
                    </listitem>
                    <listitem>
                        <para>The image's filesystem fills the
                            original size of the image</para>
                    </listitem>
                </itemizedlist></para>
            <simplesect>
                <title>Adjusting the partition table on instance
                    boot</title>
                <para>Your image will need to run a script on boot to
                    modify the partition table. Due to a limitation in
                    the Linux kernel, you cannot modify a partition
                    table of a disk that has partition currently
                    mounted (you can for LVM, but not for "raw
                    disks"); this partition adjustment has to happen
                    inside the initramfs before the root volume is
                    mounted, or a reboot has to be done to free the
                    mount of <filename>/</filename>.</para>
                <para xlink:href="https://launchpad.net/cloud-utils"
                    >Ubuntu cloud images and cirros images use a tool
                    called <command>growpart</command> that is part of
                    the <link
                        xlink:href="http://launchpad.net/cloud-utils"
                        >cloud-utils</link> package.</para>
            </simplesect>
            <simplesect>
                <title>Adjusting the filesystem</title>
                <para>You will need to resize the file system in
                    addition to the partition table. If you have
                    cloud-init installed, it will do the resize
                    assuming the partition tables have been adjusted
                    properly. Cirros images run resize2fs on the root
                    partition on boot.</para>
                <note>
                    <para>If you are using XenServer as your
                        hypervisor, the above steps are not needed as
                        the Compute service will automatically adjust
                        the partition and filesystem for your instance
                        on boot. Automatic resize will occur if the
                        following are all true:<itemizedlist>
                            <listitem>
                                <para><literal>auto_disk_config=True</literal>
                                   in
                                   <filename>nova.conf</filename>.</para>
                            </listitem>
                            <listitem>
                                <para>The disk on the image has only
                                   one partition.</para>
                            </listitem>
                            <listitem>
                                <para>The file system on the one
                                   partition is ext3 or ext4.</para>
                            </listitem>
                        </itemizedlist></para>
                </note>
            </simplesect>
        </section>

    </section>
    <section xml:id="manually-creating-qcow2-images">
        <title>Creating raw or QCOW2 images</title>
        <para>This section describes how to create a raw or QCOW2
            image from a Linux installation ISO file. Raw images are
            the simplest image file format and are supported by all of
            the hypervisors. QCOW2 images have several advantages over
            raw images. They take up less space than raw images
            (growing in size as needed), and they support snapshots.<note>
                <para>QCOW2 images are only supported with KVM and
                    QEMU hypervisors.</para>
            </note></para>
        <para>As an example, this section will describe how to create
            aa CentOS 6.2 image. <link
                xlink:href="http://isoredirect.centos.org/centos/6/isos/x86_64/"
                >64-bit ISO images of CentOS 6.2</link> can be
            downloaded from one of the CentOS mirrors. This example
            uses the CentOS netinstall ISO, which is a smaller ISO
            file that downloads packages from the Internet as
            needed.</para>
        <simplesect>
            <title>Create an empty image (raw)</title>
            <para>Here we create a a 5GB raw image using the
                    <command>kvm-img</command> command:
                <screen><prompt>$</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>$</prompt> <userinput>kvm-img create -f raw $IMAGE 5G</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Create an empty image (QCOW2)</title>
            <para>Here we create a a 5GB QCOW2 image using the
                    <command>kvm-img</command> command:
                <screen><prompt>$</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>$</prompt> <userinput>kvm-img create -f qcow $IMAGE 5G</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Boot the ISO using the image</title>
            <para>First, find a spare vnc display. (Note that vnc
                display <literal>:N</literal> correspond to TCP port
                5900+N, so that <literal>:0</literal> corresponds to
                port 5900). Check which ones are currently in use with
                the <command>lsof</command> command, as
                root:<screen><prompt>#</prompt> <userinput>lsof -i | grep "TCP \*:590"</userinput>
<computeroutput>kvm        3437 libvirt-qemu   14u  IPv4 1629164      0t0  TCP *:5900 (LISTEN)
kvm        24966 libvirt-qemu   24u  IPv4 1915470      0t0  TCP *:5901 (LISTEN)</computeroutput></screen></para>
            <para>This shows that vnc displays <literal>:0</literal>
                and <literal>:1</literal> are in use. In this example,
                we will use VNC display <literal>:2</literal>.</para>
            <para> Also, we want a temporary file to send power
                signals to the VM instance. We default to
                    <filename>/tmp/file.mon</filename>, but make sure
                it doesn't exist yet. If it does, use a different file
                name for the <literal>MONITOR</literal> variable
                defined
                below:<screen><prompt>$</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>$</prompt> <userinput>ISO=CentOS-6.2-x86_64-netinstall.iso</userinput>
<prompt>$</prompt> <userinput>VNCDISPLAY=:2</userinput>
<prompt>$</prompt> <userinput>MONITOR=/tmp/file.mon</userinput>
<prompt>$</prompt> <userinput>sudo  kvm -m 1024 -cdrom $ISO -drive file=${IMAGE},if=virtio,index=0 \
-boot d -net nic -net user -nographic -vnc ${VNCDISPLAY} \
-monitor unix:${MONITOR},server,nowait</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Connect to the instance via VNC</title>
            <para>VNC is a remote desktop protocol that will give you
                full-screen display access to the virtual machine
                instance, as well as let you interact with keyboard
                and mouse. Use a VNC client (e.g., <link
                    xlink:href="http://projects.gnome.org/vinagre/"
                    >Vinagre</link> on Gnome, <link
                    xlink:href="http://userbase.kde.org/Krdc"
                    >Krdc</link> on KDE, xvnc4viewer from <link
                    xlink:href="http://www.realvnc.com"
                >RealVNC</link>, xtightvncviewer from <link
                    xlink:href="http://www.tightvnc.com"
                    >TightVNC</link>) to connect to the machine using
                the display you specified. You should now see a CentOS
                install screen.</para>
        </simplesect>
        <simplesect>
            <title>Point the installer to a CentOS web server</title>
            <para>The CentOS net installer requires that the user
                specify the web site and a CentOS directory that
                corresponds to one of the CentOS mirrors. <itemizedlist>
                    <listitem>
                        <para>Web site name:
                                <literal>mirror.umd.edu</literal>
                            (consider using other mirrors as an
                            alternative)</para>
                    </listitem>
                    <listitem>
                        <para>CentOS directory:
                                <literal>centos/6.2/os/x86_64</literal></para>
                    </listitem>
                </itemizedlist></para>
            <para>See <link
                    xlink:href="http://www.centos.org/modules/tinycontent/index.php?id=30"
                    >CentOS mirror page</link> to get a full list of
                mirrors, click on the "HTTP" link of a mirror to
                retrieve the web site name of a mirror.</para>
        </simplesect>
        <simplesect>
            <title>Partition the disks</title>
            <para>There are different options for partitioning the
                disks. The default installation will use LVM
                partitions, and will create three partitions
                    (<filename>/boot</filename>,
                    <filename>/</filename>, swap). The simplest
                approach is to create a single ext4 partition, mounted
                to "<literal>/</literal>".</para>
        </simplesect>
        <simplesect>
            <title>Step through the install</title>
            <para>The simplest thing to do is to choose the "Server"
                install, which will install an SSH server.</para>
        </simplesect>
        <simplesect>
            <title>When install completes, shut down the
                instance</title>
            <para>Power down the instance using the monitor socket
                file to send a power down signal, as
                root:<screen><prompt>#</prompt> <userinput>MONITOR=/tmp/file.mon</userinput>
<prompt>#</prompt> <userinput>echo 'system_powerdown' | socat - UNIX-CONNECT:$MONITOR</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Start the instance again without the ISO</title>
            <para>
                <screen>
<prompt>$</prompt> <userinput>VNCDISPLAY=:2</userinput>
<prompt>$</prompt> <userinput>MONITOR=/tmp/file.mon</userinput>
<prompt>$</prompt> <userinput>sudo  kvm -m 1024 -drive file=${IMAGE},if=virtio,index=0 \
-boot c -net nic -net user -nographic -vnc ${VNCDISPLAY} \
-monitor unix:${MONITOR},server,nowait</userinput></screen>
            </para>
        </simplesect>
        <simplesect>
            <title>Connect to instance via VNC</title>
            <para>When you boot the first time, it will ask you about
                authentication tools, you can just choose 'Exit'.
                Then, log in as root using the root password you
                specified.</para>
        </simplesect>
        <simplesect>
            <title>Edit HWADDR from eth0 config file</title>
            <para>The operating system records the MAC address of the
                virtual ethernet card in
                    <filename>/etc/sysconfig/network-scripts/ifcfg-eth0</filename>
                during the instance process. However, each time the
                image boots up, the virtual ethernet card will have a
                different MAC address, so this information must be
                deleted from the configuration file.</para>
            <para>Edit
                    <filename>/etc/sysconfig/network-scripts/ifcfg-eth0</filename>
                and remove the <literal>HWADDR=</literal> line.
            </para>
        </simplesect>
        <simplesect>
            <title>Configure to fetch metadata</title>
            <para>An instance must perform several steps on startup by
                interacting with the metada service (e.g., retrieve
                ssh public key, execute user data script). There are
                several ways to implement this functionality, including:<itemizedlist>
                    <listitem>
                        <para>Install a <link
                                xlink:href="http://koji.fedoraproject.org/koji/packageinfo?packageID=12620"
                                >cloud-init RPM</link> , which is a
                            port of the Ubuntu <link
                                xlink:href="https://launchpad.net/cloud-init"
                                >cloud-init</link> package. This is
                            the recommended approach.</para>
                    </listitem>
                    <listitem>
                        <para>Modify
                                <filename>/etc/rc.local</filename> to
                            fetch desired information from the
                            metadata service, as described
                            below.</para>
                    </listitem>
                </itemizedlist></para>
        </simplesect>
        <simplesect>
            <title>Using cloud-init to fetch the public key</title>
            <para>The cloud-init package will automatically fetch the
                public key from the metadata server and place the key
                in an account. The account varies by distribution. On
                Ubuntu-based virtual virtual machines, the account is
                called "ubuntu". On Fedora-based virtual machines, the
                account is called "ec2-user".</para>
            <para>You can change the name of the account used by
                cloud-init by editing the
                    <filename>/etc/cloud/cloud.cfg</filename> file and
                adding a line with a different user. For example, to
                configure cloud-init to put the key in an account
                named "admin", edit the config file so it has the
                line:<programlisting>user: admin</programlisting></para>
        </simplesect>
        <simplesect>
            <title>Writing a script to fetch the public key</title>
            <para>To fetch the ssh public key and add it to the root
                account, edit the <filename>/etc/rc.local</filename>
                file and add the following lines before the line
                “touch /var/lock/subsys/local”</para>
            <programlisting>depmod -a
modprobe acpiphp

# simple attempt to get the user ssh key using the meta-data service
mkdir -p /root/.ssh
echo &gt;&gt; /root/.ssh/authorized_keys
curl -m 10 -s http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key | grep 'ssh-rsa' &gt;&gt; /root/.ssh/authorized_keys
echo &quot;AUTHORIZED_KEYS:&quot;
echo &quot;************************&quot;
cat /root/.ssh/authorized_keys
echo &quot;************************&quot;
</programlisting>
            <note>
                <para>Some VNC clients replace : (colon) with ;
                    (semicolon) and _ (underscore) with - (hyphen).
                    Make sure it's http: not http; and authorized_keys
                    not authorized-keys.</para>
            </note>
            <note>
                <para>The above script only retrieves the ssh public
                    key from the metadata server. It does not retrieve
                        <emphasis role="italic">user data</emphasis>,
                    which is optional data that can be passed by the
                    user when requesting a new instance. User data is
                    often used for running a custom script when an
                    instance comes up.</para>
                <para>As the OpenStack metadata service is compatible
                    with version 2009-04-04 of the Amazon EC2 metadata
                    service, consult the Amazon EC2 documentation on
                        <link
                        xlink:href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html"
                        >Using Instance Metadata</link> for details on
                    how to retrieve user data.</para>
            </note>

        </simplesect>

        <simplesect>
            <title>Shut down the instance</title>
            <para>From inside the instance, as
                root:<screen><prompt>#</prompt> <userinput>/sbin/shutdown -h now</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Modifying the image (raw)</title>
            <para>You can make changes to the filesystem of an image
                without booting it, by mounting the image as a file
                system. To mount a raw image, you need to attach it to
                a loop device (e.g., <filename>/dev/loop0</filename>,
                    <filename>/dev/loop1</filename>). To identify the
                next unused loop device, as
                root:<screen><prompt>#</prompt> <userinput>losetup -f</userinput>
<computeroutput>/dev/loop0</computeroutput></screen>In
                the example above, <filename>/dev/loop0</filename> is
                available for use. Associate it to the image using
                    <command>losetup</command>, and expose the
                partitions as device files using
                    <command>kpartx</command>, as root:</para>
            <para>
                <screen><prompt>#</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>#</prompt> <userinput>losetup /dev/loop0 $IMAGE</userinput>
<prompt>#</prompt> <userinput>kpartx -av /dev/loop0</userinput></screen>
            </para>
            <para>If the image has, say three partitions (/boot, /,
                /swap), there should be one new device created per
                partition:<screen><prompt>$</prompt> <userinput>ls -l /dev/mapper/loop0p*</userinput><computeroutput>
brw-rw---- 1 root disk 43, 49 2012-03-05 15:32 /dev/mapper/loop0p1
brw-rw---- 1 root disk 43, 50 2012-03-05 15:32 /dev/mapper/loop0p2
brw-rw---- 1 root disk 43, 51 2012-03-05 15:32 /dev/mapper/loop0p3</computeroutput></screen></para>
            <para>To mount the second partition, as
                root:<screen><prompt>#</prompt> <userinput>mkdir /mnt/image</userinput>
<prompt>#</prompt> <userinput>mount /dev/mapper/loop0p2 /mnt/image</userinput></screen></para>
            <para>You can now modify the files in the image by going
                to <filename>/mnt/image</filename>. When done, unmount
                the image and release the loop device, as
                root:<screen><prompt>#</prompt> <userinput>umount /mnt/image</userinput>
<prompt>#</prompt> <userinput>losetup -d /dev/loop0</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Modifying the image (qcow2)</title>
            <para>You can make changes to the filesystem of an image
                without booting it, by mounting the image as a file
                system. To mount a QEMU image, you need the nbd kernel
                module to be loaded. Load the nbd kernel module, as root:<screen><prompt>#</prompt> <userinput>modprobe nbd max_part=8</userinput></screen><note>
                    <para>If nbd has already been loaded with
                            <literal>max_part=0</literal>, you will
                        not be able to mount an image if it has
                        multiple partitions. In this case, you may
                        need to first unload the nbd kernel module,
                        and then load it. To unload it, as
                        root:<screen><prompt>#</prompt> <userinput>rmmod nbd</userinput></screen></para>
                </note></para>
            <para>Connect your image to one of the network block
                devices (e.g., <filename>/dev/nbd0</filename>,
                    <filename>/dev/nbd1</filename>). In this example,
                we use <filename>/dev/nbd3</filename>. As
                root:<screen><prompt>#</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>#</prompt> <userinput>qemu-nbd -c /dev/nbd3 $IMAGE</userinput></screen></para>
            <para>If the image has, say three partitions (/boot, /,
                /swap), there should be one new device created per partition:<screen><prompt>$</prompt> <userinput>ls -l /dev/nbd3*</userinput>
<computeroutput>brw-rw---- 1 root disk 43, 48 2012-03-05 15:32 /dev/nbd3
brw-rw---- 1 root disk 43, 49 2012-03-05 15:32 /dev/nbd3p1
brw-rw---- 1 root disk 43, 50 2012-03-05 15:32 /dev/nbd3p2
brw-rw---- 1 root disk 43, 51 2012-03-05 15:32 /dev/nbd3p3</computeroutput></screen><note>
                    <para>If the network block device you selected was
                        already in use, the initial
                            <command>qemu-nbd</command> command will
                        fail silently, and the
                            <filename>/dev/nbd3p{1,2,3}</filename>
                        device files will not be created.</para>
                </note></para>
            <para>To mount the second partition, as
                root:<screen><prompt>#</prompt> <userinput>mkdir /mnt/image</userinput>
<prompt>#</prompt> <userinput>mount /dev/nbd3p2 /mnt/image</userinput></screen></para>
            <para>You can now modify the files in the image by going
                to <filename>/mnt/image</filename>. When done, unmount
                the image and release the network block device, as
                root:<screen><prompt>#</prompt> <userinput>umount /mnt/image</userinput>
<prompt>#</prompt> <userinput>qemu-nbd -d /dev/nbd3</userinput></screen></para>
        </simplesect>
        <simplesect>
            <title>Upload the image to glance (raw)</title>
            <para>
                <screen><prompt>$</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>$</prompt> <userinput>NAME=centos-6.2</userinput>
<prompt>$</prompt> <userinput>glance image-create --name="${NAME}" --is-public=true --container-format=ovf --disk-format=raw &lt; ${IMAGE}</userinput></screen>
            </para>
        </simplesect>
        <simplesect>
            <title>Upload the image to glance (qcow2)</title>
            <para>
                <screen><prompt>$</prompt> <userinput>IMAGE=centos-6.2.img</userinput>
<prompt>$</prompt> <userinput>NAME=centos-6.2</userinput>
<prompt>$</prompt> <userinput>glance image-create --name="${NAME}" --is-public=true --container-format=ovf --disk-format=qcow2 &lt; ${IMAGE}</userinput></screen>
            </para>
        </simplesect>

    </section>
    <section xml:id="booting-a-test-image">
        <title>Booting a test image</title>
        <para>The following assumes you are using QEMU or KVM in your
            deployment.</para>
        <para>Download a CirrOS test image:</para>
        <screen>
<prompt>$</prompt> <userinput>wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img</userinput>
            </screen>
        <para>Add the image to glance:</para>
        <screen>
<prompt>$</prompt> <userinput>name=cirros-0.3-x86_64</userinput>
<prompt>$</prompt> <userinput>image=cirros-0.3.0-x86_64-disk.img</userinput>
<prompt>$</prompt> <userinput>glance image-create --name=$name --is-public=true --container-format=bare --disk-format=qcow2 &lt; $image</userinput>
            </screen>
        <para>Check that adding the image was successful (Status
            should be ACTIVE when the operation is complete):</para>
        <screen>
<prompt>$</prompt> <userinput>nova image-list</userinput>
<computeroutput>
+--------------------------------------+---------------------+--------+--------+
|                  ID                  |         Name        | Status | Server |
+--------------------------------------+---------------------+--------+--------+
| 254c15e1-78a9-4b30-9b9e-2a39b985001c | cirros-0.3.0-x86_64 | ACTIVE |        |
+--------------------------------------+---------------------+--------+--------+
</computeroutput>
            </screen>
        <para>Create a keypair so you can ssh to the instance: </para>
        <screen>
<prompt>$</prompt> <userinput>nova keypair-add test > test.pem</userinput>
<prompt>$</prompt> <userinput>chmod 600 test.pem</userinput>
            </screen>
        <para>In general, you need to use an ssh keypair to log in to
            a running instance, although some images have built-in
            accounts created with associated passwords. However, since
            images are often shared by many users, it is not advised
            to put passwords into the images. Nova therefore supports
            injecting ssh keys into instances before they are booted.
            This allows a user to login to the instances that he or
            she creates securely. Generally the first thing that a
            user does when using the system is create a keypair. </para>
        <para>Keypairs provide secure authentication to your
            instances. As part of the first boot of a virtual image,
            the private key of your keypair is added to
            authorized_keys file of the login account. Nova generates
            a public and private key pair, and sends the private key
            to the user. The public key is stored so that it can be
            injected into instances. </para>
        <para>Run (boot) a test instance:</para>
        <screen>
<prompt>$</prompt> <userinput>nova boot --image cirros-0.3.0-x86_64 --flavor m1.small --key_name test my-first-server</userinput>
            </screen>
        <para>Here's a description of the parameters used
            above:</para>
        <itemizedlist>
            <listitem>
                <para>
                    <literal>--image</literal>: the name or ID of the
                    image we want to launch, as shown in the output of
                        <command>nova image-list</command></para>
            </listitem>
            <listitem>
                <para>
                    <literal>--flavor</literal>: the name or ID of the
                    size of the instance to create (number of vcpus,
                    available RAM, available storage). View the list
                    of available flavors by running <command>nova
                        flavor-list</command></para>
            </listitem>
            <listitem>
                <para>
                    <literal>-key_name</literal>: the name of the key
                    to inject in to the instance at launch.</para>
            </listitem>
        </itemizedlist>
        <para>Check the status of the instance you launched:</para>
        <screen>
<prompt>$</prompt> <userinput>nova list</userinput>
            </screen>
        <para> The instance will go from BUILD to ACTIVE in a short
            time, and you should be able to connect via ssh as
            'cirros' user, using the private key you created. If your
            ssh keypair fails for some reason, you can also log in
            with the default cirros password:
                <literal>cubswin:)</literal></para>
        <screen>
<prompt>$</prompt> <userinput>ipaddress=... # Get IP address from "nova list"</userinput>
<prompt>$</prompt> <userinput>ssh -i test.pem -l cirros $ipaddress</userinput>
        </screen>
        <para>The 'cirros' user is part of the sudoers group, so you
            can escalate to 'root' via the following command when
            logged in to the instance:</para>
        <screen>
<prompt>$</prompt> <userinput>sudo -i</userinput>
        </screen>

    </section>

    <section xml:id="deleting-instances">

        <title>Tearing down (deleting) Instances</title>

        <para>When you are done with an instance, you can tear it down
            using the <command>nova delete</command> command, passing
            either the instance name or instance ID as the argument.
            You can get a listing of the names and IDs of all running
            instances using the <command>nova list</command>. For
            example:</para>
        <screen>
<prompt>$</prompt> <userinput>nova list</userinput>
<computeroutput>
+--------------------------------------+-----------------+--------+----------+
|                  ID                  |       Name      | Status | Networks |
+--------------------------------------+-----------------+--------+----------+
| 8a5d719a-b293-4a5e-8709-a89b6ac9cee2 | my-first-server | ACTIVE |          |
+--------------------------------------+-----------------+--------+----------+
</computeroutput>
<prompt>$</prompt> <userinput>nova delete my-first-server</userinput>
        </screen>

    </section>
    <section xml:id="pausing-and-suspending-instances">

        <title>Pausing and Suspending Instances</title>
        <para>Since the release of the API in its 1.1 version, it is
            possible to pause and suspend instances.</para>
        <warning>
            <para> Pausing and Suspending instances only apply to
                KVM-based hypervisors and XenServer/XCP Hypervisors.
            </para>
        </warning>
        <para> Pause/ Unpause : Stores the content of the VM in memory
            (RAM).</para>
        <para>Suspend/ Resume : Stores the content of the VM on
            disk.</para>
        <para>It can be interesting for an administrator to suspend
            instances, if a maintenance is planned; or if the instance
            are not frequently used. Suspending an instance frees up
            memory and vCPUS, while pausing keeps the instance
            running, in a "frozen" state. Suspension could be compared
            to an "hibernation" mode.</para>
        <section xml:id="pausing-instance">
            <title>Pausing instance</title>
            <para>To pause an instance :</para>
            <screen>nova pause $server-id </screen>
            <para>To resume a paused instance :</para>
            <screen>nova unpause $server-id </screen>
        </section>
        <section xml:id="suspending-instance">
            <title>Suspending instance</title>
            <para> To suspend an instance :</para>
            <screen>nova suspend $server-id </screen>
            <para>To resume a suspended instance :</para>
            <screen>nova resume $server-id </screen>
        </section>
    </section>
    <section xml:id="specify-host-to-boot-instances-on">
        <title>Select a specific host to boot instances on</title>
        <para>With the Folsom release it was not possible to
            indicate a specific compute host to use to boot an
            instance with the --force_hosts parameter. See bug <link
                xlink:href="https://bugs.launchpad.net/ubuntu/+source/nova/+bug/1061665"
                >1061665</link> for further discussion.</para>
        <para>With the Grizzly release, the provider can change the
            policy to enable users to choose a specific host to launch
            a VM instance upon using the create:forced_host setting
            within <filename>policy.json</filename> on certain roles,
            such as an admin role.</para>
        <para>Once the role is in place, users with the correct role
            can retrieve the current active node by running:</para>
        <programlisting>
            $nova-manage service list
            server1 nova-network enabled  :- ) 2011-04-06 17:05:11
            server1 nova-compute enabled  :- ) 2011-04-06 17:05:13
            server1 nova-scheduler enabled :- ) 2011-04-06 17:05:17
            server2 nova-compute disabled  :- ) 2011-04-06 17:05:19</programlisting>
        <para>We see here our "server2" runs as a node. You can now
            select the host on which the instance would be spawned,
            using the "--hint" flag:</para>
        <para>
            <screen><prompt>$</prompt> <userinput>nova boot --image 1 --flavor 2 --key_name test --hint force_hosts=server2 my-first-server</userinput></screen></para>
    </section>
    <section xml:id="specify-zone-to-boot-instances-on">
        <title>Select a specific zone to boot instances on</title>
        <para> It is possible to specify which availability zone to
            run the instance on using the nova client. In order to use
            such feature, make sure you are using an admin account. </para>
        <para>You can determine the current active zone by looking at
            the nova.conf file for the compute node and seeing the
            node_availability_zone=yyyyy where yyyyy is the name of
            the zone the host is affiliated with. </para>
        <para>We see here our "server2" lives in the "nova"
            availabilty zone. You can now select the host on which the
            instance would be spawned, using the "--availability-zone"
            parameter as an admin. </para>
        <note>
            <para>The --force_hosts scheduler hint has been replaced
                with --availability_zone in the Folsom release.
            </para>
        </note>
        <para>
            <screen><prompt>$</prompt> <userinput>nova boot --image &lt;uuid&gt; --flavor m1.tiny --key_name test --availability-zone nova:server2</userinput></screen>
        </para>
    </section>
    <section xml:id="creating-custom-images">
        <para>There are several pre-built images for OpenStack
            available from various sources. You can download such
            images and use them to get familiar with OpenStack. You
            can refer to <link
                xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/starting-images.html"
                >http://docs.openstack.org/trunk/openstack-compute/admin/content/starting-images.html</link>
            for details on using such images.</para>
        <para>For any production deployment, you may like to have the
            ability to bundle custom images, with a custom set of
            applications or configuration. This chapter will guide you
            through the process of creating Linux images of Debian and
            Redhat based distributions from scratch. We have also
            covered an approach to bundling Windows images.</para>
        <para>There are some minor differences in the way you would
            bundle a Linux image, based on the distribution. Ubuntu
            makes it very easy by providing cloud-init package, which
            can be used to take care of the instance configuration at
            the time of launch. cloud-init handles importing ssh keys
            for password-less login, setting hostname etc. The
            instance acquires the instance specific configuration from
            Nova-compute by connecting to a meta data interface
            running on 169.254.169.254.</para>
        <para>While creating the image of a distro that does not have
            cloud-init or an equivalent package, you may need to take
            care of importing the keys etc. by running a set of
            commands at boot time from rc.local.</para>
        <para>The process used for Ubuntu and Fedora is largely the
            same with a few minor differences, which are explained
            below.</para>

        <para>In both cases, the documentation below assumes that you
            have a working KVM installation to use for creating the
            images. We are using the machine called
            &#8216;client1&#8242; as explained in the chapter on
            &#8220;Installation and Configuration&#8221; for this
            purpose.</para>
        <para>The approach explained below will give you disk images
            that represent a disk without any partitions. Nova-compute
            can resize such disks ( including resizing the file
            system) based on the instance type chosen at the time of
            launching the instance. These images cannot have
            &#8216;bootable&#8217; flag and hence it is mandatory to
            have associated kernel and ramdisk images. These kernel
            and ramdisk images need to be used by nova-compute at the
            time of launching the instance.</para>
        <para>However, we have also added a small section towards the
            end of the chapter about creating bootable images with
            multiple partitions that can be used by nova to launch an
            instance without the need for kernel and ramdisk images.
            The caveat is that while nova-compute can re-size such
            disks at the time of launching the instance, the file
            system size is not altered and hence, for all practical
            purposes, such disks are not re-sizable.</para>
        <section xml:id="creating-a-linux-image">
            <title>Creating a Linux Image &#8211; Ubuntu &amp;
                Fedora</title>

            <para>The first step would be to create a raw image on
                Client1. This will represent the main HDD of the
                virtual machine, so make sure to give it as much space
                as you will need.</para>
            <screen>
kvm-img create -f raw server.img 5G
</screen>

            <simplesect>
                <title>OS Installation</title>
                <para>Download the iso file of the Linux distribution
                    you want installed in the image. The instructions
                    below are tested on Ubuntu 11.04 Natty Narwhal
                    64-bit server and Fedora 14 64-bit. Most of the
                    instructions refer to Ubuntu. The points of
                    difference between Ubuntu and Fedora are mentioned
                    wherever required.</para>
                <screen>
wget http://releases.ubuntu.com/natty/ubuntu-11.04-server-amd64.iso
</screen>
                <para>Boot a KVM Instance with the OS installer ISO in
                    the virtual CD-ROM. This will start the
                    installation process. The command below also sets
                    up a VNC display at port 0</para>
                <screen>
sudo kvm -m 256 -cdrom ubuntu-11.04-server-amd64.iso -drive   file=server.img,if=scsi,index=0 -boot d -net nic -net user -nographic  -vnc :0
</screen>
                <para>Connect to the VM through VNC (use display
                    number :0) and finish the installation.</para>
                <para>For Example, where 10.10.10.4 is the IP address
                    of client1:</para>
                <screen>
 vncviewer 10.10.10.4 :0
</screen>
                <para>During the installation of Ubuntu, create a
                    single ext4 partition mounted on &#8216;/&#8217;.
                    Do not create a swap partition.</para>
                <para>In the case of Fedora 14, the installation will
                    not progress unless you create a swap partition.
                    Please go ahead and create a swap
                    partition.</para>

                <para>After finishing the installation, relaunch the
                    VM by executing the following command.</para>
                <screen>
sudo kvm -m 256 -drive file=server.img,if=scsi,index=0 -boot c -net nic -net user -nographic -vnc :0
</screen>
                <para>At this point, you can add all the packages you
                    want to have installed, update the installation,
                    add users and make any configuration changes you
                    want in your image.</para>
                <para>At the minimum, for Ubuntu you may run the
                    following commands</para>
                <screen>
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install openssh-server cloud-init
</screen>
                <para>For Fedora run the following commands as
                    root</para>
                <screen>

yum update
yum install openssh-server
chkconfig sshd on
</screen>
                <para>Also remove the network persistence rules from
                    /etc/udev/rules.d as their presence will result in
                    the network interface in the instance coming up as
                    an interface other than eth0.</para>
                <screen>
sudo rm -rf /etc/udev/rules.d/70-persistent-net.rules
</screen>
                <para>Shutdown the Virtual machine and proceed with
                    the next steps.</para>
            </simplesect>
            <simplesect>
                <title>Extracting the EXT4 partition</title>
                <para>The image that needs to be uploaded to OpenStack
                    needs to be an ext4 filesystem image. Here are the
                    steps to create a ext4 filesystem image from the
                    raw image i.e server.img</para>
                <screen>
sudo losetup  -f  server.img
sudo losetup -a
</screen>
                <para>You should see an output like this:</para>
                <screen>
/dev/loop0: [0801]:16908388 ($filepath)
</screen>
                <para>Observe the name of the loop device ( /dev/loop0
                    in our setup) when $filepath is the path to the
                    mounted .raw file.</para>
                <para>Now we need to find out the starting sector of
                    the partition. Run:</para>
                <screen>
sudo fdisk -cul /dev/loop0
</screen>
                <para>You should see an output like this:</para>

                <screen>
Disk /dev/loop0: 5368 MB, 5368709120 bytes
149 heads, 8 sectors/track, 8796 cylinders, total 10485760 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00072bd4
Device Boot      Start         End      Blocks   Id  System
/dev/loop0p1   *        2048    10483711     5240832   83  Linux
</screen>
                <para>Make a note of the starting sector of the
                    /dev/loop0p1 partition i.e the partition whose ID
                    is 83. This number should be multiplied by 512 to
                    obtain the correct value. In this case: 2048 x 512
                    = 1048576</para>
                <para>Unmount the loop0 device:</para>
                <screen>
sudo losetup -d /dev/loop0
</screen>
                <para>Now mount only the partition(/dev/loop0p1) of
                    server.img which we had previously noted down, by
                    adding the -o parameter with value previously
                    calculated value</para>
                <screen>
sudo losetup -f -o 1048576 server.img
sudo losetup -a
</screen>
                <para>You&#8217;ll see a message like this:</para>
                <screen>
/dev/loop0: [0801]:16908388 ($filepath) offset 1048576
</screen>
                <para>Make a note of the mount point of our
                    device(/dev/loop0 in our setup) when $filepath is
                    the path to the mounted .raw file.</para>
                <para>Copy the entire partition to a new .raw
                    file</para>
                <screen>
sudo dd if=/dev/loop0 of=serverfinal.img
</screen>
                <para>Now we have our ext4 filesystem image i.e
                    serverfinal.img</para>

                <para>Unmount the loop0 device</para>
                <screen>
sudo losetup -d /dev/loop0
</screen>
            </simplesect>
            <simplesect>
                <title>Tweaking /etc/fstab</title>
                <para>You will need to tweak /etc/fstab to make it
                    suitable for a cloud instance. Nova-compute may
                    resize the disk at the time of launch of instances
                    based on the instance type chosen. This can make
                    the UUID of the disk invalid. Hence we have to use
                    File system label as the identifier for the
                    partition instead of the UUID.</para>
                <para>Loop mount the serverfinal.img, by
                    running</para>
                <screen>
sudo mount -o loop serverfinal.img /mnt
</screen>
                <para>Edit /mnt/etc/fstab and modify the line for
                    mounting root partition(which may look like the
                    following)</para>

                <programlisting>
UUID=e7f5af8d-5d96-45cc-a0fc-d0d1bde8f31c  /               ext4    errors=remount-ro  0       1
</programlisting>
                <para>to</para>
                <programlisting>

LABEL=uec-rootfs              /          ext4           defaults     0    0
                </programlisting>
            </simplesect>
            <simplesect>
                <title>Fetching Metadata in Fedora</title>
                <para>An instance must perform several steps on
                    startup by interacting with the metadata service
                    (e.g., retrieve ssh public key, execute user data
                    script). When building a Fedora image, there are
                    several options for implementing this
                    functionality, including:<itemizedlist>
                        <listitem>
                            <para>Install a <link
                                   xlink:href="http://koji.fedoraproject.org/koji/packageinfo?packageID=12620"
                                   >cloud-initRPM </link> , which is a
                                port of the Ubuntu cloud-init
                                package.</para>
                        </listitem>
                        <listitem>
                            <para>Install <link
                                   xlink:href="https://github.com/yahoo/Openstack-Condense"
                                   >Condenser</link>, an alternate
                                version of cloud-init.</para>
                        </listitem>
                        <listitem>
                            <para>Modify
                                   <filename>/etc/rc.local</filename>
                                to fetch desired information from the
                                metadata service, as described
                                below.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>To fetch the ssh public key and add it to the
                    root account, edit the
                        <filename>/etc/rc.local</filename> file and
                    add the following lines before the line “touch
                    /var/lock/subsys/local”</para>
                <programlisting>depmod -a
modprobe acpiphp

# simple attempt to get the user ssh key using the meta-data service
mkdir -p /root/.ssh
echo &gt;&gt; /root/.ssh/authorized_keys
curl -m 10 -s http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key | grep 'ssh-rsa' &gt;&gt; /root/.ssh/authorized_keys
echo &quot;AUTHORIZED_KEYS:&quot;
echo &quot;************************&quot;
cat /root/.ssh/authorized_keys
echo &quot;************************&quot;
</programlisting>
                <note>
                    <para>The above script only retrieves the ssh
                        public key from the metadata server. It does
                        not retrieve <emphasis role="italic">user
                            data</emphasis>, which is optional data
                        that can be passed by the user when requesting
                        a new instance. User data is often used for
                        running a custom script when an instance comes
                        up.</para>
                    <para>As the OpenStack metadata service is
                        compatible with version 2009-04-04 of the
                        Amazon EC2 metadata service, consult the
                        Amazon EC2 documentation on <link
                            xlink:href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html"
                            >Using Instance Metadata</link> for
                        details on how to retrieve user data.</para>
                </note>
            </simplesect>
        </section>
        <simplesect>
            <title>Kernel and Initrd for OpenStack</title>

            <para>Copy the kernel and the initrd image from /mnt/boot
                to user home directory. These will be used later for
                creating and uploading a complete virtual image to
                OpenStack.</para>
            <screen>
sudo cp /mnt/boot/vmlinuz-2.6.38-7-server /home/localadmin
sudo cp /mnt/boot/initrd.img-2.6.38-7-server /home/localadmin
</screen>
            <para>Unmount the Loop partition</para>
            <screen>
sudo umount  /mnt
</screen>
            <para>Change the filesystem label of serverfinal.img to
                &#8216;uec-rootfs&#8217;</para>
            <screen>
sudo tune2fs -L uec-rootfs serverfinal.img
</screen>
            <para>Now, we have all the components of the image ready
                to be uploaded to OpenStack imaging server.</para>
        </simplesect>
        <simplesect>
            <title>Registering with OpenStack</title>
            <para>The last step would be to upload the images to
                OpenStack Image Service. The files that need to be
                uploaded for the above sample setup of Ubuntu are:
                vmlinuz-2.6.38-7-server, initrd.img-2.6.38-7-server,
                serverfinal.img</para>
            <para>Run the following command</para>
            <screen>
uec-publish-image -t image --kernel-file vmlinuz-2.6.38-7-server --ramdisk-file initrd.img-2.6.38-7-server amd64 serverfinal.img bucket1
</screen>
            <para>For Fedora, the process will be similar. Make sure
                that you use the right kernel and initrd files
                extracted above.</para>
            <para>The uec-publish-image command returns the prompt
                back immediately. However, the upload process takes
                some time and the images will be usable only after the
                process is complete. You can keep checking the status
                using the command <command>nova image-list</command>
                as mentioned below.</para>
        </simplesect>
        <simplesect>
            <title>Bootable Images</title>
            <para>You can register bootable disk images without
                associating kernel and ramdisk images. When you do not
                want the flexibility of using the same disk image with
                different kernel/ramdisk images, you can go for
                bootable disk images. This greatly simplifies the
                process of bundling and registering the images.
                However, the caveats mentioned in the introduction to
                this chapter apply. Please note that the instructions
                below use server.img and you can skip all the
                cumbersome steps related to extracting the single ext4
                partition.</para>
            <screen>
glance image-create --name="My Server" --is-public=true --container-format=ovf --disk-format=raw &lt; server.img
</screen>
        </simplesect>
        <simplesect>
            <title>Image Listing</title>
            <para>The status of the images that have been uploaded can
                be viewed by using nova image-list command. The output
                should like this:</para>
            <screen>nova image-list</screen>
            <programlisting>
+----+---------------------------------------------+--------+
| ID |                     Name                    | Status |
+----+---------------------------------------------+--------+
| 6  | ttylinux-uec-amd64-12.1_2.6.35-22_1-vmlinuz | ACTIVE |
| 7  | ttylinux-uec-amd64-12.1_2.6.35-22_1-initrd  | ACTIVE |
| 8  | ttylinux-uec-amd64-12.1_2.6.35-22_1.img     | ACTIVE |
+----+---------------------------------------------+--------+
</programlisting>
        </simplesect>
    </section>
    <section xml:id="creating-a-windows-image">
        <title>Creating a Windows Image</title>
        <para>The first step would be to create a raw image on
            Client1, this will represent the main HDD of the virtual
            machine, so make sure to give it as much space as you will
            need.</para>
        <screen>
kvm-img create -f raw windowsserver.img 20G
</screen>
        <para>OpenStack presents the disk using aVIRTIO interface
            while launching the instance. Hence the OS needs to have
            drivers for VIRTIO. By default, the Windows Server 2008
            ISO does not have the drivers for VIRTIO. Sso download a
            virtual floppy drive containing VIRTIO drivers from the
            following location</para>
        <para><link
                xlink:href="http://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/"
                >http://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/</link></para>
        <para>and attach it during the installation</para>
        <para>Start the installation by running</para>
        <screen>
sudo kvm -m 2048 -cdrom win2k8_dvd.iso -drive file=windowsserver.img,if=virtio -drive file=virtio-win-0.1-22.iso,index=3,media=cdrom -net nic,model=virtio -net user -nographic -vnc :0
</screen>
        <para>When the installation prompts you to choose a hard disk
            device you won’t see any devices available. Click on “Load
            drivers” at the bottom left and load the drivers from
            A:\i386\Win2008</para>
        <para>After the Installation is over, boot into it once and
            install any additional applications you need to install
            and make any configuration changes you need to make. Also
            ensure that RDP is enabled as that would be the only way
            you can connect to a running instance of Windows. Windows
            firewall needs to be configured to allow incoming ICMP and
            RDP connections.</para>
        <para>For OpenStack to allow incoming RDP Connections, use
            commands to open up port 3389.</para>
        <para>Shut-down the VM and upload the image to
            OpenStack</para>
        <screen>
glance image-create --name="My WinServer" --is-public=true --container-format=ovf --disk-format=raw &lt; windowsserver.img
</screen>
    </section>
    <section xml:id="creating-images-from-running-instances">
        <title>Creating images from running instances with KVM and
            Xen</title>
        <para> It is possible to create an image from a running
            instance on KVM and Xen. This is a convenient way to spawn
            pre-configured instances; update them according to your
            needs ; and re-image the instances. The process to create
            an image from a running instance is quite simple : <itemizedlist>
                <listitem>
                    <para>
                        <emphasis role="bold">Pre-requisites
                            (KVM)</emphasis>
                    </para>
                    <para> In order to use the feature properly, you
                        will need <command>qemu-img</command> 0.14 or
                        greater. The imaging feature uses the copy
                        from a snapshot for image files. (e.g qcow-img
                        convert -f qcow2 -O qcow2 -s $snapshot_name
                        $instance-disk).</para>
                    <para>On Debian-like distros, you can check the
                        version by running :
                        <screen>dpkg -l | grep qemu</screen></para>
                    <programlisting>
ii  qemu                            0.14.0~rc1+noroms-0ubuntu4~ppalucid1            dummy transitional pacakge from qemu to qemu
ii  qemu-common                     0.14.0~rc1+noroms-0ubuntu4~ppalucid1            qemu common functionality (bios, documentati
ii  qemu-kvm                        0.14.0~rc1+noroms-0ubuntu4~ppalucid1            Full virtualization on i386 and amd64 hardwa
                   </programlisting>
                    <para>Images can only be created from running
                        instances if Compute is configured to use
                        qcow2 images, which is the default setting.
                        You can explicitly enable the use of qcow2
                        images by adding the following line to
                            <filename>nova.conf</filename>:
                        <programlisting>
use_cow_images=true
                   </programlisting>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Write data to
                            disk</emphasis></para>
                    <para> Before creating the image, we need to make
                        sure we are not missing any buffered content
                        that wouldn't have been written to the
                        instance's disk. In order to resolve that ;
                        connect to the instance and run
                            <command>sync</command> then exit. </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Create the
                            image</emphasis>
                    </para>
                    <para> In order to create the image, we first need
                        obtain the server id :
                        <screen>nova list</screen><programlisting>
+-----+------------+--------+--------------------+
|  ID |    Name    | Status |      Networks      |
+-----+------------+--------+--------------------+
| 116 | Server 116 | ACTIVE | private=20.10.0.14 |
+-----+------------+--------+--------------------+
                       </programlisting>
                        Based on the output, we run :
                        <screen>nova image-create 116 Image-116</screen>
                        The command will then perform the image
                        creation (by creating qemu snapshot) and will
                        automatically upload the image to your
                        repository. <note>
                            <para> The image that will be created will
                                be flagged as "Private" (For glance :
                                --is-public=False). Thus, the image
                                will be available only for the tenant.
                            </para>
                        </note>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Check image
                            status</emphasis>
                    </para>
                    <para> After a while the image will turn from a
                        "SAVING" state to an "ACTIVE" one.
                        <screen>nova image-list</screen> will allow
                        you to check the progress : <screen>nova image-list </screen>
                        <programlisting>
+----+---------------------------------------------+--------+
| ID |                     Name                    | Status |
+----+---------------------------------------------+--------+
| 20 | Image-116                                   | ACTIVE |
| 6  | ttylinux-uec-amd64-12.1_2.6.35-22_1-vmlinuz | ACTIVE |
| 7  | ttylinux-uec-amd64-12.1_2.6.35-22_1-initrd  | ACTIVE |
| 8  | ttylinux-uec-amd64-12.1_2.6.35-22_1.img     | ACTIVE |
+----+---------------------------------------------+--------+
                       </programlisting>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Create an instance from
                            the image</emphasis>
                    </para>
                    <para>You can now create an instance based on this
                        image as you normally do for other images
                        :<screen>nova boot --flavor 1 --image 20 New_server</screen>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold"> Troubleshooting
                        </emphasis>
                    </para>
                    <para> Mainly, it wouldn't take more than 5
                        minutes in order to go from a "SAVING" to the
                        "ACTIVE" state. If this takes longer than five
                        minutes, here are several hints: </para>
                    <para>- The feature doesn't work while you have
                        attached a volume (via nova-volume) to the
                        instance. Thus, you should dettach the volume
                        first, create the image, and re-mount the
                        volume.</para>
                    <para>- Make sure the version of qemu you are
                        using is not older than the 0.14 version. That
                        would create "unknown option -s" into
                        nova-compute.log.</para>
                    <para>- Look into nova-api.log and
                        nova-compute.log for extra information.</para>
                </listitem>
            </itemizedlist>
        </para>
    </section>
    <section xml:id="replicating-images">
        <title>Replicating images across multiple data centers </title>
        <para>The image service comes with a tool called
                <command>glance-replicator</command> that can be used
            to populate a new glance server using the images stored in
            an existing glance server. The images in the replicated
            glance server preserve the uuids, metadata, and image data
            from the original. Running the tool will output a set of
            commands that it
            supports:<screen>$ <userinput>glance-replicator</userinput>
<computeroutput>Usage: glance-replicator &lt;command> [options] [args]

Commands:

    help &lt;command>  Output help for one of the commands below

    compare         What is missing from the slave glance?
    dump            Dump the contents of a glance instance to local disk.
    livecopy        Load the contents of one glance instance into another.
    load            Load the contents of a local directory into glance.
    size            Determine the size of a glance instance if dumped to disk.

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -c CHUNKSIZE, --chunksize=CHUNKSIZE
                        Amount of data to transfer per HTTP write
  -d, --debug           Print debugging information
  -D DONTREPLICATE, --dontreplicate=DONTREPLICATE
                        List of fields to not replicate
  -m, --metaonly        Only replicate metadata, not images
  -l LOGFILE, --logfile=LOGFILE
                        Path of file to log to
  -s, --syslog          Log to syslog instead of a file
  -t TOKEN, --token=TOKEN
                        Pass in your authentication token if you have one. If
                        you use this option the same token is used for both
                        the master and the slave.
  -M MASTERTOKEN, --mastertoken=MASTERTOKEN
                        Pass in your authentication token if you have one.
                        This is the token used for the master.
  -S SLAVETOKEN, --slavetoken=SLAVETOKEN
                        Pass in your authentication token if you have one.
                        This is the token used for the slave.
  -v, --verbose         Print more verbose output</computeroutput></screen></para>
        <para>The replicator supports the following commands:</para>
        <simplesect>
            <title>livecopy: Load the contents of one glance instance
                into another</title>
            <para>
                <screen><userinput>glance-replicator livecopy <replaceable>fromserver:port</replaceable> <replaceable>toserver:port</replaceable></userinput></screen>
            </para>
            <para>
                <itemizedlist>
                    <listitem>
                        <para><replaceable>fromserver:port</replaceable>:
                            the location of the master glance
                            instance</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>toserver:port</replaceable>:
                            the location of the slave glance instance.
                        </para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>Take a copy of the fromserver, and dump it onto the
                toserver. Only images visible to the user running the
                replicator will be copied if glance is configured to
                use the Identity service (keystone) for
                authentication. Only images active on
                    <replaceable>fromserver </replaceable>are copied
                across. The copy is done "on-the-wire" so there are no
                large temporary files on the machine running the
                replicator to clean up.</para>
        </simplesect>
        <simplesect>
            <title>dump: Dump the contents of a glance instance to
                local disk</title>
            <para>
                <screen><userinput>glance-replicator dump <replaceable>server:port</replaceable> <replaceable>path</replaceable></userinput></screen>
            </para>
            <para><itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance instance.
                        </para>
                    </listitem>
                    <listitem>
                        <para><replaceable>path</replaceable>: a
                            directory on disk to contain the data.
                        </para>
                    </listitem>
                </itemizedlist>Do the same thing as
                    <command>livecopy</command>, but dump the contents
                of the glance server to a directory on disk. This
                includes metadata and image data. Depending on the
                size of the local glance repository, the resulting
                dump may consume a large amount of local storage.
                Therefore, we recommend you use the
                    <command>size</command> comamnd first to determine
                the size of the resulting dump.</para>
        </simplesect>
        <simplesect>
            <title>load: Load a directory created by the dump command
                into a glance server</title>
            <para>
                <screen><userinput>glance-replicator load server:port path</userinput></screen>
                <itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance
                            instance.</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>path</replaceable>: a
                            directory on disk containing the
                            data.</para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>Load the contents of a local directory into glance. </para>
            <para>The <command>dump</command> and
                    <command>load</command> are useful when
                replicating across two glance servers where a direct
                connection across the two glance hosts is impossible
                or too slow.</para>
        </simplesect>
        <simplesect>
            <title>compare: Compare the contents of two glance
                servers</title>
            <para><screen><userinput>glance-replicator compare <replaceable>fromserver:port</replaceable> <replaceable>toserver:port</replaceable></userinput></screen><itemizedlist>
                    <listitem>
                        <para><replaceable>fromserver:port</replaceable>:
                            the location of the master glance
                            instance.</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>
                                toserver:port</replaceable>: the
                            location of the slave glance instance.
                        </para>
                    </listitem>
                </itemizedlist>The <command>compare</command> command
                will show you the differences between the two servers,
                which is effectively a dry run of the
                    <command>livecopy</command> command.</para>
        </simplesect>
        <simplesect>
            <title>size: Determine the size of a glance instance if
                dumped to disk</title>
            <para><screen><userinput>glance-replicator size</userinput></screen><itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance
                            instance.</para>
                    </listitem>
                </itemizedlist>The <command>size</command> command
                will tell you how much disk is going to be used by
                image data in either a <command>dump</command> or a
                    <command>livecopy</command>. Note that this will
                provide raw number of bytes that would be written to
                the destination, it has no information about the
                redundancy costs associated with glance-registry
                back-ends that use replication for redundancy, such as
                Swift or Ceph.</para>
        </simplesect>
        <simplesect>
            <title>Example using livecopy</title>
            <para>Assuming you have a primary glance service running
                on a node called
                    <literal>primary.example.com</literal> with
                glance-api service running on port
                    <literal>9292</literal> (the default port) and you
                want to replicate its contents to a secondary glance
                service running on a node called
                    <literal>secondary.example.com</literal>, also on
                port 9292, you will first need to get authentication
                tokens from keystone for the primary and secondary
                glance server and then you can use the
                    <literal>glance-replicator livecopy</literal>
                command.</para>
            <para>The following example assumes that you have a
                credentials file for your primary cloud called
                    <filename>primary.openrc</filename> and one for
                your secondary cloud called
                    <filename>secondary.openrc</filename>.<screen><prompt>$</prompt> <userinput>source primary.openrc</userinput>
<prompt>$</prompt> <userinput>keystone token-get</userinput>
<computeroutput>+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
| expires   | 2012-11-16T03:13:08Z             |
| id        | 8a5d3afb5095430891f33f69a2791463 |
| tenant_id | dba21b41af584daeac5782ca15a77a25 |
| user_id   | add2ece6b1f94866994d3a3e3beb3d47 |
+-----------+----------------------------------+</computeroutput>
<prompt>$</prompt> <userinput>PRIMARY_AUTH_TOKEN=<replaceable>8e97fa8bcf4443cfbd3beb9079c7142f </replaceable></userinput>
<prompt>$</prompt> <userinput>source secondary.openrc</userinput>
<prompt>$</prompt> <userinput>keystone token-get</userinput>
<computeroutput>+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
| expires   | 2012-11-16T03:13:08Z             |
| id        | 29f777ac2c9b41a6b4ee9c3e6b85f98a |
| tenant_id | fbde89d638d947a19545b0f387ffea4d |
| user_id   | 4a7a48e7d62e4b428c78d02c1968ca7b |
+-----------+----------------------------------+</computeroutput>
<prompt>$</prompt> <userinput>SECONDARY_AUTH_TOKEN=<replaceable>29f777ac2c9b41a6b4ee9c3e6b85f98a</replaceable></userinput>
<prompt>$</prompt> <userinput>glance-replicator livecopy primary.example.com:9292 secondary.example.com:9292 -M ${PRIMARY_AUTH_TOKEN} -S ${SECONDARY_AUTH_TOKEN} </userinput></screen></para>
        </simplesect>
    </section>
</chapter>
