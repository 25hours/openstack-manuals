<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_image_mgmt">
    <title>Image Management</title>
    <para>You can use OpenStack Image Services for discovering,
        registering, and retrieving virtual machine images. The
        service includes a RESTful API that allows users to query VM
        image metadata and retrieve the actual image with HTTP
        requests, or you can use a client class in your Python code to
        accomplish the same tasks. </para>
    <para> VM images made available through OpenStack Image Service
        can be stored in a variety of locations from simple file
        systems to object-storage systems like the OpenStack Object
        Storage project, or even use S3 storage either on its own or
        through an OpenStack Object Storage S3 interface.</para>
    <para>The backend stores that OpenStack Image Service can work
        with are as follows:</para>
    <itemizedlist>
        <listitem>
            <para>OpenStack Object Storage - OpenStack Object Storage
                is the highly-available object storage project in
                OpenStack.</para>
        </listitem>

        <listitem>
            <para>Filesystem - The default backend that OpenStack
                Image Service uses to store virtual machine images is
                the filesystem backend. This simple backend writes
                image files to the local filesystem.</para>
        </listitem>

        <listitem>
            <para>S3 - This backend allows OpenStack Image Service to
                store virtual machine images in Amazon’s S3
                service.</para>
        </listitem>

        <listitem>
            <para>HTTP - OpenStack Image Service can read virtual
                machine images that are available via HTTP somewhere
                on the Internet. This store is readonly.</para>
        </listitem>
    </itemizedlist>

    <para>This chapter assumes you have a working installation of the
        Image Service, with a working endpoint and users created in
        the Identity service, plus you have sourced the environment
        variables required by the nova client and glance
        client.</para>
    <xi:include href="tenant-specific-image-storage.xml"/>
    <xi:include href="adding-images.xml"/>
    <section xml:id="starting-images">
        <title>Getting virtual machine images</title>
        <?dbhtml stop-chunking?>
        <para>Refer to the <link
                xlink:href="http://docs.openstack.org/grizzly/openstack-image/content/ch_obtaining_images.html"
                >OpenStack Virtual Machine Image Guide</link> for detailed
            information.</para>
    <section xml:id="tool-support-creating-new-images">
        <?dbhtml stop-chunking?>
        <title>Tool support for creating images</title>
        <para>There are several open-source third-party tools
            available that simplify the task of creating new virtual
            machine images. Refer to the <link
                xlink:href="http://docs.openstack.org/grizzly/openstack-image/content/ch_creating_images_automatically.html"
                >OpenStack Virtual Machine Image Guide</link> for detailed
            information.</para>
        </section>
    <section xml:id="image-customizing-what-you-need-to-know">
        <?dbhtml stop-chunking?>
        <title>Customizing an image for OpenStack</title>
        <para>The <link
            xlink:href="http://docs.openstack.org/grizzly/openstack-image/content/ch_openstack_images.html"
            >OpenStack Virtual Machine Image Guide</link> describes what customizations you should to
            your image to maximize compatibility with
            OpenStack.</para>
        </section>
    <section xml:id="manually-creating-qcow2-images">
        <title>Creating raw or QCOW2 images</title>
        <para>This <link
            xlink:href="http://docs.openstack.org/grizzly/openstack-image/content/ch_creating_images_manually.html"
            >OpenStack Virtual Machine Image Guide</link> describes how to create a raw or QCOW2
            image from a Linux installation ISO file. Raw images are
            the simplest image file format and are supported by all of
            the hypervisors. QCOW2 images have several advantages over
            raw images. They take up less space than raw images
            (growing in size as needed), and they support snapshots.<note>
                <para>QCOW2 images are only supported with KVM and
                    QEMU hypervisors.</para>
            </note></para>
    </section>
    </section>
    <section xml:id="booting-a-test-image">
        <title>Booting a test image</title>
        <para>The following assumes you are using QEMU or KVM in your
            deployment.</para>
        <para>Download a CirrOS test image:</para>
        <screen>
<prompt>$</prompt> <userinput>wget https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk.img</userinput>
            </screen>
        <para>Add the image to glance:</para>
        <screen>
<prompt>$</prompt> <userinput>name=cirros-0.3-x86_64</userinput>
<prompt>$</prompt> <userinput>image=cirros-0.3.0-x86_64-disk.img</userinput>
<prompt>$</prompt> <userinput>glance image-create --name=$name --is-public=true --container-format=bare --disk-format=qcow2 &lt; $image</userinput>
            </screen>
        <para>Check that adding the image was successful (Status
            should be ACTIVE when the operation is complete):</para>
        <screen>
<prompt>$</prompt> <userinput>nova image-list</userinput>
<computeroutput>
+--------------------------------------+---------------------+--------+--------+
|                  ID                  |         Name        | Status | Server |
+--------------------------------------+---------------------+--------+--------+
| 254c15e1-78a9-4b30-9b9e-2a39b985001c | cirros-0.3.0-x86_64 | ACTIVE |        |
+--------------------------------------+---------------------+--------+--------+
</computeroutput>
            </screen>
        <para>Create a keypair so you can ssh to the instance: </para>
        <screen>
<prompt>$</prompt> <userinput>nova keypair-add test > test.pem</userinput>
<prompt>$</prompt> <userinput>chmod 600 test.pem</userinput>
            </screen>
        <para>In general, you need to use an ssh keypair to log in to
            a running instance, although some images have built-in
            accounts created with associated passwords. However, since
            images are often shared by many users, it is not advised
            to put passwords into the images. Nova therefore supports
            injecting ssh keys into instances before they are booted.
            This allows a user to log in to the instances that he or
            she creates securely. Generally the first thing that a
            user does when using the system is create a keypair. </para>
        <para>Keypairs provide secure authentication to your
            instances. As part of the first boot of a virtual image,
            the private key of your keypair is added to
            authorized_keys file of the login account. Nova generates
            a public and private key pair, and sends the private key
            to the user. The public key is stored so that it can be
            injected into instances. </para>
        <para>Run (boot) a test instance:</para>
        <screen>
<prompt>$</prompt> <userinput>nova boot --image cirros-0.3.0-x86_64 --flavor m1.small --key_name test my-first-server</userinput>
            </screen>
        <para>Here's a description of the parameters used
            above:</para>
        <itemizedlist>
            <listitem>
                <para>
                    <literal>--image</literal>: the name or ID of the
                    image we want to launch, as shown in the output of
                        <command>nova image-list</command></para>
            </listitem>
            <listitem>
                <para>
                    <literal>--flavor</literal>: the name or ID of the
                    size of the instance to create (number of vcpus,
                    available RAM, available storage). View the list
                    of available flavors by running <command>nova
                        flavor-list</command></para>
            </listitem>
            <listitem>
                <para>
                    <literal>-key_name</literal>: the name of the key
                    to inject into the instance at launch.</para>
            </listitem>
        </itemizedlist>
        <para>Check the status of the instance you launched:</para>
        <screen>
<prompt>$</prompt> <userinput>nova list</userinput>
            </screen>
        <para> The instance will go from BUILD to ACTIVE in a short
            time, and you should be able to connect via ssh as
            'cirros' user, using the private key you created. If your
            ssh keypair fails for some reason, you can also log in
            with the default cirros password:
                <literal>cubswin:)</literal></para>
        <screen>
<prompt>$</prompt> <userinput>ipaddress=... # Get IP address from "nova list"</userinput>
<prompt>$</prompt> <userinput>ssh -i test.pem -l cirros $ipaddress</userinput>
        </screen>
        <para>The 'cirros' user is part of the sudoers group, so you
            can escalate to 'root' via the following command when
            logged in to the instance:</para>
        <screen>
<prompt>$</prompt> <userinput>sudo -i</userinput>
        </screen>

    </section>

    <section xml:id="deleting-instances">

        <title>Tearing down (deleting) Instances</title>

        <para>When you are done with an instance, you can tear it down
            using the <command>nova delete</command> command, passing
            either the instance name or instance ID as the argument.
            You can get a listing of the names and IDs of all running
            instances using the <command>nova list</command>. For
            example:</para>
        <screen>
<prompt>$</prompt> <userinput>nova list</userinput>
<computeroutput>
+--------------------------------------+-----------------+--------+----------+
|                  ID                  |       Name      | Status | Networks |
+--------------------------------------+-----------------+--------+----------+
| 8a5d719a-b293-4a5e-8709-a89b6ac9cee2 | my-first-server | ACTIVE |          |
+--------------------------------------+-----------------+--------+----------+
</computeroutput>
<prompt>$</prompt> <userinput>nova delete my-first-server</userinput>
        </screen>

    </section>
    <section xml:id="pausing-and-suspending-instances">

        <title>Pausing and Suspending Instances</title>
        <para>Since the release of the API in its 1.1 version, it is
            possible to pause and suspend instances.</para>
        <warning>
            <para> Pausing and Suspending instances only apply to
                KVM-based hypervisors and XenServer/XCP Hypervisors.
            </para>
        </warning>
        <para> Pause/ Unpause : Stores the content of the VM in memory
            (RAM).</para>
        <para>Suspend/ Resume : Stores the content of the VM on
            disk.</para>
        <para>It can be interesting for an administrator to suspend
            instances, if a maintenance is planned; or if the instance
            are not frequently used. Suspending an instance frees up
            memory and vCPUS, while pausing keeps the instance
            running, in a "frozen" state. Suspension could be compared
            to an "hibernation" mode.</para>
        <section xml:id="pausing-instance">
            <title>Pausing instance</title>
            <para>To pause an instance :</para>
            <screen>nova pause $server-id </screen>
            <para>To resume a paused instance :</para>
            <screen>nova unpause $server-id </screen>
        </section>
        <section xml:id="suspending-instance">
            <title>Suspending instance</title>
            <para> To suspend an instance :</para>
            <screen>nova suspend $server-id </screen>
            <para>To resume a suspended instance :</para>
            <screen>nova resume $server-id </screen>
        </section>
    </section>
    <section xml:id="specify-host-to-boot-instances-on">
        <title>Select a specific host to boot instances on</title>
        <para>If you have the appropriate permissions, you can select the specific host where the
            instance will be launched. This is done using the <literal>--availability_zone
                    <replaceable>zone</replaceable>:<replaceable>host</replaceable></literal>
            arguments to the <command>nova boot</command> command. For example:</para>
        <para>
            <screen><prompt>$</prompt> <userinput>nova boot --image &lt;uuid&gt; --flavor m1.tiny --key_name test --availability-zone nova:server2</userinput></screen>
        </para>
        <para>Starting with the Grizzly release, you can specify which roles are permitted to boot
            an instance to a specific host with the <literal>create:forced_host</literal> setting
            within <filename>policy.json</filename> on the desired roles. By default, only the admin
            role has this setting enabled.</para>
        <para>You can view the list of valid compute hosts by using the <command>nova
                hypervisor-list </command>command, for
            example:<screen><prompt>$</prompt> <userinput>nova hypervisor-list</userinput>
<computeroutput>+----+---------------------+
| ID | Hypervisor hostname |
+----+---------------------+
| 1  | server2             |
| 2  | server3             |
| 3  | server4             |
+----+---------------------+</computeroutput></screen></para>
        <note>
            <para>The <literal>--availability_zone
                        <replaceable>zone</replaceable>:<replaceable>host</replaceable></literal>
                flag replaced the <literal>--force_hosts</literal> scheduler hint for specifying a
                specific host, starting with the Folsom release.</para>
        </note>
    </section>
    <section xml:id="creating-custom-images">
        <title>Creating Custom Images</title>
        <para>There are several pre-built images for OpenStack
            available from various sources. You can download such
            images and use them to get familiar with OpenStack. You
            can refer to <link
                xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/starting-images.html"
                >http://docs.openstack.org/trunk/openstack-compute/admin/content/starting-images.html</link>
            for details on using such images.</para>
        <para>For any production deployment, you may like to have the
            ability to bundle custom images, with a custom set of
            applications or configuration. This chapter will guide you
            through the process of creating Linux images of Debian and
            Redhat based distributions from scratch. We have also
            covered an approach to bundling Windows images.</para>
        <para>There are some minor differences in the way you would
            bundle a Linux image, based on the distribution. Ubuntu
            makes it very easy by providing cloud-init package, which
            can be used to take care of the instance configuration at
            the time of launch. cloud-init handles importing ssh keys
            for password-less login, setting hostname etc. The
            instance acquires the instance specific configuration from
            Nova-compute by connecting to a meta data interface
            running on 169.254.169.254.</para>
        <para>While creating the image of a distro that does not have
            cloud-init or an equivalent package, you may need to take
            care of importing the keys etc. by running a set of
            commands at boot time from rc.local.</para>
        <para>The process used for Ubuntu and Fedora is largely the
            same with a few minor differences, which are explained
            below.</para>

        <para>In both cases, the documentation below assumes that you
            have a working KVM installation to use for creating the
            images. We are using the machine called
            &#8216;client1&#8242; as explained in the chapter on
            &#8220;Installation and Configuration&#8221; for this
            purpose.</para>
        <para>The approach explained below will give you disk images
            that represent a disk without any partitions. Nova-compute
            can resize such disks ( including resizing the file
            system) based on the instance type chosen at the time of
            launching the instance. These images cannot have
            &#8216;bootable&#8217; flag and hence it is mandatory to
            have associated kernel and ramdisk images. These kernel
            and ramdisk images need to be used by nova-compute at the
            time of launching the instance.</para>
        <para>However, we have also added a small section towards the
            end of the chapter about creating bootable images with
            multiple partitions that can be used by nova to launch an
            instance without the need for kernel and ramdisk images.
            The caveat is that while nova-compute can re-size such
            disks at the time of launching the instance, the file
            system size is not altered and hence, for all practical
            purposes, such disks are not re-sizable.</para>
        <section xml:id="creating-a-linux-image">
            <title>Creating a Linux Image &#8211; Ubuntu &amp;
                Fedora</title>

            <para>The first step would be to create a raw image on
                Client1. This will represent the main HDD of the
                virtual machine, so make sure to give it as much space
                as you will need.</para>
            <screen>
kvm-img create -f raw server.img 5G
</screen>

            <simplesect>
                <title>OS Installation</title>
                <para>Download the iso file of the Linux distribution
                    you want installed in the image. The instructions
                    below are tested on Ubuntu 11.04 Natty Narwhal
                    64-bit server and Fedora 14 64-bit. Most of the
                    instructions refer to Ubuntu. The points of
                    difference between Ubuntu and Fedora are mentioned
                    wherever required.</para>
                <screen>
wget http://releases.ubuntu.com/natty/ubuntu-11.04-server-amd64.iso
</screen>
                <para>Boot a KVM instance with the OS installer ISO in
                    the virtual CD-ROM. This will start the
                    installation process. The command below also sets
                    up a VNC display at port 0</para>
                <screen>
sudo kvm -m 256 -cdrom ubuntu-11.04-server-amd64.iso -drive   file=server.img,if=scsi,index=0 -boot d -net nic -net user -nographic  -vnc :0
</screen>
                <para>Connect to the VM through VNC (use display
                    number :0) and finish the installation.</para>
                <para>For example, where 10.10.10.4 is the IP address
                    of client1:</para>
                <screen>
 vncviewer 10.10.10.4 :0
</screen>
                <para>During the installation of Ubuntu, create a
                    single ext4 partition mounted on &#8216;/&#8217;.
                    Do not create a swap partition.</para>
                <para>In the case of Fedora 14, the installation will
                    not progress unless you create a swap partition.
                    Please go ahead and create a swap
                    partition.</para>

                <para>After finishing the installation, relaunch the
                    VM by executing the following command.</para>
                <screen>
sudo kvm -m 256 -drive file=server.img,if=scsi,index=0 -boot c -net nic -net user -nographic -vnc :0
</screen>
                <para>At this point, you can add all the packages you
                    want to have installed, update the installation,
                    add users and make any configuration changes you
                    want in your image.</para>
                <para>At the minimum, for Ubuntu you may run the
                    following commands</para>
                <screen>
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install openssh-server cloud-init
</screen>
                <para>For Fedora run the following commands as
                    root</para>
                <screen>

yum update
yum install openssh-server
chkconfig sshd on
</screen>
                <para>Also remove the network persistence rules from
                    /etc/udev/rules.d as their presence will result in
                    the network interface in the instance coming up as
                    an interface other than eth0.</para>
                <screen>
sudo rm -rf /etc/udev/rules.d/70-persistent-net.rules
</screen>
                <para>Shut down the Virtual machine and proceed with
                    the next steps.</para>
            </simplesect>
            <simplesect>
                <title>Extracting the EXT4 partition</title>
                <para>The image that needs to be uploaded to OpenStack
                    needs to be an ext4 filesystem image. Here are the
                    steps to create an ext4 filesystem image from the
                    raw image i.e server.img</para>
                <screen>
sudo losetup  -f  server.img
sudo losetup -a
</screen>
                <para>You should see an output like this:</para>
                <screen>
/dev/loop0: [0801]:16908388 ($filepath)
</screen>
                <para>Observe the name of the loop device ( /dev/loop0
                    in our setup) when $filepath is the path to the
                    mounted .raw file.</para>
                <para>Now we need to find out the starting sector of
                    the partition. Run:</para>
                <screen>
sudo fdisk -cul /dev/loop0
</screen>
                <para>You should see an output like this:</para>

                <screen>
Disk /dev/loop0: 5368 MB, 5368709120 bytes
149 heads, 8 sectors/track, 8796 cylinders, total 10485760 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00072bd4
Device Boot      Start         End      Blocks   Id  System
/dev/loop0p1   *        2048    10483711     5240832   83  Linux
</screen>
                <para>Make a note of the starting sector of the
                    /dev/loop0p1 partition i.e the partition whose ID
                    is 83. This number should be multiplied by 512 to
                    obtain the correct value. In this case: 2048 x 512
                    = 1048576</para>
                <para>Unmount the loop0 device:</para>
                <screen>
sudo losetup -d /dev/loop0
</screen>
                <para>Now mount only the partition (/dev/loop0p1) of
                    server.img which we had previously noted down, by
                    adding the -o parameter with the previously
                    calculated value</para>
                <screen>
sudo losetup -f -o 1048576 server.img
sudo losetup -a
</screen>
                <para>You&#8217;ll see a message like this:</para>
                <screen>
/dev/loop0: [0801]:16908388 ($filepath) offset 1048576
</screen>
                <para>Make a note of the mount point of our
                    device(/dev/loop0 in our setup) when $filepath is
                    the path to the mounted .raw file.</para>
                <para>Copy the entire partition to a new .raw
                    file</para>
                <screen>
sudo dd if=/dev/loop0 of=serverfinal.img
</screen>
                <para>Now we have our ext4 filesystem image i.e
                    serverfinal.img</para>

                <para>Unmount the loop0 device</para>
                <screen>
sudo losetup -d /dev/loop0
</screen>
            </simplesect>
            <simplesect>
                <title>Tweaking /etc/fstab</title>
                <para>You will need to tweak /etc/fstab to make it
                    suitable for a cloud instance. Nova-compute may
                    resize the disk at the time of launch of instances
                    based on the instance type chosen. This can make
                    the UUID of the disk invalid. Hence we have to use
                    a file system label as the identifier for the
                    partition instead of the UUID.</para>
                <para>Loop mount the serverfinal.img, by
                    running</para>
                <screen>
sudo mount -o loop serverfinal.img /mnt
</screen>
                <para>Edit /mnt/etc/fstab and modify the line for
                    mounting root partition(which may look like the
                    following)</para>

                <programlisting>
UUID=e7f5af8d-5d96-45cc-a0fc-d0d1bde8f31c  /               ext4    errors=remount-ro  0       1
</programlisting>
                <para>to</para>
                <programlisting>

LABEL=uec-rootfs              /          ext4           defaults     0    0
                </programlisting>
            </simplesect>
            <simplesect>
                <title>Fetching Metadata in Fedora</title>
                <para>An instance must perform several steps on
                    startup by interacting with the metadata service
                    (e.g., retrieve ssh public key, execute user data
                    script). When building a Fedora image, there are
                    several options for implementing this
                    functionality, including:<itemizedlist>
                        <listitem>
                            <para>Install a <link
                                   xlink:href="http://koji.fedoraproject.org/koji/packageinfo?packageID=12620"
                                   >cloud-initRPM </link> , which is a
                                port of the Ubuntu cloud-init
                                package.</para>
                        </listitem>
                        <listitem>
                            <para>Install <link
                                   xlink:href="https://github.com/yahoo/Openstack-Condense"
                                   >Condenser</link>, an alternate
                                version of cloud-init.</para>
                        </listitem>
                        <listitem>
                            <para>Modify
                                   <filename>/etc/rc.local</filename>
                                to fetch desired information from the
                                metadata service, as described
                                below.</para>
                        </listitem>
                    </itemizedlist></para>
                <para>To fetch the ssh public key and add it to the
                    root account, edit the
                        <filename>/etc/rc.local</filename> file and
                    add the following lines before the line “touch
                    /var/lock/subsys/local”</para>
                <programlisting>depmod -a
modprobe acpiphp

# simple attempt to get the user ssh key using the meta-data service
mkdir -p /root/.ssh
echo &gt;&gt; /root/.ssh/authorized_keys
curl -m 10 -s http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key | grep 'ssh-rsa' &gt;&gt; /root/.ssh/authorized_keys
echo &quot;AUTHORIZED_KEYS:&quot;
echo &quot;************************&quot;
cat /root/.ssh/authorized_keys
echo &quot;************************&quot;
</programlisting>
                <note>
                    <para>The above script only retrieves the ssh
                        public key from the metadata server. It does
                        not retrieve <emphasis role="italic">user
                            data</emphasis>, which is optional data
                        that can be passed by the user when requesting
                        a new instance. User data is often used for
                        running a custom script when an instance comes
                        up.</para>
                    <para>As the OpenStack metadata service is
                        compatible with version 2009-04-04 of the
                        Amazon EC2 metadata service, consult the
                        Amazon EC2 documentation on <link
                            xlink:href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html"
                            >Using Instance Metadata</link> for
                        details on how to retrieve user data.</para>
                </note>
            </simplesect>
        </section>
        <simplesect>
            <title>Kernel and Initrd for OpenStack</title>

            <para>Copy the kernel and the initrd image from /mnt/boot
                to user home directory. These will be used later for
                creating and uploading a complete virtual image to
                OpenStack.</para>
            <screen>
sudo cp /mnt/boot/vmlinuz-2.6.38-7-server /home/localadmin
sudo cp /mnt/boot/initrd.img-2.6.38-7-server /home/localadmin
</screen>
            <para>Unmount the Loop partition</para>
            <screen>
sudo umount  /mnt
</screen>
            <para>Change the filesystem label of serverfinal.img to
                &#8216;uec-rootfs&#8217;</para>
            <screen>
sudo tune2fs -L uec-rootfs serverfinal.img
</screen>
            <para>Now, we have all the components of the image ready
                to be uploaded to OpenStack imaging server.</para>
        </simplesect>
        <simplesect>
            <title>Registering with OpenStack</title>
            <para>The last step would be to upload the images to
                OpenStack Image Service. The files that need to be
                uploaded for the above sample setup of Ubuntu are:
                vmlinuz-2.6.38-7-server, initrd.img-2.6.38-7-server,
                serverfinal.img</para>
            <para>Run the following command</para>
            <screen>
uec-publish-image -t image --kernel-file vmlinuz-2.6.38-7-server --ramdisk-file initrd.img-2.6.38-7-server amd64 serverfinal.img bucket1
</screen>
            <para>For Fedora, the process will be similar. Make sure
                that you use the right kernel and initrd files
                extracted above.</para>
            <para>The uec-publish-image command returns the prompt
                back immediately. However, the upload process takes
                some time and the images will be usable only after the
                process is complete. You can keep checking the status
                using the command <command>nova image-list</command>
                as mentioned below.</para>
        </simplesect>
        <simplesect>
            <title>Bootable Images</title>
            <para>You can register bootable disk images without
                associating kernel and ramdisk images. When you do not
                want the flexibility of using the same disk image with
                different kernel/ramdisk images, you can go for
                bootable disk images. This greatly simplifies the
                process of bundling and registering the images.
                However, the caveats mentioned in the introduction to
                this chapter apply. Please note that the instructions
                below use server.img and you can skip all the
                cumbersome steps related to extracting the single ext4
                partition.</para>
            <screen>
glance image-create --name="My Server" --is-public=true --container-format=ovf --disk-format=raw &lt; server.img
</screen>
        </simplesect>
        <simplesect>
            <title>Image Listing</title>
            <para>The status of the images that have been uploaded can
                be viewed by using nova image-list command. The output
                should like this:</para>
            <screen>nova image-list</screen>
            <programlisting>
+----+---------------------------------------------+--------+
| ID |                     Name                    | Status |
+----+---------------------------------------------+--------+
| 6  | ttylinux-uec-amd64-12.1_2.6.35-22_1-vmlinuz | ACTIVE |
| 7  | ttylinux-uec-amd64-12.1_2.6.35-22_1-initrd  | ACTIVE |
| 8  | ttylinux-uec-amd64-12.1_2.6.35-22_1.img     | ACTIVE |
+----+---------------------------------------------+--------+
</programlisting>
        </simplesect>
    </section>
    <section xml:id="creating-a-windows-image">
        <title>Creating a Windows Image</title>
        <para>The first step would be to create a raw image on
            Client1, this will represent the main HDD of the virtual
            machine, so make sure to give it as much space as you will
            need.</para>
        <screen>
kvm-img create -f raw windowsserver.img 20G
</screen>
        <para>OpenStack presents the disk using aVIRTIO interface
            while launching the instance. Hence the OS needs to have
            drivers for VIRTIO. By default, the Windows Server 2008
            ISO does not have the drivers for VIRTIO. Sso download a
            virtual floppy drive containing VIRTIO drivers from the
            following location</para>
        <para><link
                xlink:href="http://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/"
                >http://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/</link></para>
        <para>and attach it during the installation</para>
        <para>Start the installation by running</para>
        <screen>
sudo kvm -m 2048 -cdrom win2k8_dvd.iso -drive file=windowsserver.img,if=virtio -drive file=virtio-win-0.1-22.iso,index=3,media=cdrom -net nic,model=virtio -net user -nographic -vnc :0
</screen>
        <para>When the installation prompts you to choose a hard disk
            device you won’t see any devices available. Click on “Load
            drivers” at the bottom left and load the drivers from
            A:\i386\Win2008</para>
        <para>After the Installation is over, boot into it once and
            install any additional applications you need to install
            and make any configuration changes you need to make. Also
            ensure that RDP is enabled as that would be the only way
            you can connect to a running instance of Windows. Windows
            firewall needs to be configured to allow incoming ICMP and
            RDP connections.</para>
        <para>For OpenStack to allow incoming RDP Connections, use
            commands to open up port 3389.</para>
        <para>Shut-down the VM and upload the image to
            OpenStack</para>
        <screen>
glance image-create --name="My WinServer" --is-public=true --container-format=ovf --disk-format=raw &lt; windowsserver.img
</screen>
    </section>
    <section xml:id="creating-images-from-running-instances">
        <title>Creating images from running instances with KVM and
            Xen</title>
        <para> It is possible to create an image from a running
            instance on KVM and Xen. This is a convenient way to spawn
            pre-configured instances; update them according to your
            needs ; and re-image the instances. The process to create
            an image from a running instance is quite simple : <itemizedlist>
                <listitem>
                    <para>
                        <emphasis role="bold">Pre-requisites
                            (KVM)</emphasis>
                    </para>
                    <para> In order to use the feature properly, you
                        will need <command>qemu-img</command> 0.14 or
                        greater. The imaging feature uses the copy
                        from a snapshot for image files. (e.g qcow-img
                        convert -f qcow2 -O qcow2 -s $snapshot_name
                        $instance-disk).</para>
                    <para>On Debian-like distros, you can check the
                        version by running :
                        <screen>dpkg -l | grep qemu</screen></para>
                    <programlisting>
ii  qemu                            0.14.0~rc1+noroms-0ubuntu4~ppalucid1            dummy transitional package from qemu to qemu
ii  qemu-common                     0.14.0~rc1+noroms-0ubuntu4~ppalucid1            qemu common functionality (bios, documentati
ii  qemu-kvm                        0.14.0~rc1+noroms-0ubuntu4~ppalucid1            Full virtualization on i386 and amd64 hardwa
                   </programlisting>
                    <para>Images can only be created from running
                        instances if Compute is configured to use
                        qcow2 images, which is the default setting.
                        You can explicitly enable the use of qcow2
                        images by adding the following line to
                            <filename>nova.conf</filename>:
                        <programlisting>
use_cow_images=true
                   </programlisting>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Write data to
                            disk</emphasis></para>
                    <para> Before creating the image, we need to make
                        sure we are not missing any buffered content
                        that wouldn't have been written to the
                        instance's disk. In order to resolve that ;
                        connect to the instance and run
                            <command>sync</command> then exit. </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Create the
                            image</emphasis>
                    </para>
                    <para> In order to create the image, we first need
                        obtain the server id :
                        <screen>nova list</screen><programlisting>
+-----+------------+--------+--------------------+
|  ID |    Name    | Status |      Networks      |
+-----+------------+--------+--------------------+
| 116 | Server 116 | ACTIVE | private=20.10.0.14 |
+-----+------------+--------+--------------------+
                       </programlisting>
                        Based on the output, we run :
                        <screen>nova image-create 116 Image-116</screen>
                        The command will then perform the image
                        creation (by creating qemu snapshot) and will
                        automatically upload the image to your
                        repository. <note>
                            <para> The image that will be created will
                                be flagged as "Private" (For glance :
                                --is-public=False). Thus, the image
                                will be available only for the tenant.
                            </para>
                        </note>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Check image
                            status</emphasis>
                    </para>
                    <para> After a while the image will turn from a
                        "SAVING" state to an "ACTIVE" one.
                        <screen>nova image-list</screen> will allow
                        you to check the progress : <screen>nova image-list </screen>
                        <programlisting>
+----+---------------------------------------------+--------+
| ID |                     Name                    | Status |
+----+---------------------------------------------+--------+
| 20 | Image-116                                   | ACTIVE |
| 6  | ttylinux-uec-amd64-12.1_2.6.35-22_1-vmlinuz | ACTIVE |
| 7  | ttylinux-uec-amd64-12.1_2.6.35-22_1-initrd  | ACTIVE |
| 8  | ttylinux-uec-amd64-12.1_2.6.35-22_1.img     | ACTIVE |
+----+---------------------------------------------+--------+
                       </programlisting>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold">Create an instance from
                            the image</emphasis>
                    </para>
                    <para>You can now create an instance based on this
                        image as you normally do for other images
                        :<screen>nova boot --flavor 1 --image 20 New_server</screen>
                    </para>
                </listitem>
                <listitem>
                    <para>
                        <emphasis role="bold"> Troubleshooting
                        </emphasis>
                    </para>
                    <para> Mainly, it wouldn't take more than 5
                        minutes in order to go from a "SAVING" to the
                        "ACTIVE" state. If this takes longer than five
                        minutes, here are several hints: </para>
                    <para>- The feature doesn't work while you have
                        attached a volume to the
                        instance. Thus, you should detach the volume
                        first, create the image, and re-mount the
                        volume.</para>
                    <para>- Make sure the version of qemu you are
                        using is not older than the 0.14 version. That
                        would create "unknown option -s" into
                        nova-compute.log.</para>
                    <para>- Look into nova-api.log and
                        nova-compute.log for extra information.</para>
                </listitem>
            </itemizedlist>
        </para>
    </section>
    <section xml:id="replicating-images">
        <title>Replicating images across multiple data centers </title>
        <para>The image service comes with a tool called
                <command>glance-replicator</command> that can be used
            to populate a new glance server using the images stored in
            an existing glance server. The images in the replicated
            glance server preserve the uuids, metadata, and image data
            from the original. Running the tool will output a set of
            commands that it
            supports:<screen>$ <userinput>glance-replicator</userinput>
<computeroutput>Usage: glance-replicator &lt;command> [options] [args]

Commands:

    help &lt;command>  Output help for one of the commands below

    compare         What is missing from the slave glance?
    dump            Dump the contents of a glance instance to local disk.
    livecopy        Load the contents of one glance instance into another.
    load            Load the contents of a local directory into glance.
    size            Determine the size of a glance instance if dumped to disk.

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -c CHUNKSIZE, --chunksize=CHUNKSIZE
                        Amount of data to transfer per HTTP write
  -d, --debug           Print debugging information
  -D DONTREPLICATE, --dontreplicate=DONTREPLICATE
                        List of fields to not replicate
  -m, --metaonly        Only replicate metadata, not images
  -l LOGFILE, --logfile=LOGFILE
                        Path of file to log to
  -s, --syslog          Log to syslog instead of a file
  -t TOKEN, --token=TOKEN
                        Pass in your authentication token if you have one. If
                        you use this option the same token is used for both
                        the master and the slave.
  -M MASTERTOKEN, --mastertoken=MASTERTOKEN
                        Pass in your authentication token if you have one.
                        This is the token used for the master.
  -S SLAVETOKEN, --slavetoken=SLAVETOKEN
                        Pass in your authentication token if you have one.
                        This is the token used for the slave.
  -v, --verbose         Print more verbose output</computeroutput></screen></para>
        <para>The replicator supports the following commands:</para>
        <simplesect>
            <title>livecopy: Load the contents of one glance instance
                into another</title>
            <para>
                <screen><userinput>glance-replicator livecopy <replaceable>fromserver:port</replaceable> <replaceable>toserver:port</replaceable></userinput></screen>
            </para>
            <para>
                <itemizedlist>
                    <listitem>
                        <para><replaceable>fromserver:port</replaceable>:
                            the location of the master glance
                            instance</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>toserver:port</replaceable>:
                            the location of the slave glance instance.
                        </para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>Take a copy of the fromserver, and dump it onto the
                toserver. Only images visible to the user running the
                replicator will be copied if glance is configured to
                use the Identity service (keystone) for
                authentication. Only images active on
                    <replaceable>fromserver </replaceable>are copied
                across. The copy is done "on-the-wire" so there are no
                large temporary files on the machine running the
                replicator to clean up.</para>
        </simplesect>
        <simplesect>
            <title>dump: Dump the contents of a glance instance to
                local disk</title>
            <para>
                <screen><userinput>glance-replicator dump <replaceable>server:port</replaceable> <replaceable>path</replaceable></userinput></screen>
            </para>
            <para><itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance instance.
                        </para>
                    </listitem>
                    <listitem>
                        <para><replaceable>path</replaceable>: a
                            directory on disk to contain the data.
                        </para>
                    </listitem>
                </itemizedlist>Do the same thing as
                    <command>livecopy</command>, but dump the contents
                of the glance server to a directory on disk. This
                includes metadata and image data. Depending on the
                size of the local glance repository, the resulting
                dump may consume a large amount of local storage.
                Therefore, we recommend you use the
                    <command>size</command> comamnd first to determine
                the size of the resulting dump.</para>
        </simplesect>
        <simplesect>
            <title>load: Load a directory created by the dump command
                into a glance server</title>
            <para>
                <screen><userinput>glance-replicator load server:port path</userinput></screen>
                <itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance
                            instance.</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>path</replaceable>: a
                            directory on disk containing the
                            data.</para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>Load the contents of a local directory into glance. </para>
            <para>The <command>dump</command> and
                    <command>load</command> are useful when
                replicating across two glance servers where a direct
                connection across the two glance hosts is impossible
                or too slow.</para>
        </simplesect>
        <simplesect>
            <title>compare: Compare the contents of two glance
                servers</title>
            <para><screen><userinput>glance-replicator compare <replaceable>fromserver:port</replaceable> <replaceable>toserver:port</replaceable></userinput></screen><itemizedlist>
                    <listitem>
                        <para><replaceable>fromserver:port</replaceable>:
                            the location of the master glance
                            instance.</para>
                    </listitem>
                    <listitem>
                        <para><replaceable>
                                toserver:port</replaceable>: the
                            location of the slave glance instance.
                        </para>
                    </listitem>
                </itemizedlist>The <command>compare</command> command
                will show you the differences between the two servers,
                which is effectively a dry run of the
                    <command>livecopy</command> command.</para>
        </simplesect>
        <simplesect>
            <title>size: Determine the size of a glance instance if
                dumped to disk</title>
            <para><screen><userinput>glance-replicator size</userinput></screen><itemizedlist>
                    <listitem>
                        <para><replaceable>server:port</replaceable>:
                            the location of the glance
                            instance.</para>
                    </listitem>
                </itemizedlist>The <command>size</command> command
                will tell you how much disk is going to be used by
                image data in either a <command>dump</command> or a
                    <command>livecopy</command>. Note that this will
                provide raw number of bytes that would be written to
                the destination, it has no information about the
                redundancy costs associated with glance-registry
                back-ends that use replication for redundancy, such as
                Swift or Ceph.</para>
        </simplesect>
        <simplesect>
            <title>Example using livecopy</title>
            <para>Assuming you have a primary glance service running
                on a node called
                    <literal>primary.example.com</literal> with
                glance-api service running on port
                    <literal>9292</literal> (the default port) and you
                want to replicate its contents to a secondary glance
                service running on a node called
                    <literal>secondary.example.com</literal>, also on
                port 9292, you will first need to get authentication
                tokens from keystone for the primary and secondary
                glance server and then you can use the
                    <literal>glance-replicator livecopy</literal>
                command.</para>
            <para>The following example assumes that you have a
                credentials file for your primary cloud called
                    <filename>primary.openrc</filename> and one for
                your secondary cloud called
                    <filename>secondary.openrc</filename>.<screen><prompt>$</prompt> <userinput>source primary.openrc</userinput>
<prompt>$</prompt> <userinput>keystone token-get</userinput>
<computeroutput>+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
| expires   | 2012-11-16T03:13:08Z             |
| id        | 8a5d3afb5095430891f33f69a2791463 |
| tenant_id | dba21b41af584daeac5782ca15a77a25 |
| user_id   | add2ece6b1f94866994d3a3e3beb3d47 |
+-----------+----------------------------------+</computeroutput>
<prompt>$</prompt> <userinput>PRIMARY_AUTH_TOKEN=<replaceable>8e97fa8bcf4443cfbd3beb9079c7142f </replaceable></userinput>
<prompt>$</prompt> <userinput>source secondary.openrc</userinput>
<prompt>$</prompt> <userinput>keystone token-get</userinput>
<computeroutput>+-----------+----------------------------------+
|  Property |              Value               |
+-----------+----------------------------------+
| expires   | 2012-11-16T03:13:08Z             |
| id        | 29f777ac2c9b41a6b4ee9c3e6b85f98a |
| tenant_id | fbde89d638d947a19545b0f387ffea4d |
| user_id   | 4a7a48e7d62e4b428c78d02c1968ca7b |
+-----------+----------------------------------+</computeroutput>
<prompt>$</prompt> <userinput>SECONDARY_AUTH_TOKEN=<replaceable>29f777ac2c9b41a6b4ee9c3e6b85f98a</replaceable></userinput>
<prompt>$</prompt> <userinput>glance-replicator livecopy primary.example.com:9292 secondary.example.com:9292 -M ${PRIMARY_AUTH_TOKEN} -S ${SECONDARY_AUTH_TOKEN} </userinput></screen></para>
        </simplesect>
    </section>
</chapter>
