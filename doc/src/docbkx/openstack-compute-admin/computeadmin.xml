<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
<!-- Some useful entities borrowed from HTML -->
<!ENTITY ndash  "&#x2013;">
<!ENTITY mdash  "&#x2014;">
<!ENTITY hellip "&#x2026;">
<!ENTITY nbsp "&#160;">
<!ENTITY CHECK  '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
<imageobject>
<imagedata fileref="img/Check_mark_23x20_02.svg"
format="SVG" scale="60"/>
</imageobject>
</inlinemediaobject>'>

<!ENTITY ARROW  '<inlinemediaobject xmlns="http://docbook.org/ns/docbook">
<imageobject>
<imagedata fileref="img/Arrow_east.svg"
format="SVG" scale="60"/>
</imageobject>
</inlinemediaobject>'>
]>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_system-administration-for-openstack-compute">

    <title>System Administration</title>
    <para>By understanding how the different installed nodes interact with each other you can
        administer the OpenStack Compute installation. OpenStack Compute offers many ways to install
        using multiple servers but the general idea is that you can have multiple compute nodes that
        control the virtual servers and a cloud controller node that contains the remaining Nova services. </para>
    <para>The OpenStack Compute cloud works via the interaction of a series of daemon processes
        named nova-* that reside persistently on the host machine or machines. These binaries can
        all run on the same machine or be spread out on multiple boxes in a large deployment. The
        responsibilities of Services, Managers, and Drivers, can be a bit confusing at first. Here
        is an outline the division of responsibilities to make understanding the system a little bit
        easier. </para>
    <para>Currently, Services are nova-api, nova-objectstore (which can be replaced with Glance, the
        OpenStack Image Service), nova-compute, nova-volume, and nova-network. Managers and Drivers
        are specified by configuration options and loaded using utils.load_object(). Managers are responsible for a
        certain aspect of the system. It is a logical grouping of code relating to a portion of the
        system. In general other components should be using the manager to make changes to the
        components that it is responsible for. </para>
    <para>For example, other components that need to deal with volumes in some way, should do so by
        calling methods on the VolumeManager instead of directly changing fields in the database.
        This allows us to keep all of the code relating to volumes in the same place. </para>
    <itemizedlist>
        <listitem>
            <para>nova-api - The nova-api service receives xml requests and sends them to the rest
                of the system. It is a wsgi app that routes and authenticate requests. It supports
                the EC2 and OpenStack APIs. There is a nova-api.conf file created when you install
                Compute.</para>
        </listitem>
        <listitem>
            <para>nova-objectstore - The nova-objectstore service is an ultra simple file-based
                storage system for images that replicates most of the S3 API. It can be replaced
                with OpenStack Image Service and a simple image manager or use OpenStack Object
                Storage as the virtual machine image storage facility. It must reside on the same
                node as nova-compute.</para>
        </listitem>
        <listitem>
            <para>nova-compute - The nova-compute service is responsible for managing virtual
                machines. It loads a Service object which exposes the public methods on
                ComputeManager via Remote Procedure Call (RPC).</para>
        </listitem>
        <listitem>
            <para>nova-volume - The nova-volume service is responsible for managing attachable block
                storage devices. It loads a Service object which exposes the public methods on
                VolumeManager via RPC.</para>
        </listitem>
        <listitem>
            <para>nova-network - The nova-network service is responsible for managing floating and
                fixed IPs, DHCP, bridging and VLANs. It loads a Service object which exposes the
                public methods on one of the subclasses of NetworkManager. Different networking
                strategies are available to the service by changing the network_manager configuration option to
                FlatManager, FlatDHCPManager, or VlanManager (default is VLAN if no other is
                specified).</para>

        </listitem>
    </itemizedlist>

    <section xml:id="understanding-the-compute-service-architecture">

        <title>Understanding the Compute Service Architecture</title>
        <para>These basic categories describe the service architecture and what's going on within the cloud controller.</para>
        <simplesect><title>API Server</title>

            <para>At the heart of the cloud framework is an API Server. This API Server makes command and control of the hypervisor, storage, and networking programmatically available to users in realization of the definition of cloud computing.
            </para>
            <para>The API endpoints are basic http web services which handle authentication, authorization, and basic command and control functions using various API interfaces under the Amazon, Rackspace, and related models. This enables API compatibility with multiple existing tool sets created for interaction with offerings from other vendors. This broad compatibility prevents vendor lock-in.
            </para> </simplesect>
        <simplesect><title>Message Queue</title>
            <para>
            A messaging queue brokers the interaction between compute nodes (processing), volumes (block storage), the networking controllers (software which controls network infrastructure), API endpoints, the scheduler (determines which physical hardware to allocate to a virtual resource), and similar components. Communication to and from the cloud controller is by HTTP requests through multiple API endpoints.</para>

<para>            A typical message passing event begins with the API server receiving a request from a user. The API server authenticates the user and ensures that the user is permitted to issue the subject command. Availability of objects implicated in the request is evaluated and, if available, the request is routed to the queuing engine for the relevant workers. Workers continually listen to the queue based on their role, and occasionally their type hostname. When such listening produces a work request, the worker takes assignment of the task and begins its execution. Upon completion, a response is dispatched to the queue which is received by the API server and relayed to the originating user. Database entries are queried, added, or removed as necessary throughout the process.
</para>
</simplesect>
        <simplesect><title>Compute Worker</title>

            <para>Compute workers manage computing instances on host machines. Through the API, commands are dispatched to compute workers to:</para>

            <itemizedlist>
            <listitem><para>Run instances</para></listitem>
            <listitem><para>Terminate instances</para></listitem>
            <listitem><para>Reboot instances</para></listitem>
            <listitem><para>Attach volumes</para></listitem>
            <listitem><para>Detach volumes</para></listitem>
            <listitem><para>Get console output</para></listitem></itemizedlist>
            </simplesect>
            <simplesect><title>Network Controller</title>

            <para>The Network Controller manages the networking resources on host machines. The API server dispatches commands through the message queue, which are subsequently processed by Network Controllers. Specific operations include:</para>

            <itemizedlist><listitem><para>Allocate fixed IP addresses</para></listitem>
            <listitem><para>Configuring VLANs for projects</para></listitem>
            <listitem><para>Configuring networks for compute nodes</para></listitem></itemizedlist>
            </simplesect>
<simplesect><title>Volume Workers</title>

            <para>Volume Workers interact with iSCSI storage to manage LVM-based instance volumes. Specific functions include:
            </para>
            <itemizedlist>
                <listitem><para>Create volumes</para></listitem>
            <listitem><para>Delete volumes</para></listitem>
            <listitem><para>Establish Compute volumes</para></listitem></itemizedlist>

    <para>Volumes may easily be transferred between instances, but may be attached to only a single instance at a time.</para></simplesect></section>
    <section xml:id="managing-compute-users">
        <title>Managing Compute Users</title>
        <para>Access to the Euca2ools (ec2) API is controlled by an access and secret key. The
            userâ€™s access key needs to be included in the request, and the request must be signed
            with the secret key. Upon receipt of API requests, Compute will verify the signature and
            execute commands on behalf of the user. </para>
        <para>In order to begin using nova, you will need to create a
            user with the Identity Service.  </para>
    </section>
    <section xml:id="managing-the-cloud">

        <title>Managing the Cloud</title><para>There are three main tools that a system administrator will find useful to manage their cloud;
            the nova client, the nova-manage command, and the Euca2ools commands. </para>
        <para>The nova-manage command may only be run by cloud administrators. Both
            novaclient and euca2ools can be used by all users, though specific commands may be
            restricted by Role Based Access Control in the deprecated nova auth system or in the Identity Management service. </para>
            <simplesect><title>Using the nova command-line tool</title>
            <para>Installing the python-novaclient gives you a <code>nova</code> shell command that enables
                Compute API interactions from the command line. You install the client, and then provide
                your username and password, set as environment variables for convenience, and then you
                can have the ability to send commands to your cloud on the command-line.</para>
            <para>To install python-novaclient, download the tarball from
                <link xlink:href="http://pypi.python.org/pypi/python-novaclient/2.6.3#downloads">http://pypi.python.org/pypi/python-novaclient/2.6.3#downloads</link> and then install it in your favorite python environment. </para>
            <screen>
<prompt>$</prompt> <userinput>curl -O http://pypi.python.org/packages/source/p/python-novaclient/python-novaclient-2.6.3.tar.gz</userinput>
<prompt>$</prompt> <userinput>tar -zxvf python-novaclient-2.6.3.tar.gz</userinput>
<prompt>$</prompt> <userinput>cd python-novaclient-2.6.3</userinput>
<prompt>$</prompt> <userinput>sudo python setup.py install</userinput>
    </screen>
            <para>Now that you have installed the python-novaclient, confirm the installation by entering:</para>
<screen>
<prompt>$</prompt> <userinput>nova help</userinput>
</screen>
<programlisting>
usage: nova [--debug] [--os-username OS_USERNAME] [--os-password OS_PASSWORD]
            [--os-tenant-name_name OS_TENANT_NAME] [--os-auth-url OS_AUTH_URL]
            [--os-region-name OS_REGION_NAME] [--service-type SERVICE_TYPE]
            [--service-name SERVICE_NAME] [--endpoint-type ENDPOINT_TYPE]
            [--version VERSION]
            &lt;subcommand&gt; ...
</programlisting>
            <para>In return, you will get a listing of all the
                commands and parameters for the nova command line
                client. By setting up the required parameters as
                environment variables, you can fly through these
                commands on the command line. You can add
                    <literal>--os-username</literal> on the nova
                command, or set them as environment variables: </para>
            <para>
<screen>
<prompt>$</prompt> <userinput>export OS_USERNAME=joecool</userinput>
<prompt>$</prompt> <userinput>export OS_PASSWORD=coolword</userinput>
<prompt>$</prompt> <userinput>export OS_TENANT_NAME=coolu</userinput>
</screen>
            </para><para>Using the Identity Service, you are supplied with an
                authentication endpoint, which nova recognizes as the
                    <literal>OS_AUTH_URL</literal>. </para>
            <para>
                <screen>
<prompt>$</prompt> <userinput>export OS_AUTH_URL=http://hostname:5000/v2.0</userinput>
<prompt>$</prompt> <userinput>export NOVA_VERSION=1.1</userinput>
</screen>
            </para></simplesect>

        <simplesect><title>Using the nova-manage command</title>
        <para>The nova-manage command may be used to perform many essential functions for
                administration and ongoing maintenance of nova, such as network creation or user
                manipulation.</para>
        <para>The man page for nova-manage has a good explanation for each of its functions, and is
              recommended reading for those starting out. Access it by running:
        </para>
            <screen>
            <prompt>$</prompt> <userinput>man nova-manage</userinput>
            </screen>

        <para>For administrators, the standard pattern for executing a nova-manage command is: </para>
            <screen>
<prompt>$</prompt> <userinput>nova-manage category command <replaceable>[args]</replaceable></userinput>
            </screen>

            <para>For example, to obtain a list of all projects:
                    <command>nova-manage project list</command></para>

            <para>Run without arguments to see a list of available command categories: nova-manage</para>
            <para>You can also run with a category argument such as user to see
	          a list of all commands in that category: nova-manage service</para>
        </simplesect>        <simplesect><title>Using the euca2ools commands</title>
            <para>For a command-line interface to EC2 API calls, use
                the euca2ools command line tool. It is documented at
                    <link
                    xlink:href="http://open.eucalyptus.com/wiki/Euca2oolsGuide_v1.3"
                    >http://open.eucalyptus.com/wiki/Euca2oolsGuide_v1.3</link></para>
        </simplesect>
    </section>

    <xi:include href="rootwrap.xml"/>

    <section xml:id="logging">
        <title>Managing logs</title>
        <simplesect>
            <title>Logging module</title>
            <para>Adding the following line to <filename>/etc/nova/nova.conf</filename> will allow
                you to specify a configuration file for changing the logging behavior, in particular
                for changing the logging level (e.g., <literal>DEBUG</literal>,
                    <literal>INFO</literal>, <literal>WARNING</literal>,
                <literal>ERROR</literal>):<programlisting>log-config=/etc/nova/logging.conf</programlisting></para>
            <para>The log config file is an ini-style config file which must contain a section
                called <literal>logger_nova</literal>, which controls the behavior of the logging
                facility in the <literal>nova-*</literal> services. The file must contain a section
                called <literal>logger_nova</literal>, for
                example:<programlisting>[logger_nova]
level = INFO
handlers = stderr
qualname = nova</programlisting></para>
            <para>This example sets the debugging level to <literal>INFO</literal> (which less
                verbose than the default <literal>DEBUG</literal> setting). See the <link
                    xlink:href="http://docs.python.org/release/2.7/library/logging.html#configuration-file-format"
                    >Python documentation on logging configuration file format </link>for more
                details on this file, including the meaning of the <literal>handlers </literal>and
                    <literal>quaname</literal> variables. See <link
                    xlink:href="https://github.com/openstack/nova/blob/master/etc/nova/logging_sample.conf"
                    >etc/nova/logging_sample.conf</link> in the openstack/nova repository on GitHub
                for an example logging.conf file with various handlers defined.</para>
        </simplesect>
        <simplesect><title>Syslog</title>

        <para>OpenStack Compute services can be configured to send logging information to syslog.
                This is particularly useful if you want to use rsyslog, which will forward the logs
                to a remote machine. You need to separately configure the Compute service (nova),
                the Identity service (keystone), the Image service (glance), and, if you are using
                it, the Block Storage service (cinder) to send log messages to syslog. To do so, add
                the following lines to:<itemizedlist>
                    <listitem>
                        <para><filename>/etc/nova/nova.conf</filename></para>
                    </listitem>
                    <listitem>
                        <para><filename>/etc/keystone/keystone.conf</filename></para>
                    </listitem>
                    <listitem>
                        <para><filename>/etc/glance/glance-api.conf</filename></para>
                    </listitem>
                    <listitem>
                        <para><filename>/etc/glance/glance-registry.conf</filename></para>
                    </listitem>
                    <listitem>
                        <para><filename>/etc/cinder/cinder.conf</filename></para>
                    </listitem>
                </itemizedlist><programlisting>verbose = False
debug = False
use_syslog = True
syslog_log_facility = LOG_LOCAL0 </programlisting>
                In addition to enabling syslog, these settings also turn off more verbose output and
                debugging output from the log.<note>
                    <para>While the example above uses the same local facility for each service
                            (<literal>LOG_LOCAL0</literal>, which corresponds to syslog facility
                            <literal>LOCAL0</literal>), we recommend that you configure a separate
                        local facility for each service, as this provides better isolation and more
                        flexibility. For example, you may want to capture logging info at different
                        severity levels for different services. Syslog allows you to define up to
                        seven local facilities, <literal>LOCAL0, LOCAL1, ..., LOCAL7</literal>. See
                        the syslog documentation for more details.</para>
                </note></para>
        </simplesect>
        <simplesect><title>Rsyslog</title>
            <para>Rsyslog is a useful tool for setting up a centralized log server across multiple
                machines. We briefly describe the configuration to set up an rsyslog server; a full
                treatment of rsyslog is beyond the scope of this document. We assume rsyslog has
                already been installed on your hosts, which is the default on most Linux
                distributions.</para>
            <para>This example shows a minimal configuration for
                    <filename>/etc/rsyslog.conf</filename> on the log server host which will receive
                the log
                files:<programlisting># provides TCP syslog reception
$ModLoad imtcp
$InputTCPServerRun 1024</programlisting></para>
            <para>Add to <filename>/etc/rsyslog.conf</filename> a filter rule on which looks for a
                hostname. The example below use <replaceable>compute-01</replaceable> as an example
                of a compute host
                name:<programlisting>:hostname, isequal, "<replaceable>compute-01</replaceable>" /mnt/rsyslog/logs/compute-01.log</programlisting></para>
            <para>On the compute hosts, create a file named
                    <filename>/etc/rsyslog.d/60-nova.conf</filename>, with the following
                content.<programlisting># prevent debug from dnsmasq with the daemon.none parameter
*.*;auth,authpriv.none,daemon.none,local0.none -/var/log/syslog
# Specify a log level of ERROR
local0.error    @@172.20.1.43:1024</programlisting></para>
            <para>Once you have created this file, restart your rsyslog daemon. Error-level log
                messages on the compute hosts  should now be sent to your log server.</para></simplesect>
    </section>

    <section xml:id="live-migration-usage">

        <title>Using Migration</title>
        <para>Before starting migrations, review the <link linkend="configuring-migrations"
                >Configuring Migrations</link> section.</para>
        <para>Migration provides a scheme to migrate running instances from one OpenStack Compute
            server to another OpenStack Compute server. This feature can be used as described below. </para>

        <itemizedlist>
            <listitem>
               <para>First, look at the running instances, to get the ID of the instance you wish to
                    migrate.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova list</userinput>
<![CDATA[+--------------------------------------+------+--------+-----------------+
|                  ID                  | Name | Status |Networks         |
+--------------------------------------+------+--------+-----------------+
| d1df1b5a-70c4-4fed-98b7-423362f2c47c | vm1  | ACTIVE | private=a.b.c.d |
| d693db9e-a7cf-45ef-a7c9-b3ecb5f22645 | vm2  | ACTIVE | private=e.f.g.h |
+--------------------------------------+------+--------+-----------------+
                 ]]></programlisting>
               <para>Second, look at information associated with that instance - our example is vm1
from above.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova show d1df1b5a-70c4-4fed-98b7-423362f2c47c</userinput>
<![CDATA[+-------------------------------------+----------------------------------------------------------+
|               Property              |                          Value                           |
+-------------------------------------+----------------------------------------------------------+
...
| OS-EXT-SRV-ATTR:host                | HostB                                                    |
...
| flavor                              | m1.tiny                                                  |
| id                                  | d1df1b5a-70c4-4fed-98b7-423362f2c47c                     |
| name                                | vm1                                                      |
| private network                     | a.b.c.d                                                  |
| status                              | ACTIVE                                                   |
...
+-------------------------------------+----------------------------------------------------------+
                 ]]></programlisting>
                 <para> In this example, vm1 is running on HostB.</para>
            </listitem>
            <listitem>
                <para>Third, select the server to migrate instances to.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova-manage service list</userinput>
<![CDATA[HostA nova-scheduler enabled  :-) None
HostA nova-volume enabled  :-) None
HostA nova-network enabled  :-) None
HostB nova-compute enabled  :-) None
HostC nova-compute enabled  :-) None
                 ]]></programlisting>
                 <para> In this example, HostC can be picked up because nova-compute is running on it.</para>
            </listitem>
            <listitem>
                <para>Third, ensure that HostC has enough resource for migration.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova-manage service describe_resource HostC</userinput>
<![CDATA[HOST             PROJECT     cpu   mem(mb)     hdd
HostC(total)                  16     32232     878
HostC(used_now)               13     21284     442
HostC(used_max)               13     21284     442
HostC            p1            5     10240     150
HostC            p2            5     10240     150
.....
                 ]]></programlisting>
                 <itemizedlist>
                     <listitem>
                         <para><emphasis role="bold">cpu:</emphasis>the nuber of cpu</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">mem(mb):</emphasis>total amount of memory (MB)</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">hdd</emphasis>total amount of NOVA-INST-DIR/instances(GB)</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">1st line shows </emphasis>total amount of resource physical server has.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">2nd line shows </emphasis>current used resource.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">3rd line shows </emphasis>maximum used resource.</para>
                     </listitem>
                     <listitem>
                         <para><emphasis role="bold">4th line and under</emphasis> is used resource per project.</para>
                     </listitem>
                 </itemizedlist>
            </listitem>
            <listitem>
                <para>Finally, use the <command>nova live-migration</command> command to migrate the
                    instances.</para>
                <programlisting>
<prompt>#</prompt> <userinput>nova live-migration bee83dd3-5cc9-47bc-a1bd-6d11186692d0 HostC</userinput>
<![CDATA[Migration of bee83dd3-5cc9-47bc-a1bd-6d11186692d0 initiated.
                 ]]></programlisting>
                 <para>Make sure instances are migrated successfully with <command>nova
                        list</command>. If instances are still running on HostB, check logfiles
                    (src/dest nova-compute and nova-scheduler) to determine why.
                     <note><para>While the nova command is called <command>live-migration</command>, under the default Compute
                            configuration options the instances are suspended before migration. See
                            the <link linkend="configuring-migrations">Configuring Migrations</link>
                            section for more details.</para></note></para>
            </listitem>
        </itemizedlist>


    </section>
    <section xml:id="nova-compute-node-down">
	<title>Recovering from a failed compute node</title>
	<para>If you have deployed OpenStack Compute with a shared filesystem,
	you can quickly recover from a failed compute node. </para>
	<simplesect>
	    <title>Working with host information</title>
            <para>
	The first step is to identify the vms on the affected hosts, using tools such as
	a combination of <literal>nova list</literal> and <literal>nova show</literal> or
	<literal>euca-describe-instances</literal>. Here's an example using the EC2 API
	 - instance i-000015b9 that is running on node np-rcc54:
            </para>
	<programlisting>
	i-000015b9 at3-ui02 running nectarkey (376, np-rcc54) 0 m1.xxlarge 2012-06-19T00:48:11.000Z 115.146.93.60
	</programlisting>
	<para>
	First, you can review the status of the host using the nova database, some of the important information is highlighted below. This example converts an EC2 API instance ID into an openstack ID - if you used the <literal>nova</literal>
	commands, you can substitute the ID directly. You can find the credentials for your database in
<literal>/etc/nova.conf</literal>.
	</para>
<programlisting>
SELECT * FROM instances WHERE id = CONV('15b9', 16, 10) \G;
*************************** 1. row ***************************
              created_at: 2012-06-19 00:48:11
              updated_at: 2012-07-03 00:35:11
              deleted_at: NULL
...
                      id: 5561
...
             power_state: 5
                vm_state: shutoff
...
                hostname: at3-ui02
                    host: np-rcc54
...
                    uuid: 3f57699a-e773-4650-a443-b4b37eed5a06
...
              task_state: NULL
...
</programlisting>
</simplesect>
<simplesect>
<title>Recover the VM</title>
<para>
Armed with the information of VMs on the failed host, determine which compute host
 the affected VMs should be moved to. In this case, the VM will move to np-rcc46, which
is achieved using this database command:</para>
<programlisting>
UPDATE instances SET host = 'np-rcc46' WHERE uuid = '3f57699a-e773-4650-a443-b4b37eed5a06';
</programlisting>
<para> Next, if using a hypervisor that relies on libvirt (such as KVM)
it is a good idea to update the <literal>libvirt.xml</literal> file
(found in <literal>/var/lib/nova/instances/[instance ID]</literal>).
The important changes to make are to change the <literal>DHCPSERVER</literal>
 value to the host ip address of the nova compute host that is the VMs new home, and
update the VNC IP if it isn't already <literal>0.0.0.0</literal>.
</para>
<para>
Next, reboot the VM:
<screen>
<prompt>$</prompt> <userinput>nova reboot --hard 3f57699a-e773-4650-a443-b4b37eed5a06</userinput>
</screen>
</para>
<para>In theory, the above database update and <literal>nova reboot</literal> command
are all that is required to recover the VMs from a failed host.
However, if further problems occur, consider looking at recreating the
network filter configuration using <literal>virsh</literal>, restarting the
nova services or updating the <literal>vm_state</literal> and
<literal>power_state</literal> in the nova database.</para>
</simplesect>
</section>
    <section xml:id="nova-uid-mismatch">
	<title>Recovering from a UID/GID mismatch</title>
	<para>When running OpenStack compute, using a shared filesystem or
         an automated configuration tool, you could encounter a situation
         where some files on your compute node are using the wrong UID or
         GID. This causes a raft of errors, such as being unable to live
         migrate, or start virtual machines. </para>
         <para> The following is a basic procedure run on nova-compute hosts, based on the KVM hypervisor,
          that could help to restore the situation:
         <itemizedlist>
         <listitem><para>First,make sure you don't use numbers that are already used for some other user/group.</para></listitem>
         <listitem><para>Set the nova uid in /etc/passwd to the same number in all hosts (e.g. 112)</para></listitem>
         <listitem><para>Set the libvirt-qemu uid in /etc/passwd to the same number in all hosts (e.g. 119)</para></listitem>
         <listitem><para>Set the nova group in /etc/group file to the same number in all hosts (e.g. 120)</para></listitem>
         <listitem><para>Set the libvirtd group in /etc/group file to the same number in all hosts (e.g. 119)</para></listitem>
         <listitem><para>Stop the services on the compute node</para></listitem>
         <listitem><para>Change all the files owned by nova or group by nova, e.g.
         <programlisting>
         find / -uid 108 -exec chown nova {} \; # note the 108 here is the old nova uid before the change
         find / -gid 120 -exec chgrp nova {} \;
         </programlisting>
         </para></listitem>
         <listitem><para>Repeat the steps for the libvirt-qemu owned files if those were needed to change</para></listitem>
         <listitem><para>Restart the services</para></listitem>
         </itemizedlist>
         </para>
         <para>Following this, you can run the <command>find</command>
         command to verify that all files using the correct identifiers.
         </para>
   </section>

    <section xml:id="nova-disaster-recovery-process">
        <title>Nova Disaster Recovery Process</title>
        <para>Sometimes, things just don't go right. An incident is
            never planned, by its definition. </para>
        <para>In this section, we will review managing your cloud
            after a disaster, and how to easily backup the persistent
            storage volumes, which is another approach when you face a
            disaster. Even apart from the disaster scenario, backup
            ARE mandatory. While the Diablo release includes the
            snapshot functions, both the backup procedure and the
            utility do apply to the Cactus release. </para>
        <para>For reference, you cand find a DRP definition here : <link
                xlink:href="http://en.wikipedia.org/wiki/Disaster_Recovery_Plan"
                >http://en.wikipedia.org/wiki/Disaster_Recovery_Plan</link>. </para>
            <simplesect>
                <title>A- The disaster Recovery Process presentation</title>
           <para>A disaster could happen to several components of your
                architecture : a disk crash, a network loss, a power
                cut, etc. In this example, we suppose the following
                setup : <orderedlist>
                    <listitem>
                        <para> A cloud controller (nova-api,
                            nova-objecstore, nova-volume,
                            nova-network) </para>
                    </listitem>
                    <listitem>
                        <para> A compute node (nova-compute) </para>
                    </listitem>
                    <listitem>
                        <para> A Storage Area Network used by
                            nova-volumes (aka SAN) </para>
                    </listitem>
                </orderedlist>The example disaster will be the worst
                one : a power loss. That power loss applies to the
                three components. <emphasis role="italic">Let's see
                    what runs and how it runs before the
                    crash</emphasis> : <itemizedlist>
                    <listitem>
                        <para>From the SAN to the cloud controller, we
                            have an active iscsi session (used for the
                            "nova-volumes" LVM's VG). </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node we also have active iscsi sessions
                            (managed by nova-volume). </para>
                    </listitem>
                    <listitem>
                        <para>For every volume an iscsi session is
                            made (so 14 ebs volumes equals 14
                            sessions). </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, we also have iptables/ ebtables
                            rules which allows the access from the
                            cloud controller to the running instance. </para>
                    </listitem>
                    <listitem>
                        <para>And at least, from the cloud controller
                            to the compute node ; saved into database,
                            the current state of the instances (in
                            that case "running" ), and their volumes
                            attachment (mountpoint, volume id, volume
                            status, etc..) </para>
                    </listitem>
                </itemizedlist> Now, after the power loss occurs and
                all hardware components restart, the situation is as
                follows: </para>
            <para>
                <itemizedlist>
                    <listitem>
                        <para>From the SAN to the cloud, the ISCSI
                            session no longer exists. </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, the ISCSI sessions no longer exist.
                        </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller to the compute
                            node, the iptables and ebtables are
                            recreated, since, at boot, nova-network
                            reapply the configurations. </para>
                    </listitem>
                    <listitem>
                        <para>From the cloud controller, instances
                            turn into a shutdown state (because they
                            are no longer running) </para>
                    </listitem>
                    <listitem>
                        <para>Into the database, data was not updated
                            at all, since nova could not have guessed
                            the crash. </para>
                    </listitem>
                </itemizedlist>Before going further, and in order to
                prevent the admin to make fatal mistakes,<emphasis
                    role="bold"> the instances won't be
                    lost</emphasis>, since no "<command role="italic"
                    >destroy</command>" or "<command role="italic"
                    >terminate</command>" command had been invoked, so
                the files for the instances remain on the compute
                node. </para>
            <para>The plan is to perform the following tasks, in that
                exact order. <emphasis role="underline">Any extra step
                    would be dangerous at this stage</emphasis>
                :</para>
            <para>
                <orderedlist>
                    <listitem>
               <para>We need to get the current relation from a volume to its instance, since we
                            will recreate the attachment.</para>
                    </listitem>
                    <listitem>
                        <para>We need to update the database in order to clean the stalled state.
                            (After that, we won't be able to perform the first step). </para>
                    </listitem>
                    <listitem>
                        <para>We need to restart the instances (so go from a "shutdown" to a
                            "running" state). </para>
                    </listitem>
                    <listitem>
                        <para>After the restart, we can reattach the volumes to their respective
                            instances. </para>
                    </listitem>
                    <listitem>
                        <para> That step, which is not a mandatory one, exists in an SSH into the
                            instances in order to reboot them. </para>
                    </listitem>
                </orderedlist>
            </para>
            </simplesect>
        <simplesect>
            <title>B - The Disaster Recovery Process itself</title>
            <para>
                <itemizedlist>
                    <listitem>
                        <para><emphasis role="bold"> Instance to
                                Volume relation </emphasis>
                        </para>
                        <para>We need to get the current relation from
                            a volume to its instance, since we will
                            recreate the attachment : </para>
                        <para>This relation could be figured by
                            running
                                <command>nova volume-list</command>
                        </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Database Update
                            </emphasis>
                        </para>
                        <para>Second, we need to update the database
                            in order to clean the stalled state. Now
                            that we have saved the attachments we need
                            to restore for every volume, the database
                            can be cleaned with the following queries:
                            <programlisting>
<prompt>mysql></prompt> <userinput>use nova;</userinput>
<prompt>mysql></prompt> <userinput>update volumes set mountpoint=NULL;</userinput>
<prompt>mysql></prompt> <userinput>update volumes set status="available" where status &lt;&gt;"error_deleting";</userinput>
<prompt>mysql></prompt> <userinput>update volumes set attach_status="detached";</userinput>
<prompt>mysql></prompt> <userinput>update volumes set instance_id=0;</userinput>
                </programlisting>Now,
                            when running
                                <command>nova volume-list</command>
                            all volumes should be available. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Instances Restart
                            </emphasis>
                        </para>
                        <para>We need to restart the instances. This
                            can be done via a simple
                                <command>nova reboot
                                   <replaceable>$instance</replaceable></command>
                        </para>
                        <para>At that stage, depending on your image,
                            some instances will completely reboot and
                            become reachable, while others will stop
                            on the "plymouth" stage. </para>
                        <para><emphasis role="bold">DO NOT reboot a
                                second time</emphasis> the ones which
                            are stopped at that stage (<emphasis
                                role="italic">see below, the fourth
                                step</emphasis>). In fact it depends
                            on whether you added an
                                <filename>/etc/fstab</filename> entry
                            for that volume or not. Images built with
                            the <emphasis role="italic"
                                >cloud-init</emphasis> package will
                            remain on a pending state, while others
                            will skip the missing volume and start.
                            (More information is available on <link
                                xlink:href="https://help.ubuntu.com/community/CloudInit"
                                >help.ubuntu.com</link>) But remember
                            that the idea of that stage is only to ask
                            nova to reboot every instance, so the
                            stored state is preserved. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> Volume Attachment
                            </emphasis>
                        </para>
                        <para>After the restart, we can reattach the
                            volumes to their respective instances. Now
                            that nova has restored the right status,
                            it is time to perform the attachments via
                            a <command>nova
                            volume-attach</command></para>
                        <para>Here is a simple snippet that uses the
                            file we created :
                            <programlisting>
#!/bin/bash

while read line; do
    volume=`echo $line | $CUT -f 1 -d " "`
    instance=`echo $line | $CUT -f 2 -d " "`
    mount_point=`echo $line | $CUT -f 3 -d " "`
        echo "ATTACHING VOLUME FOR INSTANCE - $instance"
    nova volume-attach $instance $volume $mount_point
    sleep 2
done &lt; $volumes_tmp_file
                </programlisting>At
                            that stage, instances which were pending
                            on the boot sequence (<emphasis
                                role="italic">plymouth</emphasis>)
                            will automatically continue their boot,
                            and restart normally, while the ones which
                            booted will see the volume. </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold"> SSH into
                                instances </emphasis>
                        </para>
                        <para>If some services depend on the volume,
                            or if a volume has an entry into fstab, it
                            could be good to simply restart the
                            instance. This restart needs to be made
                            from the instance itself, not via nova.
                            So, we SSH into the instance and perform a
                            reboot :
                            <screen>
<prompt>$</prompt> <userinput>shutdown -r now</userinput>
                             </screen></para>
                    </listitem>
                </itemizedlist>Voila! You successfully recovered your
                cloud after that. </para>
            <para>Here are some suggestions : </para>
            <para><itemizedlist>
                    <listitem>
                        <para> Use the <literal>parameter
                                errors=remount</literal> in the
                                <filename>fstab</filename> file, which
                            will prevent data corruption.</para>
                        <para> The system would lock any write to the
                            disk if it detects an I/O error. This
                            configuration option should be added into
                            the nova-volume server (the one which
                            performs the ISCSI connection to the SAN),
                            but also into the instances'
                                <filename>fstab</filename>
                            file.</para>
                    </listitem>
                    <listitem>
                        <para>Do not add the entry for the SAN's disks
                            to the nova-volume's
                                <filename>fstab</filename> file. </para>
                        <para>Some systems will  hang on that step,
                            which means you could lose access to your
                            cloud-controller. In order to re-run the
                            session manually, you would run the
                            following command before performing the
                            mount:
                            <screen>
<prompt>#</prompt> <userinput>iscsiadm -m discovery -t st -p $SAN_IP $ iscsiadm -m node --target-name $IQN -p $SAN_IP -l</userinput>
 </screen></para>
                    </listitem>
                    <listitem>
                        <para> For your instances, if you have the
                            whole <filename>/home/</filename>
                            directory on the disk, instead of emptying
                            the <filename>/home</filename> directory
                            and map the disk on it, leave a user's
                            directory with the user's bash files and
                            the <filename>authorized_keys</filename>
                            file. </para>
                        <para>This will allow you to connect to the
                            instance, even without the volume
                            attached, if you allow only connections
                            via public keys. </para>
                    </listitem>
            </itemizedlist>
                </para>
        </simplesect>
        <simplesect>
                    <title>C- Scripted DRP</title>
            <para>You can download from <link
                    xlink:href="https://github.com/Razique/BashStuff/blob/master/SYSTEMS/OpenStack/SCR_5006_V00_NUAC-OPENSTACK-DRP-OpenStack.sh"
                    >here</link> a bash script which performs these
                five steps : </para>
            <para>The "test mode" allows you to perform that whole sequence for only one
                instance.</para>
            <para>To reproduce the power loss,  connect to the compute
                node which runs that same instance and close the iscsi
                session. <emphasis role="underline">Do not dettach the
                    volume via
                    <command>nova volume-detach</command></emphasis>, but
                instead manually close the iscsi session. </para>
            <para>In the following example, the iscsi session is
                number 15 for that instance :
                    <screen>
<prompt>$</prompt> <userinput>iscsiadm -m session -u -r 15</userinput>
                    </screen><emphasis
                    role="bold">Do not forget the flag
                        -<literal>r</literal>; otherwise, you will
                    close ALL sessions</emphasis>.</para>
        </simplesect>
    </section>

</chapter>
