<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_installing-openstack-compute">
        <title>Installing OpenStack Compute</title>
        <para>The OpenStack system has several key projects that are separate installations but can
        work together depending on your cloud needs: OpenStack Compute, OpenStack Object Storage,
        and OpenStack Image Service. You can install any of these projects separately and then
        configure them either as standalone or connected entities.</para>
    <section xml:id="compute-system-requirements">
            <title>System Requirements</title>
            <para><emphasis role="bold">Hardware</emphasis>: OpenStack components are intended to
            run on standard hardware. Recommended hardware configurations for a minimum production
            deployment are as follows for the cloud controller nodes and compute nodes.</para>
        <table rules="all">
            <caption>Hardware Recommendations </caption>
            <col width="20%"/>
            <col width="23%"/>
            <col width="57%"/>
           
            <thead>
                <tr>
                    <td>Server</td>
                    <td>Recommended Hardware</td>
                    <td>Notes</td>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Cloud Controller node (runs network, volume, API, scheduler and image
                        services) </td>
                    <td>
                        <para>Processor: 64-bit x86</para>
                        <para>Memory: 12 GB RAM </para>
                        <para>Disk space: 30 GB (SATA or SAS or SSD) </para>
                        <para>Volume storage: two disks with 2 TB (SATA) for volumes attached to the
                            compute nodes </para>
                        <para>Network: one 1 GB Network Interface Card (NIC)</para>
                    </td>
                    <td>
                        <para>Two NICS are recommended but not required. A quad core server with 12
                            GB RAM would be more than sufficient for a cloud controller node.</para>
                        <para>32-bit processors will work for the cloud controller node. </para>
                    </td>
                </tr>
                <tr>
                    <td>Compute nodes (runs virtual instances)</td>
                    <td>
                        <para>Processor: 64-bit x86</para>
                        <para>Memory: 32 GB RAM</para>
                        <para>Disk space: 30 GB (SATA)</para>
                        <para>Network: two 1 GB NICs</para>
                    </td>
                    <td>
                        <para>Note that you cannot run 64-bit VM instances on a 32-bit compute node.
                            A 64-bit compute node can run either 32- or 64-bit VMs, however.</para>
                        <para>With 2 GB RAM you can run one m1.small instance on a node or three
                            m1.tiny instances without memory swapping, so 2 GB RAM would be a
                            minimum for a test-environment compute node. As an example, Rackspace
                            Cloud Builders use 96 GB RAM for compute nodes in OpenStack
                            deployments.</para>
                        <para>Specifically for virtualization on certain hypervisors on the node or
                            nodes running nova-compute, you need a x86 machine with an AMD processor
                            with SVM extensions (also called AMD-V) or an Intel processor with VT
                            (virtualization technology) extensions. </para>
                        <para>For Xen-based hypervisors, the Xen wiki contains a list of compatible
                            processors on the <link
                                xlink:href="http://wiki.xensource.com/xenwiki/HVM_Compatible_Processors"
                                >HVM Compatible Processors</link> page. For XenServer-compatible
                            Intel processors, refer to the <link
                                xlink:href="http://ark.intel.com/VTList.aspx">IntelÂ® Virtualization
                                Technology List</link>. </para>
                        <para>For LXC, the VT extensions are not required.</para>
                    </td>
                </tr>
            </tbody>
        </table>
        <para>
        <emphasis role="bold">Operating System</emphasis>: OpenStack currently has
            packages for the following distributions: Ubuntu, RHEL, SUSE, Debian, and Fedora. These
            packages are maintained by community members, refer to <link
                xlink:href="http://wiki.openstack.org/Packaging"
                >http://wiki.openstack.org/Packaging</link> for additional links. </para>
            <para><emphasis role="bold">Networking</emphasis>: 1000 Mbps are suggested. For
            OpenStack Compute, networking is configured on multi-node installations between the
            physical machines on a single subnet. For networking between virtual machine instances,
            three network options are available: flat, DHCP, and VLAN. Two NICs (Network Interface
            Cards) are recommended on the server running nova-network. </para>
            <para><emphasis role="bold">Database</emphasis>: For OpenStack Compute, you need access
            to either a PostgreSQL or MySQL database, or you can install it as part of the OpenStack
            Compute installation process.</para>
            <para><emphasis role="bold">Permissions</emphasis>: You can install OpenStack Compute
            either as root or as a user with sudo permissions if you configure the sudoers file to
            enable all the permissions. </para>
    </section><section xml:id="example-installation-architecture">
        <title>Example Installation Architectures</title>
        <para>OpenStack Compute uses a shared-nothing, messaging-based architecture. While very
            flexible, the fact that you can install each nova- service on an independent server
            means there are many possible methods for installing OpenStack Compute. The only
            co-dependency between possible multi-node installations is that the Dashboard must be
            installed nova-api server. Here are the types of installation architectures:</para>
       
            <itemizedlist>
                <listitem>
                        <para xmlns="http://docbook.org/ns/docbook">Single node: Only one server
                    runs all nova- services and also drives all the virtual instances. Use this
                    configuration only for trying out OpenStack Compute, or for development
                    purposes.</para></listitem>
                <listitem><para>Two nodes: A cloud controller node runs the nova- services except for nova-compute, and a
                    compute node runs nova-compute. A client computer is likely needed to bundle
                    images and interfacing to the servers, but a client is not required. Use this
                    configuration for proof of concepts or development environments. </para></listitem>
                        <listitem><para xmlns="http://docbook.org/ns/docbook">Multiple nodes: You can add more compute nodes to the
                    two node installation by simply installing nova-compute on an additional server
                    and copying a nova.conf file to the added node. This would result in a multiple
                    node installation. You can also add a volume controller and a network controller
                    as additional nodes in a more complex multiple node installation.  A minimum of
                    4 nodes is best for running multiple virtual instances that require a lot of
                    processing power.</para>
                </listitem>
            </itemizedlist>
      
        <para>This is an illustration of one possible multiple server installation of OpenStack
            Compute; virtual server networking in the cluster may vary.</para>
        
        <para><inlinemediaobject>
            <imageobject>
                <imagedata scale="80" fileref="../figures/NOVA_install_arch.png"/></imageobject>
            
        </inlinemediaobject></para>
        <para>An alternative architecture would be to add more messaging servers if you notice a lot
            of back up in the messaging queue causing performance problems. In that case you would
            add an additional RabbitMQ server in addition to or instead of scaling up the database
            server. Your installation can run any nova- service on any server as long as the
            nova.conf is configured to point to the RabbitMQ server and the server can send messages
            to the server.</para>
        <para>Multiple installation architectures are possible, here is another example
        illustration. </para>
        <para><inlinemediaobject>
            <imageobject>
                <imagedata scale="40" fileref="../figures/NOVA_compute_nodes.png"/></imageobject>
            
        </inlinemediaobject></para>
    </section>
    
    <section xml:id="service-architecture"><title>Service Architecture</title>
        <para>Because Compute has multiple services and many configurations are possible, here is a diagram showing the overall service architecture and communication systems between the services.</para>
        <para><inlinemediaobject>
            <imageobject>
                <imagedata scale="80" fileref="../figures/NOVA_ARCH.png"/></imageobject>
            
        </inlinemediaobject></para></section>
    <section xml:id="installing-openstack-compute-on-ubuntu">
        <title>Installing OpenStack Compute on Ubuntu </title>
        <para>How you go about installing OpenStack Compute depends on your goals for the
            installation. You can use an ISO image, you can use a scripted installation, and you can
            manually install with a step-by-step installation.</para>
        
        
        <section xml:id="iso-ubuntu-installation">
            <title>ISO Distribution Installation</title>
            <para>You can download and use an ISO image that is based on a Ubuntu Linux Server 10.04
                LTS distribution containing only the components needed to run OpenStack Compute. See
                    <link xlink:href="http://sourceforge.net/projects/stackops/files/"
                    >http://sourceforge.net/projects/stackops/files/</link> for download files and
                information, license information, and a README file. For documentation on the
                StackOps distro, see <link xlink:href="http://docs.stackops.org">http://docs.stackops.org</link>. For free support, go to
                <link xlink:href="http://getsatisfaction.com/stackops">http://getsatisfaction.com/stackops</link>.</para></section>
        <section xml:id="scripted-ubuntu-installation">
            <title>Scripted Installation</title>
            <para>You can download a script from GitHub at <link
                    xlink:href="https://github.com/elasticdog/OpenStack-NOVA-Installer-Script/raw/master/nova-install"
                    >https://github.com/elasticdog/OpenStack-NOVA-Installer-Script/raw/master/nova-install</link>.</para>
            <para>Copy the file to the servers where you want to install OpenStack Compute services
                - with multiple servers, you could install a cloud controller node and multiple
                compute nodes. The compute nodes manage the virtual machines through the
                nova-compute service. The cloud controller node contains all other nova-
                services.</para>
            <para>Ensure you can execute the script by modifying the permissions on the script
                file.</para>
            <literallayout class="monospaced">wget --no-check-certificate https://github.com/elasticdog/OpenStack-NOVA-Installer-Script/raw/master/nova-install
sudo chmod 755 nova-install</literallayout><para>You
                must run the script with root permissions. </para>
            <literallayout class="monospaced">sudo bash nova-install -t cloud</literallayout>
            <para>The way this script is designed, you can have multiple servers for the cloud
                controller, the messaging service, and the database server, or run it all on one
                server. The -t or -type parameter has two options: <code>nova-install -t
                    cloud</code> installs the cloud controller and <code>nova-install -t
                    compute</code> installs a compute node for an existing cloud controller.</para>
            <para>These are the parameters you enter using the script:</para>
            <para>
                <itemizedlist>
                    <listitem>
                        <para>Enter the Cloud Controller Host IP address.</para>
                    </listitem>
                    <listitem>
                        <para>Enter the S3 IP, or use the default address as the current server's IP
                            address.</para>
                    </listitem>
                    <listitem>
                        <para>Enter the RabbitMQ Host IP. Again, you can use the default to install
                            it to the local server. RabbitMQ will be installed. </para>
                    </listitem>
                    <listitem>
                        <para>Enter the MySQL host IP address.</para>
                    </listitem>
                    <listitem>
                        <para>Enter the MySQL root password and verify it.</para>
                    </listitem>
                    <listitem>
                        <para>Enter a network range for all projects in CIDR format.</para>
                    </listitem>
                </itemizedlist>
            </para>
            <para>The script uses all these values entered for the configuration information to
                create the nova.conf configuration file. The script also walks you through creating
                a user and project. Enter a user name and project name when prompted. After the script is finished, you also need to create the project zip file. Credentials are generated after you create the project zip file with <code>nova-manage project zipfile projname username</code></para>
            <para>After configuring OpenStack Compute and creating a project zip file using the nova-manage project create command, be sure to unizp the project zip file and then source the novarc
                credential file that you extracted. </para>
            <literallayout class="monospaced">source /root/creds/novarc                </literallayout>
            <para>Now all the necessary nova services are started up and you can begin to issue
                nova-manage commands. If you configured it to all run from one server, you're done.
                If you have a second server that you intend to use as a compute node (a node that
                does not contain the database), install the nova services on the second node using
                the -t compute parameters using the same nova-install script.</para>
                <para>To run from two or more servers, copy the nova.conf from the cloud controller node to the compute node. </para>

        </section>
        <section xml:id="manual-ubuntu-installation">
            <title>Manual Installation</title>
            <para>The manual installation involves installing from packages on Ubuntu 10.04 or 10.10
                as a user with root permission. Depending on your environment, you may need to
                prefix these commands with sudo.</para>
            <para>This installation process walks through installing a cloud controller node and a
                compute node. The cloud controller node contains all the nova- services including
                the API server and the database server. The compute node needs to run only the
                nova-compute service. You only need one nova-network service running in a multi-node
                install. You cannot install nova-objectstore on a different machine from
                nova-compute (production-style deployments will use a Glance server for virtual
                images).</para>
            <section xml:id="installing-the-cloud-controller">
                <title>Installing the Cloud Controller</title>
                <para>First, set up pre-requisites to use the Nova PPA (Personal Packages Archive)
                    provided through https://launchpad.net/~nova-core/+archive/trunk. The
                    âpython-software-propertiesâ package is a pre-requisite for setting up the nova
                    package repository. You can also use the release package by adding the
                    ppa:nova-core/release repository.</para>
                <literallayout class="monospaced">sudo apt-get install python-software-properties</literallayout>
                <literallayout class="monospaced">sudo add-apt-repository ppa:nova-core/trunk</literallayout>
                <para>Run update.</para>
                <literallayout class="monospaced">sudo apt-get update</literallayout>
                <para>Install the messaging queue server, RabbitMQ.</para>
                <literallayout class="monospaced">sudo apt-get install -y rabbitmq-server</literallayout>
                <para>Now, install the Python dependencies. </para>
                <literallayout class="monospaced">sudo apt-get install -y python-greenlet python-mysqldb </literallayout>
                <para>Install the required nova- packages, and dependencies should be automatically
                    installed.</para>
                <literallayout class="monospaced">sudo apt-get install -y nova-common nova-doc python-nova nova-api
                        nova-network nova-objectstore nova-scheduler nova-compute</literallayout>
                <para>Install the supplemental tools such as euca2ools and unzip.</para>
                <literallayout class="monospaced">sudo apt-get install -y euca2ools unzip</literallayout>

                <section xml:id="setting-up-sql-database-mysql">
                        <title>Setting up the SQL Database (MySQL) on the Cloud Controller</title>
                        <para>You must use a SQLAlchemy-compatible database, such as MySQL or
                            PostgreSQL. This example shows MySQL. </para>
                        <para>First you can set environments with a "pre-seed" line to bypass all
                            the installation prompts, running this as root: </para>
                        <para>
                           <literallayout class="monospaced">bash
MYSQL_PASS=nova
NOVA_PASS=notnova
cat &lt;&lt;MYSQL_PRESEED | debconf-set-selections
mysql-server-5.1 mysql-server/root_password password $MYSQL_PASS
mysql-server-5.1 mysql-server/root_password_again password $MYSQL_PASS
mysql-server-5.1 mysql-server/start_on_boot boolean true
MYSQL_PRESEED</literallayout>
                        </para>
                    <para>Next, install MySQL with: <code>sudo apt-get install -y
                            mysql-server</code>
                    </para>
                        <para>Edit /etc/mysql/my.cnf to change âbind-addressâ from localhost
                        (127.0.0.1) to any (0.0.0.0) and restart the mysql service: </para>
                    <para>
                        <literallayout class="monospaced">sudo sed -i 's/127.0.0.1/0.0.0.0/g' /etc/mysql/my.cnf
sudo service mysql restart</literallayout></para>
                        <para>To configure the MySQL database, create the nova database:  </para>
                        <literallayout class="monospaced">sudo mysql -uroot -p$MYSQL_PASS -e 'CREATE DATABASE nova;'</literallayout>

                        <para>Update the DB to give user ânovaâ@â%â full control of the nova
                        database:</para>
                    <para>
                        <literallayout class="monospaced">sudo mysql -uroot -p$MYSQL_PASS -e "GRANT ALL PRIVILEGES ON *.* TO
                            'nova'@'%' WITH GRANT OPTION;"</literallayout>
                    </para>
                        <para>Set MySQL password for 'nova'@'%':</para>
                    <para>
                        <literallayout class="monospaced">sudo mysql -uroot -p$MYSQL_PASS -e "SET PASSWORD FOR 'nova'@'%' =
                            PASSWORD('$NOVA_PASS');"</literallayout>
                    </para>
                    </section>

            </section>
            <section xml:id="installing-the-compute-node">
                <title>Installing the Compute Node</title>
                <para>There are many different ways to perform a multinode install of Compute. In
                    this case, you can install all the nova- packages and dependencies as you did
                    for the Cloud Controller node, or just install nova-network and nova-compute.
                    Your installation can run any nova- services anywhere, so long as the service
                    can access nova.conf so it knows where the rabbitmq server is installed.</para>
                <para>The Compute Node is where you configure the Compute network, the networking
                    between your instances. There are three options: flat, flatDHCP, and
                    VLAN.</para>
                <para>If you use FlatManager as your network manager, there are some additional
                    networking changes to ensure connectivity between your nodes and VMs. If you
                    chose VlanManager or FlatDHCP, you may skip this section because they are set up
                    for you automatically. </para>
                <para>Compute defaults to a bridge device named âbr100â. This needs to be created
                    and somehow integrated into your network. To keep things as simple as possible,
                    have all the VM guests on the same network as the VM hosts (the compute nodes).
                    To do so, set the compute nodeâs external IP address to be on the bridge and add
                    eth0 to that bridge. To do this, edit your network interfaces configuration to
                    look like the following example: </para>
                <para>
                    <literallayout class="monospaced">
&lt; begin /etc/network/interfaces >
# The loopback network interface
auto lo
iface lo inet loopback

# Networking for OpenStack Compute
auto br100

iface br100 inet dhcp
bridge_ports        eth0
bridge_stp           off
bridge_maxwait   0
bridge_fd            0
&lt; end /etc/network/interfaces >
</literallayout>
                </para>
                <para>Next, restart networking to apply the changes: </para>
                <literallayout class="monospaced">sudo /etc/init.d/networking restart</literallayout>
                <para>If you use flat networking, you must manually insert the IP address into the
                    'fixed_ips' table in the nova database. Also ensure that the database lists the
                    bridge name correctly that matches the network configuration you are working
                    within. Flat networking should insert this automatically but you may need to
                    check it.</para>
                <para>Because you may need to query the database from the Compute node and learn
                    more information about instances, euca2ools and mysql-client packages should be
                    installed on any additional Compute nodes.</para>
            </section>
            <section xml:id="restart-nova-services">
                <title>Restart All Relevant Services on the Compute Node</title>
                <para>On both nodes, restart all six services in total, just to cover the entire
                    spectrum: </para>
                <para>
                    <literallayout class="monospaced">restart libvirt-bin; restart nova-network; restart nova-compute;
restart nova-api; restart nova-objectstore; restart nova-scheduler</literallayout>
                </para>
            </section>
        </section>
    </section>
    <section xml:id="installing-openstack-compute-on-rhel6">
        <title>Installing OpenStack Compute on Red Hat Enterprise Linux 6 </title>
        <para>This section documents a multi-node installation using RHEL 6. RPM repos for the Bexar
            release, the Cactus release, and also per-commit trunk builds for OpenStack Nova are
            available at <link xlink:href="http://yum.griddynamics.net"
                >http://yum.griddynamics.net</link>. </para>
        
        <para>Known limitations for RHEL version 6 installations: </para>

<itemizedlist><listitem><para>iSCSI LUN not supported due to tgtadm vs ietadm differences</para></listitem>
<listitem><para>Only KVM hypervisor has been tested with this installation</para></listitem></itemizedlist>
        <para>To install Nova on RHEL v.6 you need access to two repositories, one available on the
            yum.griddynamics.net website and the RHEL DVD image connected as repo. </para>
        
        <para>First, install RHEL 6.0, preferrably with a minimal set of packages.</para>
        <para>Disable SELinux in /etc/sysconfig/selinux and then reboot. </para>
        <para>Connect the RHEL 3. 6.0 x86_64 DVD as a repository in YUM. </para>
            
            <literallayout class="monospaced">sudo mount /dev/cdrom /mnt/cdrom
cat /etc/yum.repos.d/rhel.repo 
[rhel]
name=RHEL 6.0
baseurl=file:///mnt/cdrom/Server
enabled=1
gpgcheck=0</literallayout>
        <para>Download and install repo config and key.</para>
        <literallayout class="monospaced">wget http://yum.griddynamics.net/openstack-repo-2011.1-2.noarch.rpm
sudo rpm -i openstack-repo-2011.1-2.noarch.rpm</literallayout>
        <para>Install the libvirt package (these instructions are tested only on KVM). </para>
        <literallayout class="monospaced">sudo yum install libvirt
sudo chkconfig libvirtd on
sudo service libvirtd start</literallayout>
        <para>Repeat the basic installation steps to put the pre-requisites on all cloud controller and compute nodes. Nova has many different possible configurations. You can install Nova services on separate servers as needed but these are the basic pre-reqs.</para>
        <para>These are the basic packages to install for a cloud controller node:</para>
        <literallayout class="monospaced">sudo yum install euca2ools openstack-nova-{api,compute,network,objectstore,scheduler,volume} openstack-nova-cc-config openstack-glance</literallayout>
        <para>These are the basic packages to install compute nodes. Repeat for each compute node (the node that runs the VMs) that you want to install.</para>
        <literallayout class="monospaced">sudo yum install openstack-nova-compute openstack-nova-compute-config</literallayout>
        <para>On the cloud controller node, create a MySQL database named nova. </para>
        <literallayout class="monospaced">sudo service mysqld start
sudo chkconfig mysqld on
sudo service rabbitmq-server start
sudo chkconfig rabbitmq-server on
mysqladmin -uroot password nova</literallayout>
        <para>You can use this script to create the database. </para>
        <literallayout class="monospaced">#!/bin/bash

DB_NAME=nova
DB_USER=nova
DB_PASS=nova
PWD=nova

CC_HOST="A.B.C.D" # IPv4 address
HOSTS='node1 node2 node3' # compute nodes list

mysqladmin -uroot -p$PWD -f drop nova
mysqladmin -uroot -p$PWD create nova

for h in $HOSTS localhost; do
        echo "GRANT ALL PRIVILEGES ON $DB_NAME.* TO '$DB_USER'@'$h' IDENTIFIED BY '$DB_PASS';" | mysql -uroot -p$DB_PASS mysql
done
echo "GRANT ALL PRIVILEGES ON $DB_NAME.* TO $DB_USER IDENTIFIED BY '$DB_PASS';" | mysql -uroot -p$DB_PASS mysql
echo "GRANT ALL PRIVILEGES ON $DB_NAME.* TO root IDENTIFIED BY '$DB_PASS';" | mysql -uroot -p$DB_PASS mysql </literallayout>
        <para>Now, ensure the database version matches the version of nova that you are installing:</para>
        <literallayout class="monospaced">nova-manage db sync</literallayout>
        
        <para>On each node, set up the configuration file in /etc/nova/nova.conf.</para>
        <para>Start the Nova services after configuring and you then are running an OpenStack
            cloud!</para>
        <literallayout class="monospaced">for n in api compute network objectstore scheduler volume; do sudo service openstack-nova-$n start; done
sudo service openstack-glance start
for n in node1 node2 node3; do ssh $n sudo service openstack-nova-compute start; done</literallayout>
        </section>
    <section xml:id="configuring-openstack-compute-basics">
        <title>Post-Installation Configuration for OpenStack Compute</title>
        <para>Configuring your Compute installation involves nova-manage commands plus editing the
            nova.conf file to ensure the correct flags are set. This section contains the basics for
            a simple multi-node installation, but Compute can be configured many ways. You can find
            networking options and hypervisor options described in separate chapters, and you will
            read about additional configuration information in a separate chapter as well.</para>
        <section xml:id="setting-flags-in-nova-conf-file">
            <title>Setting Flags in the nova.conf File</title>
            <para>The configuration file nova.conf is installed in /etc/nova by default. You only
                need to do these steps when installing manually, the scripted installation above
                does this configuration during the installation. A default set of options are
                already configured in nova.conf when you install manually. The defaults are as
                follows:</para>
            <literallayout class="monospaced">--daemonize=1
--dhcpbridge_flagfile=/etc/nova/nova.conf
--dhcpbridge=/usr/bin/nova-dhcpbridge
--logdir=/var/log/nova
--state_path=/var/lib/nova                </literallayout>
            <para>Starting with the default file, you must define the following required items in
                /etc/nova/nova.conf. The flag variables are described below. You can place
                comments in the nova.conf file by entering a new line with a # sign at the beginning of the line. To see a listing of all possible flag settings, see
                the output of running /bin/nova-api --help.</para>
            <table rules="all">
                <caption>Description of nova.conf flags (not comprehensive)</caption>
                <thead>
                    <tr>
                        <td>Flag</td>
                        <td>Description</td>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>--sql_connection</td>
                        <td>IP address; Location of OpenStack Compute SQL database</td>
                    </tr>
                    <tr>
                        <td>--s3_host</td>
                        <td>IP address; Location where OpenStack Compute is hosting the objectstore
                            service, which will contain the virtual machine images and buckets</td>
                    </tr>
                    <tr>
                        <td>--rabbit_host</td>
                        <td>IP address; Location of OpenStack Compute SQL database</td>
                    </tr>
                    <tr>
                        <td>--ec2_api</td>
                        <td>IP address; Location where the nova-api service runs</td>
                    </tr>
                    <tr>
                        <td>--verbose</td>
                        <td>Set to 1 to turn on; Optional but helpful during initial setup</td>
                    </tr>
                    <tr>
                        <td>--ec2_url</td>
                        <td>HTTP URL; Location to interface nova-api. Example:
                            http://184.106.239.134:8773/services/Cloud</td>
                    </tr>
                    <tr>
                        <td>--network_manager</td>
                        <td>
                            <para>Configures how your controller will communicate with additional
                                OpenStack Compute nodes and virtual machines. Options: </para>
                            <itemizedlist>
                                <listitem>
                                    <para>nova.network.manager.FlatManager</para>
                                    <para>Simple, non-VLAN networking</para>
                                </listitem>
                                <listitem>
                                    <para>nova.network.manager.FlatDHCPManager</para>
                                    <para>Flat networking with DHCP</para>
                                </listitem>
                                <listitem>
                                    <para>nova.network.manager.VlanManager</para>
                                    <para>VLAN networking with DHCP; This is the Default if no
                                        network manager is defined here in nova.conf. </para>
                                </listitem>
                            </itemizedlist>
                        </td>
                    </tr>
                    <tr>
                        <td>--fixed_range</td>
                        <td>IP address/range; Network prefix for the IP network that all the
                            projects for future VM guests reside on. Example: 192.168.0.0/12</td>
                    </tr>
                    <tr>
                        <td>--network_size</td>
                        <td>Number value; Number of addresses in each private subnet.</td>
                    </tr>

                </tbody>
            </table>
            <para>Here is a simple example nova.conf file for a small private cloud, with all the
                cloud controller services, database server, and messaging server on the same
                server.</para>
            <literallayout class="monospaced">--dhcpbridge_flagfile=/etc/nova/nova.conf
--dhcpbridge=/usr/bin/nova-dhcpbridge
--logdir=/var/log/nova
--state_path=/var/lib/nova
--verbose
--s3_host=184.106.239.134
--rabbit_host=184.106.239.134
--ec2_api=184.106.239.134
--ec2_url=http://184.106.239.134:8773/services/Cloud
--fixed_range=192.168.0.0/16
--network_size=8
--routing_source_ip=184.106.239.134
--sql_connection=mysql://nova:notnova@184.106.239.134/nova            </literallayout>
            <para>Create a ânovaâ group, so you can set permissions on the configuration file: </para>
            <literallayout class="monospaced">sudo addgroup nova</literallayout>
            <para>The nova.config file should have its owner set to root:nova, and mode set to 0640,
                since the file contains your MySQL serverâs username and password. </para>
            <literallayout class="monospaced">chown -R root:nova /etc/nova
chmod 640 /etc/nova/nova.conf</literallayout>
        </section><section xml:id="setting-up-openstack-compute-environment-on-the-compute-node">
                <title>Setting Up OpenStack Compute Environment on the Compute Node</title>
                <para>These are the commands you run to ensure the database schema is current, and
                then set up a user and project: </para>
                <para>
<literallayout class="monospaced">/usr/bin/nova-manage db sync
/usr/bin/nova-manage user admin &lt;user_name>
/usr/bin/nova-manage project create &lt;project_name> &lt;user_name>
/usr/bin/nova-manage network create &lt;project-network> &lt;number-of-networks-in-project> &lt;addresses-in-each-network></literallayout></para>
                <para>Here is an example of what this looks like with real values entered: </para>
                <literallayout class="monospaced">/usr/bin/nova-manage db sync
/usr/bin/nova-manage user admin dub
/usr/bin/nova-manage project create dubproject dub
/usr/bin/nova-manage network create 192.168.0.0/24 1 256 </literallayout>
                <para>For this example, the number of IPs is /24 since that falls inside the /16
                    range that was set in âfixed-rangeâ in nova.conf. Currently, there can only be
                    one network, and this set up would use the max IPs available in a /24. You can
                    choose values that let you use any valid amount that you would like. </para>
                <para>The nova-manage service assumes that the first IP address is your network
                (like 192.168.0.0), that the 2nd IP is your gateway (192.168.0.1), and that the
                broadcast is the very last IP in the range you defined (192.168.0.255). If this is
                not the case you will need to manually edit the sql db ânetworksâ table.o. </para>
            <para>When you run the <code>nova-manage network create</code> command, entries are made
                in the ânetworksâ and âfixed_ipsâ table. However, one of the networks listed in the
                ânetworksâ table needs to be marked as bridge in order for the code to know that a
                bridge exists. The network in the Nova networks table is marked as bridged
                automatically for Flat Manager.</para>
            </section>
        <section xml:id="creating-certifications">
                <title>Creating Certifications</title>
                <para>Generate the certifications as a zip file. These are the certs you will use to
                    launch instances, bundle images, and all the other assorted API functions. </para>
                <para>
                    <literallayout class="monospaced">mkdir âp /root/creds
/usr/bin/python /usr/bin/nova-manage project zipfile $NOVA_PROJECT $NOVA_PROJECT_USER /root/creds/novacreds.zip</literallayout>
                </para>
            <para>If you are using one of the Flat modes for networking, you may see a Warning
                message "No vpn data for project &lt;project_name>" which you can safely
                ignore.</para>
                <para>Unzip them in your home directory, and add them to your environment. </para>
                <literallayout class="monospaced">unzip /root/creds/novacreds.zip -d /root/creds/
cat /root/creds/novarc >> ~/.bashrc
source ~/.bashrc </literallayout>
            </section>
        <section xml:id="enabling-access-to-vms-on-the-compute-node">
                <title>Enabling Access to VMs on the Compute Node</title>
                <para>One of the most commonly missed configuration areas is not allowing the proper
                    access to VMs. Use the âeuca-authorizeâ command to enable access. Below, you
                    will find the commands to allow âpingâ and âsshâ to your VMs: </para>
                <literallayout class="monospaced">euca-authorize -P icmp -t -1:-1 default
euca-authorize -P tcp -p 22 default</literallayout>
                <para>Another
                    common issue is you cannot ping or SSH your instances after issuing the
                    âeuca-authorizeâ commands. Something to look at is the amount of âdnsmasqâ
                    processes that are running. If you have a running instance, check to see that
                    TWO âdnsmasqâ processes are running. If not, perform the following:</para>
                        <literallayout class="monospaced">killall dnsmasq 
service nova-network restart</literallayout>
            </section>
        <section xml:id="configuring-multiple-compute-nodes">
            <title>Configuring Multiple Compute Nodes</title><para>If your goal is to split your VM load across more than one server, you can connect an
                additional nova-compute node to a cloud controller node. This configuring can be
                reproduced on multiple compute servers to start building a true multi-node OpenStack
                Compute cluster. </para><para>To build out and scale the Compute platform, you spread out services amongst many servers.
                While there are additional ways to accomplish the build-out, this section describes
                adding compute nodes, and the service we are scaling out is called
                'nova-compute.'</para>
            <para>With the Bexar release we have two configuration files: nova-api.conf and nova.conf. For a multi-node install you only make changes to nova.conf and copy it to additional compute nodes. Ensure each nova.conf file points to the correct IP addresses for the respective services. Customize the nova.config example below to match your environment. The CC_ADDR is the Cloud Controller IP Address.
                   </para>
                  <literallayout class="monospaced">
                   --dhcpbridge_flagfile=/etc/nova/nova.conf
                   --dhcpbridge=/usr/bin/nova-dhcpbridge
                   --logdir=/var/log/nova
                   --state_path=/var/lib/nova
                   --verbose
                   --sql_connection=mysql://root:nova@CC_ADDR/nova
                   --s3_host=CC_ADDR
                   --rabbit_host=CC_ADDR
                   --ec2_api=CC_ADDR
                   --ec2_url=http://CC_ADDR:8773/services/Cloud
                   --network_manager=nova.network.manager.FlatManager
                   --fixed_range= network/CIDR
                   --network_size=number of addresses</literallayout><para>By default, Nova sets 'br100' as the bridge device, and this is what needs to be done next.  Edit /etc/network/interfaces with the following template, updated with your IP information. </para>


<literallayout class="monospaced">
            # The loopback network interface
            auto lo
            iface lo inet loopback

            # The primary network interface
            auto br100
            iface br100 inet static
            bridge_ports    eth0
            bridge_stp      off
            bridge_maxwait  0
            bridge_fd       0
            address xxx.xxx.xxx.xxx
            netmask xxx.xxx.xxx.xxx
            network xxx.xxx.xxx.xxx
            broadcast xxx.xxx.xxx.xxx
            gateway xxx.xxx.xxx.xxx
            # dns-* options are implemented by the resolvconf package, if installed
            dns-nameservers xxx.xxx.xxx.xxx</literallayout>

            <para>Restart networking:</para>

            <literallayout class="monospaced">/etc/init.d/networking restart</literallayout>
            <para>With nova.conf updated and networking set, configuration is nearly complete.  First, lets bounce the relevant services to take the latest updates:</para>

            <literallayout class="monospaced">restart libvirt-bin; service nova-compute restart</literallayout>
            <para>To avoid issues with KVM and permissions with Nova, run the following commands to ensure we have VM's that are running optimally:</para>

<literallayout class="monospaced">chgrp kvm /dev/kvm
chmod g+rwx /dev/kvm</literallayout>
            <para>If you want to use the 10.04 Ubuntu Enterprise Cloud images that are readily available at http://uec-images.ubuntu.com/releases/10.04/release/, you may run into delays with booting. Any server that does not have nova-api running on it needs this iptables entry so that UEC images can get metadata info. On compute nodes, configure the iptables with this next step:</para>

           <literallayout class="monospaced"> # iptables -t nat -A PREROUTING -d 169.254.169.254/32 -p tcp -m tcp --dport 80 -j DNAT --to-destination $NOVA_API_IP:8773</literallayout>

            <para>Lastly, confirm that your compute node is talking to your cloud controller. From the cloud controller, run this database query:</para>
            <literallayout class="monospaced">mysql -u$MYSQL_USER -p$MYSQL_PASS nova -e 'select * from services;'</literallayout>
            <para>In return, you should see something similar to this:</para>

            <literallayout class="monospaced">            +---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
            | created_at          | updated_at          | deleted_at | deleted | id | host     | binary         | topic     | report_count | disabled | availability_zone |
            +---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+
            | 2011-01-28 22:52:46 | 2011-02-03 06:55:48 | NULL       |       0 |  1 | osdemo02 | nova-network   | network   |        46064 |        0 | nova              |
            | 2011-01-28 22:52:48 | 2011-02-03 06:55:57 | NULL       |       0 |  2 | osdemo02 | nova-compute   | compute   |        46056 |        0 | nova              |
            | 2011-01-28 22:52:52 | 2011-02-03 06:55:50 | NULL       |       0 |  3 | osdemo02 | nova-scheduler | scheduler |        46065 |        0 | nova              |
            | 2011-01-29 23:49:29 | 2011-02-03 06:54:26 | NULL       |       0 |  4 | osdemo01 | nova-compute   | compute   |        37050 |        0 | nova              |
            | 2011-01-30 23:42:24 | 2011-02-03 06:55:44 | NULL       |       0 |  9 | osdemo04 | nova-compute   | compute   |        28484 |        0 | nova              |
            | 2011-01-30 21:27:28 | 2011-02-03 06:54:23 | NULL       |       0 |  8 | osdemo05 | nova-compute   | compute   |        29284 |        0 | nova              |
            +---------------------+---------------------+------------+---------+----+----------+----------------+-----------+--------------+----------+-------------------+</literallayout>

            <para>You can see that 'osdemo0{1,2,4,5} are all running 'nova-compute.'  When you start spinning up instances, they will allocate on any node that is running nova-compute from this list.</para>
            
        </section>
        <section xml:id="determining-version-of-compute">
            
            <title>Determining the Version of Compute</title>
            <para>In the Diablo release, you can find the version of the installation by using the
                nova-manage command:</para>
            <literallayout class="monospaced">nova-manage version list</literallayout>
        </section>
        <section xml:id="migrating-from-cactus-to-diablo"><title>Migrating from Cactus to Diablo</title>
            <para>If you have an installation already installed and running, to migrate to Diablo
                you must update the installation first, then your database, then perhaps your images
                if you were already running images in the nova-objectstore. You can also export your
                users for importing into the OpenStack Identity Service (Keystone). </para>
            <para>Here are the overall steps for upgrading the Image Service.</para>
            <para>Download and install the Diablo Glance packages.</para>
            <para>Migrate the registry database schema by running:</para>
            <literallayout class="monospaced"> glance-manage db_sync </literallayout>
            <para>Update configuration files, including the glance-api.conf and glance-registry.conf configuration files by using the examples in the examples/paste directory for the Diablo release.</para>
            <para>Here are the overall steps for upgrading Compute. </para>
            <para>If your installation already pointed to ppa:nova-core/release, the release
                package has been updated from Cactus to Diablo so you can simply run: </para>
            <literallayout class="monospaced">apt-get update
apt-get upgrade</literallayout>
            <para>Next, update the database schema. </para><literallayout class="monospaced">nova-manage db sync</literallayout>
            <para>Restart all the nova- services. </para>
            <para>A separate command is available to migrate users from the deprecated auth system to the Identity Service. </para>
            <literallayout class="monospaced">nova-manage shell export</literallayout>
            <para>Within the Keystone project there is a keystone-import script that you can run to
                import these users.</para>
            <para>Make sure that you can launch images. You can convert images that were previously stored in the nova object store using this command: </para>
            <literallayout class="monospaced">nova-manage image convert /var/lib/nova/images</literallayout>
           
        </section>
    </section>
    </chapter>
