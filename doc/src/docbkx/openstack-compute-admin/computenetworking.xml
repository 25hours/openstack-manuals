<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_networking">
    <title>Networking with nova-network</title>
    <para>By understanding the available networking configuration
        options you can design the best configuration for your
        OpenStack Compute instances.</para>
    <section xml:id="networking-options">
        <title>Networking Options</title>
        <para>This section offers a brief overview of each concept in
            networking for Compute. With the Grizzly release, you can
            choose to either install and configure nova-network for
            networking between VMs or use the Networking service
            (neutron) for networking. To configure Compute networking
            options with Neutron, see <link
                xlink:href="http://docs.openstack.org/trunk/openstack-network/admin/content/"
                >Network Administration Guide</link>.</para>
        <para>For each VM instance, Compute assigns to it a private IP
            address. (Currently, Compute with nova-network only
            supports Linux bridge networking that allows the virtual
            interfaces to connect to the outside network through the
            physical interface.)</para>
        <para>The network controller with nova-network provides
            virtual networks to enable compute servers to interact
            with each other and with the public network.</para>
        <para>Currently, Compute with nova-network supports three
            kinds of networks, implemented in three “Network Manager” types:<itemizedlist>
                <listitem>
                    <para>Flat Network Manager</para>
                </listitem>
                <listitem>
                    <para>Flat DHCP Network Manager</para>
                </listitem>
                <listitem>
                    <para>VLAN Network Manager</para>
                </listitem>
            </itemizedlist></para>
        <para>The three kinds of networks can co-exist in a cloud
            system. However, since you can't yet select the type of
            network for a given project, you cannot configure more
            than one type of network in a given Compute
            installation.</para>
        <note>
            <para>All networking options require network
                connectivity to be already set up between OpenStack
                physical nodes. OpenStack will not configure any
                physical network interfaces. OpenStack will
                automatically create all network bridges (i.e., br100)
                and VM virtual interfaces. </para>
            <para>All machines must have a <emphasis role="italic"
                    >public</emphasis> and <emphasis role="italic"
                    >internal</emphasis> network interface (controlled
                by the options: <literal>public_interface</literal>
                for the public interface, and
                    <literal>flat_interface</literal> and
                    <literal>vlan_interface</literal> for the internal
                interface with flat / VLAN managers). </para>
            <para>The internal network interface is used for
                communication with VMs, it shouldn't have an IP
                address attached to it before OpenStack installation
                (it serves merely as a fabric where the actual
                endpoints are VMs and dnsmasq). Also, the internal
                network interface must be put in <emphasis
                    role="italic">promiscuous mode</emphasis>, because
                it will have to receive packets whose target MAC
                address is of the guest VM, not of the host.</para>
        </note>
        <para>All the network managers configure the network using
                <emphasis role="italic">network drivers</emphasis>,
            e.g. the linux L3 driver (<literal>l3.py</literal> and
                <literal>linux_net.py</literal>) which makes use of
                <literal>iptables</literal>, <literal>route</literal>
            and other network management facilities, and also of
            libvirt's <link
                xlink:href="http://libvirt.org/formatnwfilter.html"
                >network filtering facilities</link>. The driver isn't
            tied to any particular network manager; all network
            managers use the same driver. The driver usually
            initializes (creates bridges etc.) only when the first VM
            lands on this host node. </para>
        <para>All network managers operate in either <emphasis
                role="italic">single-host</emphasis> or <emphasis
                role="italic">multi-host</emphasis> mode. This choice
            greatly influences the network configuration. In
            single-host mode, there is just 1 instance of
                <literal>nova-network</literal> which is used as a
            default gateway for VMs and hosts a single DHCP server
            (dnsmasq), whereas in multi-host mode every compute node
            has its own <literal>nova-network</literal>. In any case,
            all traffic between VMs and the outer world flows through
                <literal>nova-network</literal>. There are pros and
            cons to both modes, read more in <link
                linkend="existing-ha-networking-options">Existing High
                Availability Options</link>.</para>
        <para>Compute makes a distinction between <emphasis
                role="italic">fixed IPs</emphasis> and <emphasis
                role="italic">floating IPs</emphasis> for VM
            instances. Fixed IPs are IP addresses that are assigned to
            an instance on creation and stay the same until the
            instance is explicitly terminated. By contrast, floating
            IPs are addresses that can be dynamically associated with
            an instance. A floating IP address can be disassociated
            and associated with another instance at any time. A user
            can reserve a floating IP for their project. </para>
        <para>In <emphasis role="bold">Flat Mode</emphasis>, a network
            administrator specifies a subnet. The IP addresses for VM
            instances are grabbed from the subnet, and then injected
            into the image on launch. Each instance receives a fixed
            IP address from the pool of available addresses. A system
            administrator may create the Linux networking bridge
            (typically named <literal>br100</literal>, although this
            configurable) on the systems running the
                <literal>nova-network</literal> service. All instances
            of the system are attached to the same bridge, configured
            manually by the network administrator. </para>
        <para>
            <note>
                <para>The configuration injection currently only works
                    on Linux-style systems that keep networking
                    configuration in
                        <filename>/etc/network/interfaces</filename>.</para>
            </note>
        </para>
        <para>In <emphasis role="bold">Flat DHCP Mode</emphasis>,
            OpenStack starts a DHCP server (dnsmasq) to pass out IP
            addresses to VM instances from the specified subnet in
            addition to manually configuring the networking bridge. IP
            addresses for VM instances are grabbed from a subnet
            specified by the network administrator. </para>
        <para>Like Flat Mode, all instances are attached to a single
            bridge on the compute node. In addition a DHCP server is
            running to configure instances (depending on
            single-/multi-host mode, alongside each
                <literal>nova-network</literal>). In this mode,
            Compute does a bit more configuration in that it attempts
            to bridge into an ethernet device
                (<literal>flat_interface</literal>, eth0 by default).
            It will also run and configure dnsmasq as a DHCP server
            listening on this bridge, usually on IP address 10.0.0.1
            (see <link linkend="dnsmasq">DHCP server: dnsmasq</link>).
            For every instance, nova will allocate a fixed IP address
            and configure dnsmasq with the MAC/IP pair for the VM,
            i.e. dnsmasq doesn't take part in the IP address
            allocation process, it only hands out IPs according to the
            mapping done by nova. Instances receive their fixed IPs by
            doing a dhcpdiscover. These IPs are <emphasis
                role="italic">not</emphasis> assigned to any of the
            host's network interfaces, only to the VM's guest-side
            interface.</para>

        <para>In any setup with flat networking, the host(-s) with
            nova-network on it is (are) responsible for forwarding
            traffic from the private network. Compute can determine
            the NAT entries for each network when you have
                <literal>fixed_range=''</literal> in your
                <filename>nova.conf</filename>. Sometimes NAT is not
            used, such as when fixed_range is configured with all
            public IPs and a hardware router is used (one of the HA
            options). Such host(-s) needs to have
                <literal>br100</literal> configured and physically
            connected to any other nodes that are hosting VMs. You
            must set the <literal>flat_network_bridge</literal> option
            or create networks with the bridge parameter in order to
            avoid raising an error. Compute nodes have
            iptables/ebtables entries created per project and instance
            to protect against IP/MAC address spoofing and ARP
            poisoning. <note>
                <para>To use the new dynamic
                        <literal>fixed_range</literal> setup in
                    Grizzly, set <literal>fixed_range=''</literal> in
                    your <filename>nova.conf</filename>. For backwards
                    compatibility, Grizzly supports the
                        <literal>fixed_range</literal> option and if
                    set will perform the default logic from Folsom and
                    earlier releases.</para>
            </note></para>
        <note>
            <para>In single-host Flat DHCP mode you <emphasis
                    role="italic">will</emphasis> be able to ping VMs
                through their fixed IP from the nova-network node, but
                you <emphasis role="italic">will not</emphasis> be
                able to ping them from the compute nodes. This is
                expected behavior.</para>
        </note>

        <para><emphasis role="bold">VLAN Network Mode is the default
                mode</emphasis> for OpenStack Compute. In this mode,
            Compute creates a VLAN and bridge for each project. For
            multiple machine installation, the VLAN Network Mode
            requires a switch that supports VLAN tagging (IEEE
            802.1Q). The project gets a range of private IPs that are
            only accessible from inside the VLAN. In order for a user
            to access the instances in their project, a special VPN
            instance (code named cloudpipe) needs to be created.
            Compute generates a certificate and key for the user to
            access the VPN and starts the VPN automatically. It
            provides a private network segment for each project's
            instances that can be accessed through a dedicated VPN
            connection from the Internet. In this mode, each project
            gets its own VLAN, Linux networking bridge, and subnet. </para>

        <para>The subnets are specified by the network administrator,
            and are assigned dynamically to a project when required. A
            DHCP Server is started for each VLAN to pass out IP
            addresses to VM instances from the subnet assigned to the
            project. All instances belonging to one project are
            bridged into the same VLAN for that project. OpenStack
            Compute creates the Linux networking bridges and VLANs
            when required.</para>
    </section>
    <section xml:id="dnsmasq">
        <title>DHCP server: dnsmasq</title>
        <para>The Compute service uses <link
                xlink:href="http://www.thekelleys.org.uk/dnsmasq/doc.html"
                >dnsmasq</link> as the DHCP server when running with
            either that Flat DHCP Network Manager or the VLAN Network
            Manager. The <systemitem class="service"
                >nova-network</systemitem> service is responsible for
            starting up dnsmasq processes.</para>
        <para>The behavior of dnsmasq can be customized by creating a
            dnsmasq configuration file. Specify the config file using
            the <literal>dnsmasq_config_file</literal> configuration
            option. For
            example:<programlisting>dnsmasq_config_file=/etc/dnsmasq-nova.conf</programlisting>See
            the <link linkend="existing-ha-networking-options">high
                availability section</link> for an example of how to
            change the behavior of dnsmasq using a dnsmasq
            configuration file. The dnsmasq documentation has a more
            comprehensive <link
                xlink:href="http://www.thekelleys.org.uk/dnsmasq/docs/dnsmasq.conf.example"
                >dnsmasq configuration file example</link>.</para>
        <para>Dnsmasq also acts as a caching DNS server for instances.
            You can explicitly specify the DNS server that dnsmasq
            should use by setting the <literal>dns_server</literal>
            configuration option in
                <filename>/etc/nova/nova.conf</filename>. The
            following example would configure dnsmasq to use Google's
            public DNS
            server:<programlisting>dns_server=8.8.8.8</programlisting></para>
        <para>Dnsmasq logging output goes to the syslog (typically
                <filename>/var/log/syslog</filename> or
                <filename>/var/log/messages</filename>, depending on
            Linux distribution). The dnsmasq logging output can be
            useful for troubleshooting if VM instances boot
            successfully but are not reachable over the
            network.</para>
        <para>A network administrator can run <code>nova-manage fixed
                reserve
                --address=<replaceable>x.x.x.x</replaceable></code> to
            specify the starting point IP address (x.x.x.x) to reserve
            with the DHCP server. This reservation only affects which
            IP address the VMs start at, not the fixed IP addresses
            that the nova-network service places on the
            bridges.</para>
    </section>
    <section xml:id="metadata-service">
        <title>Metadata service</title>
        <simplesect>
            <title>Introduction</title>

            <para>The Compute service uses a special metadata service
                to enable virtual machine instances to retrieve
                instance-specific data. Instances access the metadata
                service at <literal>http://169.254.169.254</literal>.
                The metadata service supports two sets of APIs: an
                OpenStack metadata API and an EC2-compatible API. Each
                of the APIs is versioned by date. </para>
            <para>To retrieve a list of supported versions for the
                OpenStack metadata API, make a GET request to
                <programlisting>http://169.254.169.254/openstack</programlisting>For

                example:</para>
            <para><screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/openstack</userinput>
<computeroutput>2012-08-10
latest</computeroutput></screen>
                To retrieve a list of supported versions for the
                EC2-compatible metadata API, make a GET request to
                <programlisting>http://169.254.169.254</programlisting></para>
            <para>For example:</para>
            <para>
                <screen><prompt>$</prompt> <userinput>curl http://169.254.169.254</userinput>
<computeroutput>1.0
2007-01-19
2007-03-01
2007-08-29
2007-10-10
2007-12-15
2008-02-01
2008-09-01
2009-04-04
latest</computeroutput></screen>
            </para>
            <para>If you write a consumer for one of these APIs,
                always attempt to access the most recent API version
                supported by your consumer first, then fall back to an
                earlier version if the most recent one is not
                available.</para>
        </simplesect>
        <simplesect>
            <title>OpenStack metadata API</title>
            <para>Metadata from the OpenStack API is distributed in
                JSON format. To retrieve the metadata, make a GET
                request to
                <programlisting>http://169.254.169.254/openstack/2012-08-10/meta_data.json</programlisting>
                For
                example:<screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/openstack/2012-08-10/meta_data.json</userinput>
{"uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38", "availability_zone": "nova", "hostname": "test.novalocal", "launch_index": 0, "meta": {"priority": "low", "role": "webserver"}, "public_keys": {"mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova\n"}, "name": "test"}</screen>Here
                is the same content after having run through a JSON
                pretty-printer:</para>
            <para>
                <programlisting>{
    "availability_zone": "nova",
    "hostname": "test.novalocal",
    "launch_index": 0,
    "meta": {
        "priority": "low",
        "role": "webserver"
    },
    "name": "test",
    "public_keys": {
        "mykey": "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova\n"
    },
    "uuid": "d8e02d56-2648-49a3-bf97-6be8f1204f38"
}
</programlisting>
            </para>
            <para>Instances also retrieve user data (passed as the
                    <literal>user_data</literal> parameter in the API
                call or by the <literal>--user_data</literal> flag in
                the <command>nova boot</command> command) through the
                metadata service, by making a GET request
                to:<programlisting>http://169.254.169.254/openstack/2012-08-10/user_data</programlisting>For
                example:</para>
            <para>
                <screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/openstack/2012-08-10/user_data</userinput>
<computeroutput>#!/bin/bash
echo 'Extra user data here'</computeroutput></screen>
            </para>
        </simplesect>
        <simplesect>
            <title>EC2 metadata API</title>
            <para>The metadata service has an API that is compatible
                with version 2009-04-04 of the <link
                    xlink:href="http://docs.amazonwebservices.com/AWSEC2/2009-04-04/UserGuide/AESDG-chapter-instancedata.html"
                    >Amazon EC2 metadata service</link>; virtual
                machine images that are designed for EC2 will work
                properly with OpenStack.</para>
            <para>The EC2 API exposes a separate URL for each
                metadata. A listing of these elements can be retrieved
                by making a GET query
                to:<programlisting>http://169.254.169.254/2009-04-04/meta-data/</programlisting></para>
            <para>For example:</para>
            <para>
                <screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/meta-data/</userinput>
<computeroutput>ami-id
ami-launch-index
ami-manifest-path
block-device-mapping/
hostname
instance-action
instance-id
instance-type
kernel-id
local-hostname
local-ipv4
placement/
public-hostname
public-ipv4
public-keys/
ramdisk-id
reservation-id
security-groups</computeroutput>
<prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/meta-data/block-device-mapping/</userinput>
<computeroutput>ami</computeroutput>
<prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/meta-data/placement/</userinput>
<computeroutput>availability-zone</computeroutput>
<prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/meta-data/public-keys/</userinput>
<computeroutput>0=mykey</computeroutput></screen>
            </para>
            <para>Instances can retrieve the public SSH key
                (identified by keypair name when a user requests a new
                instance) by making a GET request
                to:<programlisting>http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key</programlisting></para>
            <para>For example:</para>
            <para>
                <screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key</userinput>
<computeroutput>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAAAgQDYVEprvtYJXVOBN0XNKVVRNCRX6BlnNbI+USLGais1sUWPwtSg7z9K9vhbYAPUZcq8c/s5S9dg5vTHbsiyPCIDOKyeHba4MUJq8Oh5b2i71/3BISpyxTBH/uZDHdslW2a+SrPDCeuMMoss9NFhBdKtDkdG9zyi0ibmCP6yMdEX8Q== Generated by Nova
</computeroutput></screen>
            </para>
            <para>Instances can retrieve user data by making a GET
                request
                to:<programlisting>http://169.254.169.254/2009-04-04/user-data</programlisting>
                For example:</para>
            <para>
                <screen><prompt>$</prompt> <userinput>curl http://169.254.169.254/2009-04-04/user-data</userinput>
<computeroutput>#!/bin/bash
echo 'Extra user data here'</computeroutput></screen>
            </para>
        </simplesect>
        <simplesect>
            <title>Running the metadata service</title>
            <para>The metadata service is implemented by either the
                    <systemitem class="service">nova-api</systemitem>
                service or the <systemitem class="service"
                    >nova-api-metadata</systemitem> service. (The
                    <systemitem class="service"
                    >nova-api-metadata</systemitem> service is
                generally only used when running in multi-host mode,
                see the section titled <link
                    linkend="existing-ha-networking-options">Existing
                    High Availability Options for Networking</link>
                for details). If you are running the <systemitem
                    class="service">nova-api</systemitem> service, you
                must have <literal>metadata</literal> as one of the
                elements of the list of the
                    <literal>enabled_apis</literal> configuration
                option in <filename>/etc/nova/nova.conf</filename>.
                The default <literal>enabled_apis</literal>
                configuration setting includes the metadata service,
                so you should not need to modify it.</para>
            <para>To allow instances to reach the metadata service,
                the <systemitem class="service"
                    >nova-network</systemitem> service will configure
                iptables to NAT port <literal>80</literal> of the
                    <literal>169.254.169.254</literal> address to the
                IP address specified in
                    <literal>metadata_host</literal> (default
                    <literal>$my_ip</literal>, which is the IP address
                of the <systemitem class="service"
                    >nova-network</systemitem> service) and port
                specified in <literal>metadata_port</literal> (default
                    <literal>8775</literal>) in
                    <filename>/etc/nova/nova.conf</filename>. <warning>
                    <para>The <literal>metadata_host</literal>
                        configuration option must be an IP address,
                        not a hostname.</para>
                </warning>
                <note>
                    <para>The default Compute service settings assume
                        that the <systemitem class="service"
                            >nova-network</systemitem> service and the
                            <systemitem class="service"
                            >nova-api</systemitem> service are running
                        on the same host. If this is not the case, you
                        must make the following change in the
                            <filename>/etc/nova/nova.conf</filename>
                        file on the host running the <systemitem
                            class="service">nova-network</systemitem>
                        service:</para>
                    <para>Set the <literal>metadata_host</literal>
                        configuration option to the IP address of the
                        host where the <systemitem class="service"
                            >nova-api</systemitem> service is
                        running.</para>
                </note></para>
            <xi:include href="../common/tables/nova-metadata.xml"/>
        </simplesect>
    </section>
    <section xml:id="configuring-networking-on-the-compute-node">
        <title>Configuring Networking on the Compute Node</title>
        <para>To configure the Compute node's networking for the VM
            images, the overall steps are:</para>

        <orderedlist>
            <listitem>
                <para>Set the <literal>network_manager</literal>
                    option in nova.conf.</para>
            </listitem>
            <listitem>
                <para>Use the <code>nova network-create label
                        --fixed-range-v4 CIDR [--vlan
                            <replaceable>vlan_id</replaceable>]</code>
                    command to create the subnet that the VMs reside
                    on, specifying a VLAN if running in VLAN Network
                    Mode.</para>
            </listitem>
            <listitem>
                <para>Integrate the bridge with your network.</para>
            </listitem>
        </orderedlist>
        <para>By default, Compute uses the VLAN Network Mode. You
            choose the networking mode for your virtual instances in
            the nova.conf file. Here are the three possible options: </para>
        <itemizedlist>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.FlatManager</literal></para>
                <para>Simple, non-VLAN networking</para>
            </listitem>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.FlatDHCPManager</literal></para>
                <para>Flat networking with DHCP, you must set a bridge
                    using the <literal>flat_network_bridge</literal>
                    option</para>
            </listitem>
            <listitem>
                <para><literal>--network_manager=nova.network.manager.VlanManager</literal></para>
                <para>VLAN networking with DHCP. This is the Default
                    if no network manager is defined in nova.conf.
                </para>
            </listitem>
        </itemizedlist>
        <para>Use the following command to create a subnet (named
                <emphasis>private</emphasis> in this example) that
            your VMs will run on:
            <literallayout class="monospaced"><literal>nova network-create private --fixed-range-v4 192.168.0.0/24</literal></literallayout>
        </para>
        <para>When using the XenAPI compute driver, the OpenStack
            services run in a virtual machine. This means networking
            is significantly different when compared to the networking
            with the libvirt compute driver. Before reading how to
            configure networking using the XenAPI compute driver, you
            may find it useful to read the Citrix article on <link
                xlink:href="http://support.citrix.com/article/CTX117915"
                > Understanding XenServer Networking</link>.
            <!--<phrase>and the section of this
            document that describes
            <link linkend="introduction-to-xen">XenAPI and OpenStack</link>.</phrase>-->
        </para>

        <section xml:id="configuring-flat-networking">
            <title>Configuring Flat Networking</title>
            <para>FlatNetworking uses ethernet adapters configured as
                bridges to allow network traffic to transit between
                all the various nodes. This setup can be done with a
                single adapter on the physical host, or multiple. This
                option does not require a switch that does VLAN
                tagging as VLAN networking does, and is a common
                development installation or proof of concept setup.
                When you choose Flat networking, Nova does not manage
                networking at all. Instead, IP addresses are injected
                into the instance through the file system (or passed
                in through a guest agent). Metadata forwarding must be
                configured manually on the gateway if it is required
                within your network.</para>
            <para>To configure flat networking, ensure that your
                    <filename>nova.conf</filename> file contains the
                following line:</para>
            <para>
                <programlisting>
network_manager=nova.network.manager.FlatManager
                </programlisting>
            </para>
            <note>
                <para>When configuring Flat Networking, failing to
                    enable <literal>flat_injected</literal> can
                    prevent guest VMs from receiving their IP
                    information at boot time.</para>
            </note>

            <section xml:id="libvirt-flat-networking">
                <title>Libvirt Flat Networking</title>
                <para>Compute defaults to a bridge device named
                    ‘br100’ which is stored in the Nova database, so
                    you can change the name of the bridge device by
                    modifying the entry in the database. Consult the
                    diagrams for additional configuration
                    options.</para>
                <para>In any set up with FlatNetworking (either Flat
                    or FlatDHCP), the host with nova-network on it is
                    responsible for forwarding traffic from the
                    private network. Compute determines the "fixed
                    range" for IPs configured dynamically by pulling
                    from the configured networks when you set
                        <literal>fixed_range=''</literal> in
                        <literal>nova.conf</literal>. This dynamic
                    range configuration allows for non-contiguous
                    subnets to be configured in the fixed_ip space and
                    only configures the NAT rules as they are needed.
                    This also restricts the NAT range to the smallest
                    range required preventing the NAT from impacting
                    subnets that might exist on the external
                    network.</para>
                <para>This host needs to have br100 configured and
                    talking to any other nodes that are hosting VMs.
                    With either of the Flat Networking options, the
                    default gateway for the virtual machines is set to

                    the host which is running nova-network. </para>
                <para>Set the compute node's external IP address to be
                    on the bridge and add eth0 to that bridge. To do
                    this, edit your network interfaces configuration
                    to look like the following example: </para>
                <para>
                    <programlisting>

# The loopback network interface
auto lo
iface lo inet loopback

# Networking for OpenStack Compute
auto br100

iface br100 inet dhcp
    bridge_ports        eth0
    bridge_stp           off
    bridge_maxwait   0
    bridge_fd            0
 </programlisting>
                </para>
                <para>Next, restart networking to apply the changes:
                        <code>sudo /etc/init.d/networking
                        restart</code></para>
                <para>For an all-in-one development setup, this
                    diagram represents the network setup.</para>

                <para><figure>
                        <title>Flat network, all-in-one server
                            installation</title>
                        <mediaobject>
                            <imageobject>
                                <imagedata scale="80"
                                   fileref="figures/FlatNetworkSingleInterfaceAllInOne.png"
                                />
                            </imageobject>
                        </mediaobject>
                    </figure></para>
                <para>For multiple compute nodes with a single network
                    adapter, which you can use for smoke testing or a
                    proof of concept, this diagram represents the
                    network setup.</para>
                <figure>
                    <title>Flat network, single interface, multiple
                        servers</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/FlatNetworkSingleInterface.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure>
                <para>For multiple compute nodes with multiple network
                    adapters, this diagram represents the network
                    setup. You may want to use this setup for separate
                    admin and data traffic.</para>
                <figure xml:id="flat-dhcp-diagram">
                    <title>Flat network, multiple interfaces, multiple
                        servers</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/FlatNetworkMultInterface.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure>
            </section>

            <section xml:id="xenapi-flat-networking">
                <title xml:id="xenapi-flat-networking.title">XenAPI
                    Flat Networking</title>
                <para>When using the XenAPI driver, the virtual
                    machines creates OpenStack are attached to the
                    XenServer bridge configured in the
                        <literal>flat_network_bridge</literal>
                    setting. Otherwise, flat networking works in a
                    very similar way with both the libvirt driver and
                    the XenAPI driver. </para>
            </section>
        </section>

        <section xml:id="configuring-flat-dhcp-networking">
            <title>Configuring Flat DHCP Networking</title>
            <para>With Flat DHCP, the host(-s) running nova-network
                act as the gateway to the virtual nodes. If you're
                using single-host networking, you can optionally set
                    <literal>network_host</literal> on the
                    <filename>nova.conf</filename> stored on the
                    <systemitem class="service"
                    >nova-compute</systemitem> node to tell it which
                host the nova-network is running on so it can more
                efficiently communicate with nova-network. In any
                setup with flat networking, the hosts with
                nova-network on it are responsible for forwarding
                traffic from the private network configured with the
                    <literal>fixed_range=</literal> directive in
                    <filename>nova.conf</filename> and the
                    <literal>flat_network_bridge</literal> flag which
                you must also set to the name of the bridge (as there
                is no default). The nova-network service will track
                leases and releases in the database, using dnsmasq's
                    <emphasis role="italic">dhcp-script</emphasis>
                facility (the script <emphasis role="italic"
                    >bin/nova-dhcpbridge</emphasis> is supplied) so it
                knows if a VM instance has stopped properly
                configuring through DHCP (e.g. when a DHCP lease
                expires, the fixed IP is released from the nova
                database). Lastly, it sets up iptables rules to allow
                the VMs to communicate with the outside world and
                contact a special metadata server to retrieve
                information from the cloud.</para>
            <para>Compute hosts in the FlatDHCP model are responsible
                for bringing up a matching bridge and bridging the VM
                tap devices into the same ethernet device that the
                network host is on. The compute hosts should not have
                an IP address on the VM network, because the bridging
                puts the VMs and the network host on the same logical
                network. When a VM boots, the VM sends out DHCP
                packets, and the DHCP server on the network host
                responds with their assigned IP address (remember, the
                address is actually <emphasis role="italic"
                    >assigned</emphasis> by nova and put into DHCP
                server's configuration file, the DHCP server merely
                tells the VM what it is).</para>
            <para>You can read a detailed walk-through of what exactly
                happens in single-host Flat DHCP mode in <link
                    xlink:href="http://www.mirantis.com/blog/openstack-networking-single-host-flatdhcpmanager/"
                    >this blogpost</link>, parts of which are also
                relevant in other networking modes.</para>
            <para>FlatDHCP doesn't create VLANs, it creates a bridge.
                This bridge works just fine on a single host, but when
                there are multiple hosts, traffic needs a way to get
                out of the bridge onto a physical interface.</para>


            <section xml:id="libvirt-flat-dhcp-networking">
                <title>Libvirt Flat DHCP Networking</title>
                <para>When using the libvirt driver, the setup will
                    look like the figure below:</para>
                <figure>
                    <title>Flat DHCP network, multiple interfaces,
                        multiple servers with libvirt driver</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="50"
                                fileref="figures/flatdchp-net.jpg"/>
                        </imageobject>
                    </mediaobject>
                </figure>
                <para>Be careful when setting up
                        <literal>--flat_interface</literal>. If you
                    specify an interface that already has an IP it
                    will break and if this is the interface you are
                    connecting through with SSH, you cannot fix it
                    unless you have ipmi/console access. In FlatDHCP
                    mode, the setting for
                        <literal>--network_size</literal> should be
                    number of IPs in the entire fixed range. If you
                    are doing a /12 in CIDR notation, then this number
                    would be 2^20 or 1,048,576 IP addresses. That
                    said, it will take a very long time for you to
                    create your initial network, as an entry for each
                    IP will be created in the database. </para>
                <para>If you have an unused interface on your hosts
                    (eg eth2) that has connectivity with no IP
                    address, you can simply tell FlatDHCP to bridge
                    into the interface by specifying

                            <literal>flat_interface=<replaceable>&lt;interface></replaceable></literal>
                    in your configuration file. The network host will
                    automatically add the gateway ip to this bridge.
                    If this is the case for you, edit your
                        <filename>nova.conf</filename> file to contain
                    the following lines: </para>
                <para>
                    <programlisting>
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
network_manager=nova.network.manager.FlatDHCPManager
fixed_range=''
flat_network_bridge=br100
flat_interface=eth2
flat_injected=False
public_interface=eth0
                </programlisting>
                    You can also add the unused interface to br100

                    manually and not set flat_interface. </para>
                <para>Integrate your network interfaces to match this
                    configuration.</para>
            </section>
            <section xml:id="xenapi-flat-dhcp-networking">
                <title xml:id="xenapi-flat-dhcp-networking.title"
                    >XenAPI Flat DHCP Networking</title>
                <para>The following figure shows a setup with Flat
                    DHCP networking, network HA, and using multiple
                    interfaces. For simplicity, the management network
                    (on XenServer eth0 and eth2 of the VM running the
                    OpenStack services) has been omitted from the
                    figure below.</para>
                <figure xml:id="xenapi-flat-dhcp-diagram">
                    <title>Flat DHCP network, multiple interfaces,
                        multiple servers, network HA with XenAPI
                        driver</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/XenApiFlatDHCPMultInterfaceHA.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure>
                <para>Here is an extract from a
                        <filename>nova.conf</filename> file in a
                    system running the above setup:</para>
                <para>
                    <programlisting>network_manager=nova.network.manager.FlatDHCPManager

xenapi_vif_driver=nova.virt.xenapi.vif.(XenAPIBridgeDriver or XenAPIOpenVswitchDriver)
flat_interface=eth1
flat_network_bridge=xenbr2
public_interface=eth3
multi_host=True
dhcpbridge_flagfile=/etc/nova/nova.conf
fixed_range=''
force_dhcp_release=True
send_arp_for_ha=True
flat_injected=False
firewall_driver=nova.virt.xenapi.firewall.Dom0IptablesFirewallDriver</programlisting>
                </para>
                <para>You should notice that
                        <literal>flat_interface</literal> and
                        <literal>public_interface</literal> refer to
                    the network interface on the VM running the
                    OpenStack services, not the network interface on
                    the Hypervisor. </para>
                <para>Secondly <literal>flat_network_bridge</literal>
                    refers to the name of XenAPI network that you wish
                    to have your instance traffic on, i.e. the network
                    on which the VMs will be attached. You can either
                    specify the bridge name, such an
                        <literal>xenbr2</literal>, or the name label,
                    such as <literal>vmbr</literal>. Specifying the
                    name-label is very useful in cases where your
                    networks are not uniform across your XenServer
                    hosts. </para>
                <para>When you have a limited number of network cards
                    on your server, it is possible to use networks
                    isolated using VLANs for the public and network
                    traffic. For example, if you have two XenServer
                    networks <literal>xapi1</literal> and
                        <literal>xapi2</literal> attached on VLAN 102
                    and 103 on <literal>eth0</literal>, respectively,
                    you could use these for eth1 and eth3 on your VM,
                    and pass the appropriate one to
                        <literal>flat_network_bridge</literal>. </para>
                <para>When using XenServer, it is best to use the
                    firewall driver written specifically for
                    XenServer. This pushes the firewall rules down to
                    the hypervisor, rather than running them in the VM
                    that is running <literal>nova-network</literal>.
                </para>
            </section>
        </section>

        <section
            xml:id="outbound-traffic-flow-with-any-flat-networking">
            <title>Outbound Traffic Flow with Any Flat
                Networking</title>
            <para>In any set up with FlatNetworking, the host with
                nova-network on it is responsible for forwarding
                traffic from the private network dynamically
                determined by Compute with the
                    <literal>fixed_range=''</literal> directive in
                    <filename>nova.conf</filename>. This host needs to
                have a bridge interface (e.g.,
                    <literal>br100</literal>) configured and talking
                to any other nodes that are hosting VMs. With either
                of the Flat Networking options, the default gateway
                for the virtual machines is set to the host which is
                running nova-network.</para>
            <para>When a virtual machine sends traffic out to the
                public networks, it sends it first to its default
                gateway, which is where nova-network is configured.</para>
            <figure>
                <title>Single adaptor hosts, first route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_1.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <para>Next, the host on which nova-network is configured
                acts as a router and forwards the traffic out to the
                Internet.</para>
            <figure>
                <title>Single adaptor hosts, second route</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="80"
                            fileref="figures/SingleInterfaceOutbound_2.png"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <warning>
                <para>If you're using a single interface, then that
                    interface (often eth0) needs to be set into
                    promiscuous mode for the forwarding to happen
                    correctly. This does not appear to be needed if
                    you're running with physical hosts that have and
                    use two interfaces.</para>
            </warning>
        </section>
        <section xml:id="configuring-vlan-networking">
            <?dbhtml stop-chunking?>
            <title>Configuring VLAN Networking</title>
            <para>Compute can be configured so that the virtual
                machine instances of different projects (tenants) are
                in different subnets, with each subnet having a
                different VLAN tag. This can be useful in networking
                environments where you have a large IP space which is
                cut up into smaller subnets. The smaller subnets are
                then trunked together at the switch level (dividing
                layer 3 by layer 2) so that all machines in the larger
                IP space can communicate. The purpose of this is
                generally to control the size of broadcast domains. It
                can also be useful to provide an additional layer of
                isolation in a multi-tenant environment.</para>
            <note>

                <para>The terms <emphasis role="italic"
                        >network</emphasis> and <emphasis
                        role="italic">subnet</emphasis> are often used
                    interchangeably in discussions of VLAN mode. In
                    all cases, we are referring to a range of IP
                    addresses specified by a <emphasis role="italic"
                        >subnet </emphasis>(e.g.,
                        <literal>172.16.20.0/24</literal>) that are on
                    the same VLAN (layer 2 <emphasis role="italic"
                        >network</emphasis>). </para>

            </note>
            <para>Running in VLAN mode is more complex than the other
                network modes. In particular:<itemizedlist>
                    <listitem>
                        <para>IP forwarding must be enabled</para>
                    </listitem>
                    <listitem>
                        <para>The hosts running nova-network and
                                <systemitem class="service"
                                >nova-compute</systemitem> must have
                            the <literal>8021q</literal> kernel module
                            loaded</para>
                    </listitem>
                    <listitem>
                        <para>Your networking switches must support
                            VLAN tagging</para>
                    </listitem>
                    <listitem>
                        <para>Your networking switches must be
                            configured to enable the specific VLAN
                            tags you specify in your Compute
                            setup</para>
                    </listitem>
                    <listitem>
                        <para>You will need information about your
                            networking setup from your network
                            administrator to configure Compute
                            properly (e.g., netmask, broadcast,
                            gateway, ethernet device, VLAN IDs)</para>
                    </listitem>
                </itemizedlist>
            </para>

            <para>The
                    <literal>network_manager=nova.network.manager.VlanManager</literal>
                option specifies VLAN mode, which happens to be the
                default networking mode. </para>
            <para>The bridges that are created by the network manager
                will be attached to the interface specified by
                    <literal>vlan_interface</literal>, the example
                above uses the <literal>eth0</literal> interface,
                which is the default. </para>

            <para>The <literal>fixed_range</literal> option deprecated
                in Grizzly and should be set to
                    <literal>fixed_range=''</literal> so that Nova
                determines a CIDR block which describes the IP address
                space for all of the instances: this space will be
                divided up into subnets. This range is typically a
                    <link
                    xlink:href="https://en.wikipedia.org/wiki/Private_network"
                    >private network</link>. The example above uses
                the private range
                <literal>172.16.0.0/12</literal>.</para>
            <para>The <literal>network_size</literal> option refers to
                the default number of IP addresses in each network,
                although this can be overridden at network creation
                time . The example above uses a network size of
                    <literal>256</literal>, which corresponds to a
                    <literal>/24</literal> network.</para>
            <para>Networks are created with the <command>nova
                    network-create</command> command. Here is an
                example of how to create a network consistent with the
                above example configuration options, as
                root:<screen><prompt>#</prompt> <userinput>nova network-create example-net --fixed-range-v4=172.16.169.0/24 --vlan=169 --bridge=br169 --project-id=a421ae28356b4cc3a25e1429a0b02e98</userinput> </screen></para>
            <para>This creates a network called
                    <literal>example-net</literal> associated with
                tenant
                    <literal>a421ae28356b4cc3a25e1429a0b02e98</literal>.
                The subnet is <literal>172.16.169.0/24</literal> with
                a VLAN tag of <literal>169</literal> (the VLAN tag
                does not need to match the third byte of the address,
                though it is a useful convention to remember the
                association). This will create a bridge interface
                device called <literal>br169</literal> on the host
                running the nova-network service. This device will
                appear in the output of an <command>ifconfig</command>
                command.</para>
            <para>Each network is associated with one tenant. As in
                the example above, you may (optionally) specify this
                association at network creation time by using the
                    <literal>--project_id</literal> flag which
                corresponds to the tenant ID. Use the
                    <command>keystone tenant-list</command> command to
                list the tenants and corresponding IDs that you have
                already created.</para>
            <para>The <command>nova network-create</command> command
                supports many configuration options, which are
                displayed when called with the <command>nova help
                    network-create</command>:</para>
            <programlisting>usage: nova network-create [--fixed-range-v4 &lt;x.x.x.x/yy>]
                           [--fixed-range-v6 CIDR_V6] [--vlan &lt;vlan id>]
                           [--vpn &lt;vpn start>] [--gateway GATEWAY]
                           [--gateway-v6 GATEWAY_V6] [--bridge &lt;bridge>]
                           [--bridge-interface &lt;bridge interface>]
                           [--multi-host &lt;'T'|'F'>] [--dns1 &lt;DNS Address>]
                           [--dns2 &lt;DNS Address>] [--uuid &lt;network uuid>]
                           [--fixed-cidr &lt;x.x.x.x/yy>]
                           [--project-id &lt;project id>] [--priority &lt;number>]
                           &lt;network_label>

Create a network.

Positional arguments:
  &lt;network_label>       Label for network

Optional arguments:
  --fixed-range-v4 &lt;x.x.x.x/yy>
                        IPv4 subnet (ex: 10.0.0.0/8)
  --fixed-range-v6 CIDR_V6
                        IPv6 subnet (ex: fe80::/64
  --vlan &lt;vlan id>      vlan id
  --vpn &lt;vpn start>     vpn start
  --gateway GATEWAY     gateway
  --gateway-v6 GATEWAY_V6
                        ipv6 gateway
  --bridge &lt;bridge>     VIFs on this network are connected to this bridge
  --bridge-interface &lt;bridge interface>
                        the bridge is connected to this interface
  --multi-host &lt;'T'|'F'>
                        Multi host
  --dns1 &lt;DNS Address>  First DNS
  --dns2 &lt;DNS Address>  Second DNS
  --uuid &lt;network uuid>
                        Network UUID
  --fixed-cidr &lt;x.x.x.x/yy>
                        IPv4 subnet for fixed IPS (ex: 10.20.0.0/16)
  --project-id &lt;project id>
                        Project id
  --priority &lt;number>   Network interface priority                   </programlisting>
            <para>In particular, flags to the <command>nova
                    network-create</command> command can be used to
                override settings from <filename>nova.conf</filename>:<variablelist>
                    <varlistentry>
                        <term><literal>--bridge_interface</literal></term>
                        <listitem>
                            <para>Overrides the
                                   <literal>vlan_interface</literal>
                                configuration option</para>
                        </listitem>
                    </varlistentry>
                </variablelist></para>
            <para>To view a list of the networks that have been
                created, as
                root:<screen><prompt>#</prompt> <userinput>nova network-list</userinput></screen></para>
            <para>The <command>nova</command> command-line tool does
                not yet support network modifications. To modify an
                existing network, you must use the
                    <command>nova-manage</command> command. Use the
                    <command>nova-manage network modify</command>
                command, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network modify --help</userinput>
<computeroutput>Usage: nova-manage network modify &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to modify
  --project=&lt;project name>
                        Project name to associate
  --host=&lt;host>         Host to associate
  --disassociate-project
                        Disassociate Network from Project
  --disassociate-host   Disassociate Host from Project</computeroutput></screen></para>
            <para>The <command>nova</command> command-line tool does
                not yet support network deletions.. To delete an
                existing network, you must use the
                    <command>nova-manage</command> command. To delete
                a network, use <command>nova-manage network
                    delete</command>, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage network delete --help</userinput><computeroutput>
Usage: nova-manage network delete &lt;args> [options]

Options:
  -h, --help            show this help message and exit
  --fixed_range=&lt;x.x.x.x/yy>
                        Network to delete
  --uuid=&lt;uuid>         UUID of network to delete</computeroutput></screen></para>
            <para>Note that a network must first be disassociated from
                a project using the <command>nova
                    network-disassociate</command> command before it
                can be deleted.</para>
            <para>Creating a network will automatically cause the
                Compute database to populate with a list of available
                fixed IP addresses. You can view the list of fixed IP
                addresses and their associations with active virtual
                machines by doing, as
                root:<screen><prompt>#</prompt> <userinput>nova-manage fix list</userinput></screen></para>
            <para>If users need to access the instances in their
                project across a VPN, a special VPN instance (code
                named cloudpipe) needs to be created as described in
                the section titled <link
                    linkend="cloudpipe-per-project-vpns">Cloudpipe —
                    Per Project VPNs</link>. </para>

            <section xml:id="libvirt-vlan-networking">
                <title>Libvirt VLAN networking</title>
                <para>To configure your nodes to support VLAN tagging,
                    install the <literal>vlan</literal> package and
                    load the <literal>8021q</literal> kernel module,
                    as
                    root:<screen><prompt>#</prompt> <userinput>apt-get install vlan</userinput>
<prompt>#</prompt> <userinput>modprobe 8021q </userinput></screen></para>
                <para>To have this kernel module loaded on boot, add
                    the following line to
                        <filename>/etc/modules</filename>:<programlisting>8021q</programlisting></para>
                <para>Here is an example of settings from
                        <filename>/etc/nova/nova.conf</filename> for a
                    host configured to run
                        <command>nova-network</command> in VLAN
                    mode</para>
                <programlisting>network_manager=nova.network.manager.VlanManager
vlan_interface=eth0
fixed_range=172.16.0.0/12
network_size=256                </programlisting>
                <para>In certain cases, the network manager may not
                    properly tear down bridges and VLANs when it is
                    stopped. If you attempt to restart the network
                    manager and it does not start, check the logs for
                    errors indicating that a bridge device already
                    exists. If this is the case, you will likely need
                    to tear down the bridge and VLAN devices manually.
                    It is also advisable to kill any remaining dnsmasq
                    processes. These commands would stop the service,
                    manually tear down the bridge and VLAN from the
                    previous example, kill any remaining dnsmasq
                    processes, and start the service up again, as
                    root:</para>
                <screen><prompt>#</prompt> <userinput>stop nova-network</userinput>
<prompt>#</prompt> <userinput>vconfig rem vlan169</userinput>
<prompt>#</prompt> <userinput>ip link set br169 down</userinput>
<prompt>#</prompt> <userinput>brctl delbr br169</userinput>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>start nova-network</userinput></screen>
            </section>
            <section xml:id="xenapi-vlan-networking">
                <title>XenAPI VLAN networking</title>
                <para>VLAN networking works quite differently with the
                    XenAPI driver, compared to the libvirt driver. The
                    following figure shows how your setup might look: </para>
                <figure xml:id="xenapi-vlan-diagram">
                    <title>VLAN network, multiple interfaces, multiple
                        servers, network HA with XenAPI driver</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="80"
                                fileref="figures/XenApiVLANMultInterfaceHA.png"
                            />
                        </imageobject>
                    </mediaobject>
                </figure>
                <para>Here is an extract from a
                        <filename>nova.conf</filename> file in a
                    system running the above setup:</para>
                <para>
                    <programlisting>network_manager=nova.network.manager.VlanManager
xenapi_vif_driver=nova.virt.xenapi.vif.(XenAPIBridgeDriver or XenAPIOpenVswitchDriver)
vlan_interface=eth1
public_interface=eth3
multi_host=True
force_dhcp_release=True
send_arp_for_ha=True
flat_injected=False
firewall_driver=nova.virt.xenapi.firewall.Dom0IptablesFirewallDriver</programlisting>
                </para>
                <para>You should notice that
                        <literal>vlan_interface</literal> refers to
                    the network interface on the Hypervisor and the
                    network interface on the VM running the OpenStack
                    services. As with before
                        <literal>public_interface</literal> refers to
                    the network interfce on the VM running the
                    OpenStack services.</para>
                <para>With VLAN networking and the XenAPI driver, the
                    following things happen when you start a VM:
                    <itemizedlist>
                        <listitem>
                            <para>First the XenServer network is
                                attached to the appropriate physical
                                interface (PIF) and VLAN unless the
                                network already exists.</para>
                        </listitem>
                        <listitem>
                            <para>When the VM is created, its VIF is
                                attached to the above network.</para>
                        </listitem>
                        <listitem>
                            <para>The 'Openstack domU', i.e. where
                                nova-network is running, acts as a
                                gateway and DHCP for this instance.
                                The DomU does this for multiple VLAN
                                networks, so it has to be attached on
                                a VLAN trunk. For this reason it must
                                have an interface on the parent bridge
                                of the VLAN bridge where VM instances
                                are plugged.</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>To help understand VLAN networking with the
                    XenAPI further, here are some important things to
                    note:
                    <itemizedlist>
                        <listitem>
                            <para>A physical interface (PIF)
                                identified either by (A) the
                                vlan_interface flag or (B) the
                                bridge_interface column in the
                                networks db table will be used for
                                creating a XenServer VLAN network. The
                                VLAN tag is found in the vlan column,
                                still in the networks table, and by
                                default the first tag is 100.</para>
                        </listitem>
                        <listitem>
                            <para>VIF for VM instances within this
                                network will be plugged in this VLAN
                                network. You won't see the bridge
                                until a VIF is plugged in it.</para>
                        </listitem>
                        <listitem>
                            <para>The 'Openstack domU', i.e. the VM
                                running the nova network node, instead
                                will not be plugged into this network;
                                since it acts as a gateway for
                                multiple VLAN networks, it has to be
                                attached on a VLAN trunk. For this
                                reason it must have an interface on
                                the parent bridge of the VLAN bridge
                                where VM instances are plugged. For
                                example, if
                                   <literal>vlan_interface</literal>
                                is eth0 it must be plugged in xenbr1,
                                eth1 --> xenbr1, etc.</para>
                        </listitem>
                        <listitem>
                            <para>Within the Openstack domU, 'ip
                                link' is then used to configure VLAN
                                interfaces on the 'trunk' port. Each
                                of this vlan interfaces is associated
                                with a dnsmasq instance, which will
                                distribute IP addresses to instances.
                                The lease file for dnsmasq is
                                constantly updated by nova-network,
                                thus ensuring VMs get the IP address
                                specified by the layer3 network driver
                                (nova IPAM or Melange).</para>
                        </listitem>
                    </itemizedlist>
                </para>
                <para>With this configuration, VM instances should be
                    able to get the IP address assigned to them from
                    the appropriate dnsmasq instance, and should be
                    able to communicate without any problem with other
                    VMs on the same network and with the their
                    gateway.</para>
                <para>The above point (3) probably needs some more
                    explanations. With Open vSwitch, we don't really
                    have distinct bridges for different VLANs; even if
                    they appear as distinct bridges to linux and
                    XenServer, they are actually the same OVS
                    instance, which runs a distinct 'fake-bridge' for
                    each VLAN. The 'real' bridge is the 'parent' of
                    the fake one. You can easily navigate fake and
                    real bridges with ovs-vsctl.</para>
                <para>As you can see I am referring to Openvswitch
                    only. This is for a specific reason: the
                    fake-parent mechanism automatically imply that
                    ports which are not on a fake bridge are trunk
                    ports. This does not happen with linux bridge. A
                    packet forwarded on a VLAN interfaces does not get
                    back in the xenbrX bridge for ethX. For this
                    reason, with XenAPI, you must use Open vSwitch
                    when running VLAN networking with network HA (i.e.
                    multi host) enabled. On XenServer 6.0 and later,
                    Open vSwitch is the default network stack. When
                    using VLAN networking with XenAPI and linux
                    bridge, the default networking stack on XenServer
                    prior to version 6.0, you must run the network
                    node on a VM on a XenServer that does not host any
                        <systemitem class="service"
                        >nova-compute</systemitem> controlled
                    instances.</para>
            </section>
            <section xml:id="vlan-known-issues">
                <title>Known issue with failed DHCP leases in VLAN
                    configuration</title>
                <para>Text in this section was adapted from <link
                        xlink:href="https://lists.launchpad.net/openstack/msg11696.html"
                        >an email from Vish Ishaya on the OpenStack
                        mailing list</link>.</para>
                <para>There is an issue with the way Compute uses
                        <link
                        xlink:href="http://www.thekelleys.org.uk/dnsmasq/doc.html"
                        >dnsmasq</link> in VLAN mode. Compute starts
                    up a single copy of dnsmasq for each VLAN on the
                    network host (or on every host in multi_host
                    mode). <link
                        xlink:href="http://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2011q3/005233.html"
                        >The problem</link> is in the way that dnsmasq
                    binds to an IP address and port. Both copies can
                    respond to broadcast packets, but unicast packets
                    can only be answered by one of the copies.</para>
                <para>As a consequence, guests from only one project
                    will get responses to their unicast DHCP renew
                    requests. Unicast projects from guests in other
                    projects get ignored. What happens next is
                    different depending on the guest OS. Linux
                    generally will send a broadcast packet out after
                    the unicast fails, and so the only effect is a
                    small (tens of ms) hiccup while the interface is
                    reconfigured. It can be much worse than that,
                    however. There have been observed cases where
                    Windows just gives up and ends up with a
                    non-configured interface.</para>
                <para>This bug was first noticed by some users of
                    OpenStack who rolled their own fix. In short, on
                    Linux, if you set the
                        <literal>SO_BINDTODEVICE</literal> socket
                    option, it will allow different daemons to share
                    the port and respond to unicast packets, as long
                    as they listen on different interfaces. Simon
                    Kelley, the maintainer of dnsmasq, <link
                        xlink:href="http://thekelleys.org.uk/gitweb/?p=dnsmasq.git;a=commitdiff;h=9380ba70d67db6b69f817d8e318de5ba1e990b12"

                        >has integrated a fix</link> for the issue in
                    dnsmaq version 2.61.</para>
                <para>If upgrading dnsmasq is out of the question, a
                    possible workaround is to minimize lease renewals
                    with something like the following combination of
                    config options.

                    <programlisting># release leases immediately on terminate
force_dhcp_release
# one week lease time
dhcp_lease_time=604800
# two week disassociate timeout
fixed_ip_disassociate_timeout=1209600</programlisting></para>
            </section>
        </section>
        <section xml:id="cloudpipe-per-project-vpns">
            <title>Cloudpipe — Per Project Vpns</title>

            <para>Cloudpipe is a method for connecting end users to
                their project instances in VLAN networking mode.</para>
            <para>The support code for cloudpipe implements admin
                commands (through an extension) to automatically
                create a VM for a project that allows users to VPN
                into the private network of their project. Access to
                this VPN is provided through a public port on the
                network host for the project. This allows users to
                have free access to the virtual machines in their
                project without exposing those machines to the public
                internet.</para>
            <para>The cloudpipe image is basically just a Linux
                instance with openvpn installed. It needs a simple
                script to grab user data from the metadata server, b64
                decode it into a zip file, and run the autorun.sh
                script from inside the zip. The autorun script will
                configure and run openvpn to run using the data from
                nova.</para>
            <para>It is also useful to have a cron script that will
                periodically re download the metadata and copy the new
                Certificate Revocation List (CRL). This list is
                contained within the payload file and will keeps
                revoked users from connecting and will disconnect any
                users that are connected with revoked certificates
                when their connection is renegotiated (every hour).
                (More infos about revocation can be found in the
                following section: "Certificates and
                Revocation").</para>
            <para>In this how-to, we are going to create our
                cloud-pipe image from a running Ubuntu instance which
                will serve as a template. When all the components will
                be installed and configured, we will create an image
                from that instance that will be uploaded to the Glance
                repositories.</para>

            <section xml:id="cloudpipe-create-image">
                <?dbhtml stop-chunking?>
                <title>Creating a Cloudpipe Image Template</title>
                <orderedlist>
                    <listitem>
                        <para>Installing the required packages</para>

                        <para>We start by installing the required
                            packages on our instance:
                            <screen><prompt>#</prompt> <userinput>apt-get update &amp;&amp; apt-get upgrade &amp;&amp; apt-get install openvpn bridge-utils unzip -y</userinput></screen></para>
                    </listitem>
                    <listitem>
                        <para>Creating the server configuration
                            template</para>
                        <para>Create a configuration for Openvpn, and
                            save it under
                                <filename>/etc/openvpn/server.conf</filename>
                           :</para>

                        <para>
                            <programlisting>port 1194
proto udp
dev tap0
up "/etc/openvpn/up.sh br0"
down "/etc/openvpn/down.sh br0"
script-security 3 system

persist-key
persist-tun

ca ca.crt
cert server.crt
key server.key  # This file should be kept secret

dh dh1024.pem
ifconfig-pool-persist ipp.txt

server-bridge VPN_IP DHCP_SUBNET DHCP_LOWER DHCP_UPPER

client-to-client
keepalive 10 120
comp-lzo

max-clients 1

user nobody
group nogroup

persist-key
persist-tun

status openvpn-status.log

verb 3
mute 20</programlisting>
                        </para>
                    </listitem>
                    <listitem>
                        <para>Create the network scripts</para>

                        <para>The next step is to create both scripts
                            that will be used when the network
                            components will start up and shut down.
                            The scripts will be respectively saved
                            under
                                <filename>/etc/openvpn/up.sh</filename>
                            and
                                <filename>/etc/openvpn/down.sh</filename>
                           :</para>

                        <para>
                            <filename> /etc/openvpn/up.sh </filename>
                            <programlisting>#!/bin/sh
# Openvpn startup script.

BR=$1
DEV=$2
MTU=$3
/sbin/ifconfig $DEV mtu $MTU promisc up
/sbin/brctl addif $BR $DEV                      </programlisting>
                            <filename>/etc/openvpn/down.sh</filename>
                            <programlisting>#!/bin/sh
# Openvpn shutdown script
BR=$1
DEV=$2

/usr/sbin/brctl delif $BR $DEV
/sbin/ifconfig $DEV down                  </programlisting>
                        </para>
                        <para>Make these two scripts executables by
                            running the following command:
                            <screen><prompt>#</prompt> <userinput>chmod +x /etc/openvpn/{up.sh,down.sh}</userinput></screen></para>
                    </listitem>
                    <listitem>

                        <para>Edit the network interface configuration
                            file</para>
                        <para>Update the
                                <filename>/etc/network/interfaces</filename>
                            accordingly (We tear down the main
                            interface and enable the bridged
                            interface):</para>

                        <programlisting># This file describes the network interfaces available on your system
# and how to activate them. For more information, see interfaces(5).

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
auto eth0
iface eth0 inet manual
  up ifconfig $IFACE 0.0.0.0 up
  down ifconfig $IFACE down

auto br0
iface br0 inet dhcp
bridge_ports eth0                     </programlisting>
                    </listitem>
                    <listitem>
                        <para>Edit the rc.local file</para>

                        <para>The next step consists in updating the
                                <filename>/etc/rc.local</filename>
                            file. We will ask our image to retrieve
                            the payload, decrypt it, and use both key
                            and CRL for our Openvpn service:
                                <filename>/etc/rc.local</filename>

                            <programlisting>#!/bin/sh -e
#
# rc.local
#
# This script is executed at the end of each multiuser runlevel.
# Make sure that the script will "exit 0" on success or any other
# value on error.
#
# In order to enable or disable this script just change the execution
# bits.
#
# By default this script does nothing.
####### These lines go at the end of /etc/rc.local #######
. /lib/lsb/init-functions

echo Downloading payload from userdata
wget http://169.254.169.254/latest/user-data -O /tmp/payload.b64
echo Decrypting base64 payload
openssl enc -d -base64 -in /tmp/payload.b64 -out /tmp/payload.zip

mkdir -p /tmp/payload
echo Unzipping payload file
unzip -o /tmp/payload.zip -d /tmp/payload/

# if the autorun.sh script exists, run it
if [ -e /tmp/payload/autorun.sh ]; then
    echo Running autorun.sh
    cd /tmp/payload
    chmod 700 /etc/openvpn/server.key
    sh /tmp/payload/autorun.sh
    if [ ! -e /etc/openvpn/dh1024.pem ]; then
        openssl dhparam -out /etc/openvpn/dh1024.pem 1024
    fi
else
  echo rc.local: No autorun script to run
fi


exit 0 </programlisting>
                            The called script
                                (<filename>autorun.sh</filename>) is a
                            script which mainly parses the network
                            settings of the running instances in order
                            to set up the initial routes. Your
                            instance is now ready to be used as a
                            cloudpipe image. In the next step, we will
                            update that instance to Glance.</para>
                    </listitem>
                </orderedlist>
            </section>
            <section xml:id="cloudpipe-upload-image">
                <title>Upload your instance to Glance</title>

                <para>We will make use of the <systemitem
                        class="service">nova</systemitem> snapshot
                    feature in order to create an image from our
                    running instance. We start by retrieving the
                    instance ID:</para>

                <para>
                    <screen><prompt>$</prompt> <userinput>nova list</userinput></screen>
                </para>
                <programlisting>
+--------------------------------------+------------+--------+---------------------+
|                  ID                  |  Name      | Status |       Networks      |
+--------------------------------------+------------+--------+---------------------+
| 739079ab-0f8e-404a-ae6e-a91f4fe99c94 | cloud-pipe | ACTIVE | vlan1=192.168.22.43 |
+--------------------------------------+------------+--------+---------------------+
                </programlisting>

                <para>We create an image with, using the instance ID
                   :

                    <screen><prompt>$</prompt> <userinput>nova image-create 739079a-b-0f8e-404a-ae6e-a91f4fe99c94</userinput></screen>
                    Make sure the instance has been upload to the
                    Glance repository:
                    <screen><prompt>$</prompt> <userinput>nova image-list</userinput></screen><programlisting>
+--------------------------------------+---------------+--------+--------------------------------------+
|                  ID                  |      Name     | Status |                Server                |
+--------------------------------------+---------------+--------+--------------------------------------+
| 0bfc8fd3-1590-463b-b178-bce30be5ef7b | cloud-pipance | ACTIVE | fb93eda8-4eb8-42f7-b53c-91c6d83cface |
+--------------------------------------+---------------+--------+--------------------------------------+
                    </programlisting>
                    Make that image public (snapshot-based images are
                    private by default):
                    <screen><prompt>$</prompt> <userinput>glance image-update 0bfc8fd3-1590-463b-b178-bce30be5ef7b is_public=true</userinput></screen>
                    You can ensure the image is now public, running <screen><prompt>$</prompt> <userinput>glance show 0bfc8fd3-1590-463b-b178-bce30be5ef7b | grep Public</userinput></screen>
                    <computeroutput>Public: Yes </computeroutput>
                </para>
            </section>
            <section xml:id="cloudpipe-update-nova-conf">
                <title>Update /etc/nova.conf</title>
                <para>Some settings need to be added into
                        <filename>/etc/nova.conf</filename>file in
                    order to make nova able to use our image:
                        <filename>/etc/nova.conf</filename>
                    <programlisting>
## cloud-pipe vpn client ##
vpn_image_id=0bfc8fd3-1590-463b-b178-bce30be5ef7b
use_project_ca=true
cnt_vpn_clients=5
                </programlisting>
                    You can now restart all the services:
                    <screen><prompt>#</prompt> <userinput>cd /etc/int.d &amp;&amp; for i in $( ls nova-*); do service $i restart; done</userinput></screen></para>
            </section>
            <section xml:id="cloudpipe-start-instance">
                <title>Power-up your instance</title>

                <para>Use the <systemitem class="service">nova
                        cloudpipe</systemitem> feature the following
                    way:

                    <screen><prompt>$</prompt> <userinput>nova cloud-pipe create $tenant_id</userinput></screen>
                    Retrieve all the tenants:
                    <screen><prompt>$</prompt> <userinput>keystone tenant-list</userinput></screen><programlisting>
+----------------------------------+---------+---------+
|                id                |   name  | enabled |
+----------------------------------+---------+---------+
| 071ffb95837e4d509cb7153f21c57c4d | stone   | True    |
| 520b6689e344456cbb074c83f849914a | service | True    |
| d1f5d27ccf594cdbb034c8a4123494e9 | admin   | True    |
| dfb0ef4ab6d94d5b9e9e0006d0ac6706 | demo    | True    |
+----------------------------------+---------+---------+
                    </programlisting>
                    Let's create our cloudpipe project using the
                    tenant"s ID:
                    <screen><prompt>$</prompt> <userinput>nova cloudpipe-create d1f5d27ccf594cdbb034c8a4123494e9</userinput></screen>
                    We can check the service availability:
                    <screen><prompt>$</prompt> <userinput>nova cloudpipe-list</userinput></screen><programlisting>
+----------------------------------+------------+-------------+---------------+
|            Project Id            | Public IP  | Public Port |  Internal IP  |
+----------------------------------+------------+-------------+---------------+
| d1f5d27ccf594cdbb034c8a4123494e9 | 172.17.1.3 | 1000        | 192.168.22.34 |
+----------------------------------+------------+-------------+---------------+
                    </programlisting>
                    The output basically shows our instance is
                    started. Nova will create the necessary rules for
                    our cloudpipe instance (icmp and OpenVPN port):
                    <programlisting>
ALLOW 1194:1194 from 0.0.0.0/0
ALLOW -1:-1 from 0.0.0.0/0
                    </programlisting>
                </para>
            </section>
            <section xml:id="cloudpipe-vpn-access">
                <title>VPN Access</title>
                <para>In VLAN networking mode, the second IP in each
                    private network is reserved for the cloudpipe
                    instance. This gives a consistent IP to the
                    instance so that nova-network can create
                    forwarding rules for access from the outside
                    world. The network for each project is given a
                    specific high-numbered port on the public IP of
                    the network host. This port is automatically
                    forwarded to 1194 on the VPN instance.</para>
                <para>If specific high numbered ports do not work for
                    your users, you can always allocate and associate
                    a public IP to the instance, and then change the
                        <literal>vpn_public_ip</literal> and
                        <literal>vpn_public_port</literal> in the
                    database. Rather than using the database directly,
                    you can also use <command>nova-manage vpn change
                            <replaceable>[new_ip]</replaceable>
                        <replaceable>[new_port]</replaceable></command>
                </para>
                <para/>
            </section>
            <section xml:id="cloudpipe-certificates-and-revocation">
                <title>Certificates and Revocation</title>

                <para>For certificate management, it is also useful to
                    have a cron script that will periodically download
                    the metadata and copy the new Certificate
                    Revocation List (CRL). This will keep revoked
                    users from connecting and disconnects any users
                    that are connected with revoked certificates when
                    their connection is re-negotiated (every hour).
                    You set the use_project_ca option in nova.conf for
                    cloudpipes to work securely so that each project
                    has its own Certificate Authority (CA).</para>
                <para>If the <literal>use_project_ca config</literal>
                    option is set (required to for cloudpipes to work
                    securely), then each project has its own CA. This
                    CA is used to sign the certificate for the vpn,
                    and is also passed to the user for bundling
                    images. When a certificate is revoked using
                    nova-manage, a new Certificate Revocation List
                    (crl) is generated. As long as cloudpipe has an
                    updated crl, it will block revoked users from
                    connecting to the vpn.</para>
                <para>The user data for cloudpipe isn't currently
                    updated when certs are revoked, so it is necessary
                    to restart the cloudpipe instance if a user's
                    credentials are revoked.</para>
                <para/>
            </section>
            <section
                xml:id="cloudpipe-restarting-and-logging-into-vpn">
                <title>Restarting and Logging into the Cloudpipe
                    VPN</title>
                <para>You can reboot a cloudpipe vpn through the api
                    if something goes wrong (using <command>nova
                        reboot</command> for example), but if you
                    generate a new crl, you will have to terminate it
                    and start it again using the cloudpipe extension.
                    The cloudpipe instance always gets the first ip in
                    the subnet and if
                        <literal>force_dhcp_release</literal>
                    configuration key is not set to
                        <literal>True</literal> it takes some time for
                    the ip to be recovered. If you try to start the
                    new vpn instance too soon, the instance will fail
                    to start because of a "NoMoreAddresses" error. It
                    is therefore recommended that the
                        <literal>force_dhcp_release</literal>
                    configuration option is set to
                        <literal>True</literal>, which is the default
                    value.</para>
                <para>The keypair that was used to launch the
                    cloudpipe instance should be in the
                            <filename>keys/<replaceable>&lt;project_id&gt;</replaceable></filename>
                    folder. You can use this key to log into the
                    cloudpipe instance for debugging purposes. If you
                    are running multiple copies of <systemitem class="service">nova-api</systemitem> this key
                    will be on whichever server used the original
                    request. To make debugging easier, you may want to
                    put a common administrative key into the cloudpipe
                    image that you create.</para>
                <para/>
            </section>
            <section xml:id="cloudpipe-remote-acceess">

                <title>Remote access to your cloudpipe instance from
                    an OpenVPN client</title>
                <para>Now your cloudpipe instance is running, you can
                    use your favorite OpenVPN client in order to
                    access your instances within their private network
                    cloudpipe is connected to. In these sections we
                    will present both ways of using cloudpipe, the
                    first using a configuration file for clients
                    without interfaces, and for clients using an
                    interface. </para>
                <para><emphasis role="bold">Connect to your cloudpipe
                        instance without an interface
                    (CLI)</emphasis></para>
                <orderedlist>
                    <listitem>
                        <para>Generate your certificates</para>
                        <para>Start by generating a private key and a
                            certificate for your project:

                            <screen><prompt>$</prompt> <userinput>nova x509-create-cert</userinput></screen>
                        </para>
                    </listitem>
                    <listitem>

                        <para>Create the openvpn configuration
                            file</para>
                        <para>The following template, which can be
                            found under
                                <filename>nova/cloudpipe/client.ovpn.template</filename>
                            contains the necessary instructions for
                            establishing a connection: </para>

                        <programlisting># NOVA user connection
# Edit the following lines to point to your cert files:
cert <emphasis role="bold">/path/to/the/cert/file</emphasis>
key <emphasis role="bold">/path/to/the/key/file</emphasis>

ca cacert.pem

client
dev tap
proto udp

remote <emphasis role="bold">$cloudpipe-public-ip $cloudpipe-port</emphasis>
resolv-retry infinite
nobind

# Downgrade privileges after initialization (non-Windows only)
user nobody
group nogroup
comp-lzo

# Set log file verbosity.
verb 2

keepalive 10 120
ping-timer-rem
persist-tun
persist-key             </programlisting>

                        <para>Update the file accordingly. In order
                            to get the public IP and port of your
                            cloudpipe instance, you can run the
                            following command:
                            <screen><prompt>$</prompt> <userinput>nova cloudpipe-list</userinput></screen><programlisting>
+----------------------------------+------------+-------------+---------------+
|            Project Id            | Public IP  | Public Port |  Internal IP  |
+----------------------------------+------------+-------------+---------------+
| d1f5d27ccf594cdbb034c8a4123494e9 | 172.17.1.3 | 1000        | 192.168.22.34 |
+----------------------------------+------------+-------------+---------------+
                            </programlisting>
                        </para>
                    </listitem>
                    <listitem>
                        <para>Start your OpenVPN client</para>
                        <para>Depending on the client you are using,
                            make sure to save the configuration file
                            under the directory it should be, so the
                            certificate file and the private key.
                            Usually, the file is saved under
                                <filename>/etc/openvpn/clientconf/client.conf</filename>
                        </para>

                    </listitem>
                </orderedlist>
                <para><emphasis role="bold">Connect to your cloudpipe
                        instance using an interface</emphasis><orderedlist>
                        <listitem>

                            <para>Download an OpenVPN client</para>
                            <para>In order to connect to the
                                project's network, you will need an
                                OpenVPN client for your computer. Here
                                are several clients <itemizedlist>
                                   <listitem>
                                   <para>For Ubuntu:</para>
                                   <para><link
                                   xlink:href="apt://openvpn"
                                   >OpenVPN</link>
                                   </para>
                                   <para>
                                   <link
                                   xlink:href="apt://network-manager-openvpn"
                                   >network-manager-openvpn</link>
                                   </para>
                                   <para>
                                   <link xlink:href="apt://kvpnc"
                                   >kvpnc</link> (For Kubuntu)</para>
                                   <para>
                                   <link xlink:href="apt://gopenvpn"
                                   >gopenvpn</link></para>
                                   </listitem>
                                   <listitem>
                                   <para>For Mac OsX:</para>
                                   <para>
                                   <link
                                   xlink:href="http://openvpn.net/"
                                   >OpenVPN (Official Client)</link>
                                   </para>
                                   <para>
                                   <link
                                   xlink:href="http://www.thesparklabs.com/viscosity/"
                                   >Viscosity</link>
                                   </para>
                                   <para>
                                   <link
                                   xlink:href="http://code.google.com/p/tunnelblick/"
                                   >Tunnelblick</link>
                                   </para>
                                   </listitem>
                                   <listitem>
                                   <para>For Windows:</para>
                                   <para>
                                   <link
                                   xlink:href="http://openvpn.net/"
                                   >OpenVPN (Official Client)</link>
                                   </para>
                                   </listitem>
                                </itemizedlist></para>
                        </listitem>
                        <listitem>
                            <para>Configure your client</para>
                            <para>In this example we will use
                                Viscosity, but the same settings apply
                                to any client. Start by filling the
                                public ip and the public port of the
                                cloudpipe instance.</para>
                            <para>This information can be found by
                                running a
                                <screen><prompt>$</prompt> <userinput>nova cloudpipe-list</userinput></screen></para>
                            <programlisting>
+----------------------------------+------------+-------------+---------------+
|            Project Id            | Public IP  | Public Port |  Internal IP  |
+----------------------------------+------------+-------------+---------------+
| d1f5d27ccf594cdbb034c8a4123494e9 | 172.17.1.3 | 1000        | 192.168.22.34 |
+----------------------------------+------------+-------------+---------------+
                            </programlisting>
                            <figure
                                xml:id="cloudpipe-viscosity-configuration">
                                <title>Configuring Viscosity</title>
                                <itemizedlist>
                                   <listitem>
                                   <para>Connection Name:
                                   "Openstack-cloudpipe"</para>
                                   <para>Remote server: "172.17.1.3" </para>
                                   <para>Port: "1000"</para>
                                   <para>Protocol: "udp"</para>
                                   <para>Device Type: "tap"</para>
                                   </listitem>
                                </itemizedlist>
                            </figure>

                            <mediaobject>
                                <imageobject>
                                   <imagedata scale="80"
                                   fileref="figures/cloudpipe/cloudpipe-viscosity-step1.jpg"
                                   />
                                </imageobject>
                            </mediaobject>
                            <itemizedlist>
                                <listitem>
                                   <para>Certificate: The generated
                                   certificate</para>
                                   <para>Key: The private key</para>
                                </listitem>
                            </itemizedlist>
                            <mediaobject>
                                <imageobject>
                                   <imagedata scale="80"
                                   fileref="figures/cloudpipe/cloudpipe-viscosity-step2.jpg"
                                   />
                                </imageobject>
                            </mediaobject>
                            <itemizedlist>
                                <listitem>
                                   <para>Persistence options:
                                   "Persistent TUN" and "Persistent
                                   key"</para>
                                   <para>Other:" No bind"</para>
                                </listitem>
                            </itemizedlist>
                            <mediaobject>
                                <imageobject>
                                   <imagedata scale="80"
                                   fileref="figures/cloudpipe/cloudpipe-viscosity-step3.jpg"
                                   />
                                </imageobject>
                            </mediaobject>
                            <itemizedlist>
                                <listitem>
                                   <para>Advanced</para>
                                   <para>Extra settings: "nobind" and
                                   "resolv-retry infinite" </para>
                                </listitem>
                            </itemizedlist>
                            <mediaobject>
                                <imageobject>
                                   <imagedata scale="80"
                                   fileref="figures/cloudpipe/cloudpipe-viscosity-step4.jpg"
                                   />
                                </imageobject>
                            </mediaobject>

                        </listitem>
                    </orderedlist>You can now save the configuration
                    and establish the connection! </para>
                <para/>
            </section>
            <section xml:id="cloudpipe-misc">
                <title>Cloudpipe Troubleshooting and
                    Automation</title>
                <itemizedlist>

                    <listitem>
                        <para><emphasis role="bold">Troubleshoot your
                                cloudpipe instance</emphasis></para>
                        <para>A periodic task disassociates the fixed
                            ip address for the cloudpipe instance.
                            Into
                                <filename>/var/log/nova/nova-network.log</filename>,
                            the following line should appear:
                            <programlisting>Running periodic task VlanManager._disassociate_stale_fixed_ips from (pid=21578) periodic_tasks /usr/lib/python2.7/dist-packages/nova/manager.py:152   </programlisting>
                            Once the job has been run,
                                <prompt>$</prompt>
                            <userinput> nova
                                cloudpipe-list</userinput>should not
                            return anything ; but if the cloudpipe
                            instance is respawned too quickly; the
                            following error could be encountered:
                            <programlisting>ERROR nova.rpc.amqp Returning exception Fixed IP address 192.168.22.34 is already in use.</programlisting>
                            In order to resolve that issue, log into
                            the mysql server and update the ip address
                            status:
                            <screen><prompt>(mysql)</prompt> <userinput>use nova;</userinput></screen><screen><prompt>(mysql)</prompt> <userinput>SELECT * FROM fixed_ips WHERE address='192.168.22.34';</userinput></screen><programlisting>
+---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
| created_at          | updated_at          | deleted_at | deleted | id  | address       | network_id | instance_id | allocated | leased | reserved | virtual_interface_id | host |
+---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
| 2012-05-21 12:06:18 | 2012-06-18 09:26:25 | NULL       |       0 | 484 | 192.168.22.34 |         13 |         630 |         0 |      0 |        1 |                 NULL | NULL |
+---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
                    </programlisting><screen><prompt>(mysql)</prompt> <userinput>UPDATE fixed_ips SET allocated=0, leased=0, instance_id=NULL WHERE address='192.168.22.34';</userinput></screen><screen><prompt>(mysql)</prompt> <userinput>SELECT * FROM fixed_ips WHERE address='192.168.22.34';</userinput></screen><programlisting> +---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
| created_at          | updated_at          | deleted_at | deleted | id  | address       | network_id | instance_id | allocated | leased | reserved | virtual_interface_id |      |                 +---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
| 2012-05-21 12:06:18 | 2012-06-18 09:26:25 | NULL       |       0 | 484 | 192.168.22.34 |         13 |        NULL |         0 |      0 |        1 |                 NULL | NULL |
+---------------------+---------------------+------------+---------+-----+---------------+------------+-------------+-----------+--------+----------+----------------------+------+
                    </programlisting>
                        </para>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold">Cloudpipe-related
                                configuration option
                                reference</emphasis></para>
                        <xi:include
                            href="../common/tables/nova-vpn.xml"/>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold">Cloudpipe-related
                                files</emphasis></para>
                        <para>Nova stores cloudpipe keys into

                                <filename>/var/lib/nova/keys</filename>. </para>
                        <para>Certificates are stored into
                                <filename>/var/lib/nova/CA</filename>. </para>

                        <para>Credentials are stored into
                                <filename>/var/lib/nova/CA/projects/</filename>
                        </para>
                        <para/>
                    </listitem>
                    <listitem>
                        <para><emphasis role="bold">Automate the
                                cloudpipe image
                                installation</emphasis></para>
                        <para>You can automate the image creation by
                            download that script and running it from
                            inside the instance: <link
                                xlink:href="https://github.com/leseb/cloudpipe-image-auto-creation/blob/master/cloudpipeconf.sh"
                                >Get the script from Github</link>
                        </para>
                    </listitem>
                </itemizedlist>
            </section>
        </section>
    </section>
    <section xml:id="enabling-ping-and-ssh-on-vms">
        <title>Enabling Ping and SSH on VMs</title>
        <para>Be sure you enable access to your VMs by using the
                <command>euca-authorize</command> or <command>nova
                secgroup-add-rule</command> command. Below, you will
            find the commands to allow <command>ping</command> and
                <command>ssh</command> to your VMs: </para>
        <note>
            <para>These commands need to be run as root only if the
                credentials used to interact with <systemitem class="service">nova-api</systemitem> have been
                put under <filename>/root/.bashrc</filename>. If the
                EC2 credentials have been put into another user's
                    <filename>.bashrc</filename> file, then, it is
                necessary to run these commands as the user.</para>
        </note>
        <para>Using the nova command-line tool:</para>
        <screen>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0</userinput>
<prompt>$</prompt> <userinput>nova secgroup-add-rule default tcp 22 22 0.0.0.0/0</userinput>
            </screen>
        <para>Using euca2ools:</para>
        <screen>
<prompt>$</prompt> <userinput>euca-authorize -P icmp -t -1:-1 -s 0.0.0.0/0 default</userinput>
<prompt>$</prompt> <userinput>euca-authorize -P tcp -p 22 -s 0.0.0.0/0 default</userinput>
            </screen>

        <para>If you still cannot ping or SSH your instances after
            issuing the <command>nova secgroup-add-rule</command>
            commands, look at the number of <literal>dnsmasq</literal>
            processes that are running. If you have a running
            instance, check to see that TWO <literal>dnsmasq</literal>
            processes are running. If not, perform the following as
            root:</para>
        <screen>
<prompt>#</prompt> <userinput>killall dnsmasq</userinput>
<prompt>#</prompt> <userinput>service nova-network restart</userinput>
           </screen>
    </section>
    <section xml:id="associating-public-ip">
        <title>Configuring Public (Floating) IP Addresses</title>
        <?dbhtml stop-chunking?>
        <section xml:id="Private_and_Public_IP_Addresses">
            <title>Private and Public IP Addresses</title>
            <para>Every virtual instance is automatically assigned a
                private IP address. You may optionally assign public
                IP addresses to instances. OpenStack uses the term
                "floating IP" to refer to an IP address (typically
                public) that can be dynamically added to a running
                virtual instance. OpenStack Compute uses Network
                Address Translation (NAT) to assign floating IPs to
                virtual instances.</para>
            <para>If you plan to use this feature, you must add the
                following to your nova.conf file to specify which
                interface the nova-network service will bind public IP
                addresses to:</para>
            <programlisting>
public_interface=vlan100
        </programlisting>
            <para>Restart the nova-network service if you change
                nova.conf while the service is running.</para>
            <note>
                <title>Traffic between VMs using floating IPs</title>
                <para>Note that due to the way floating IPs are
                    implemented using a source NAT (SNAT rule in
                    iptables), inconsistent behaviour of security
                    groups can be seen if VMs use their floating IP to
                    communicate with other virtual machines -
                    particularly on the same physical host. Traffic
                    from VM to VM across the fixed network does not
                    have this issue, and this is the recommended path.
                    To ensure traffic doesn't get SNATed to the
                    floating range, explicitly set
                        <literal>dmz_cidr=x.x.x.x/y</literal>.
                    x.x.x.x/y is the range of floating ips for each
                    pool of floating ips you define. This
                    configuration is also necessary to make
                    source_groups work if the vms in the source group
                    have floating ips.</para>
            </note>
        </section>
        <section xml:id="Enabling_ip_forwarding">

            <title>Enabling IP forwarding</title>
            <para>By default, the IP forwarding is disabled on most of
                Linux distributions. The "floating IP" feature
                requires the IP forwarding enabled in order to work. <note>
                    <para>The IP forwarding only needs to be enabled
                        on the nodes running the service nova-network.
                        If the <literal>multi_host</literal> mode is
                        used, make sure to enable it on all the
                        compute node, otherwise, enable it on the node
                        running the nova-network service.</para>
                </note>
            </para>
            <para>you can check if the forwarding is enabled by
                running the following command: <screen><prompt>$</prompt> <userinput>cat /proc/sys/net/ipv4/ip_forward</userinput></screen>
                <screen><computeroutput>0</computeroutput></screen></para>
            <para>Or using sysctl<screen><prompt>$</prompt> <userinput> sysctl net.ipv4.ip_forward</userinput></screen>
                <screen><computeroutput>net.ipv4.ip_forward = 0</computeroutput></screen>
            </para>
            <para>In this example, the IP forwarding is disabled. You
                can enable it on the fly by running the following
                command:
                <screen><prompt>$</prompt> <userinput>sysctl -w net.ipv4.ip_forward=1</userinput></screen>or
                <screen><prompt>$</prompt> <userinput>echo 1 > /proc/sys/net/ipv4/ip_forward</userinput></screen>
                In order to make the changes permanent, edit the
                    <filename>/etc/sysctl.conf</filename> and update
                the IP forwarding setting:
                <programlisting>net.ipv4.ip_forward = 1</programlisting>
                Save the file and run the following command in order
                to apply the changes:
                <screen><prompt>$</prompt> <userinput>sysctl -p</userinput></screen>
                It is also possible to update the setting by
                restarting the network service. Here's an example for
                Ubuntu:
                <screen os="ubuntu"><userinput><prompt>$</prompt>/etc/init.d/procps.sh restart</userinput></screen></para>
            <para>Here's an example for RHEL/Fedora/CentOS:</para>
            <para>
                <screen os="rhel;fedora;centos"><prompt>$</prompt> <userinput>service network restart</userinput></screen>
            </para>
        </section>
        <section
            xml:id="Creating_a_List_of_Available_Floating_IP_Addresses">
            <title>Creating a List of Available Floating IP
                Addresses</title>
            <para>Nova maintains a list of floating IP addresses that
                are available for assigning to instances. Use the
                    <command>nova-manage floating create</command>
                command to add entries to this list, as root.</para>
            <para>For example:</para>
            <screen>
<prompt>#</prompt> <userinput>nova-manage floating create --pool=nova --ip_range=68.99.26.170/31</userinput>
        </screen>
            <para>The following nova-manage commands apply to floating
                IPs.</para>
            <itemizedlist>
                <listitem>
                    <para><command>nova-manage floating
                        list</command>: List the floating IP addresses
                        in the pool.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating create
                            --pool=[pool name]
                            --ip_range=[CIDR]</command>: Create
                        specific floating IPs for either a single
                        address or a subnet.</para>
                </listitem>
                <listitem>
                    <para><command>nova-manage floating delete
                            [cidr]</command>: Remove floating IP
                        addresses using the same parameters as the
                        create command.</para>
                </listitem>
            </itemizedlist>
            <para>Refer to <link
                    xlink:href="http://docs.openstack.org/cli/quick-start/content/nova_client.html#floating_ip_addresses"
                    >Manage Floating IP Addresses</link> in the
                    <emphasis role="italic">OpenStack Clients
                    Guide</emphasis> for information on how to
                associate floating IPs to instances.</para>

        </section>
        <section xml:id="Automatically_adding_floating_IPs">
            <title>Automatically adding floating IPs</title>
            <para>The nova-network service can be configured to
                automatically allocate and assign a floating IP
                address to virtual instances when they are launched.
                Add the following line to nova.conf and restart the
                nova-network service</para>

            <programlisting>
auto_assign_floating_ip=True
            </programlisting>
            <para>Note that if this option is enabled and all of the
                floating IP addresses have already been allocated, the
                    <command>nova boot</command> command will fail
                with an error.</para>
        </section>
    </section>
    <section xml:id="removing-network-from-project">
        <title>Removing a Network from a Project</title>
        <para>You will find that you cannot remove a network that has
            already been associated to a project by simply deleting
            it.</para>
        <para>To determine the project ID you must have admin rights.
            You can disassociate the project from the network with a
            scrub command and the project ID as the final parameter: </para>
        <screen>
<prompt>$</prompt> <userinput>nova-manage project scrub --project=<replaceable>&lt;id></replaceable></userinput>
        </screen>
    </section>
    <section xml:id="using-multi-nics">
        <title>Using multiple interfaces for your instances
            (multinic)</title>
        <?dbhtml stop-chunking?>
        <para>The multi-nic feature allows you to plug more than one
            interface to your instances, making it possible to make
            several use cases available: <itemizedlist>
                <listitem>
                    <para>SSL Configurations (VIPs)</para>
                </listitem>
                <listitem>
                    <para>Services failover/ HA</para>
                </listitem>
                <listitem>
                    <para>Bandwidth Allocation</para>
                </listitem>
                <listitem>
                    <para>Administrative/ Public access to your
                        instances</para>
                </listitem>
            </itemizedlist> Each VIF is representative of a separate
            network with its own IP block. Every network mode
            introduces it's own set of changes regarding the mulitnic
            usage: <figure>
                <title>multinic flat manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic flatdhcp manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-Flat-DHCP-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
            <figure>
                <title>multinic VLAN manager</title>
                <mediaobject>
                    <imageobject>
                        <imagedata scale="40"
                            fileref="figures/SCH_5007_V00_NUAC-multi_nic_OpenStack-VLAN-manager.jpg"
                        />
                    </imageobject>
                </mediaobject>
            </figure>
        </para>
        <section xml:id="using-multiple-nics-usage">
            <title>Using the multinic feature</title>

            <para>In order to use the multinic feature, first create
                two networks, and attach them to your project:
                <screen><prompt>$</prompt> <userinput>nova network-create first-net --fixed-range-v4=20.20.0.0/24 --project-id=$your-project</userinput>
<prompt>$</prompt> <userinput>nova network-create second-net --fixed-range-v4=20.20.10.0/24 --project-id=$your-project</userinput>              </screen>
                Now every time you spawn a new instance, it gets two
                IP addresses from the respective DHCP servers: <screen><prompt>$</prompt> <userinput>nova list</userinput>
<computeroutput>+-----+------------+--------+----------------------------------------+
 |  ID |    Name    | Status |                Networks                |
 +-----+------------+--------+----------------------------------------+
 | 124 | Server 124 | ACTIVE | network2=20.20.0.3; private=20.20.10.14|
 +-----+------------+--------+----------------------------------------+</computeroutput></screen>
                <note>
                    <para>Make sure to power up the second interface
                        on the instance, otherwise that last won't be
                        reachable through its second IP. Here is an
                        example of how to setup the interfaces within
                        the instance (this is the configuration that
                        needs to be applied inside the image): </para>
                    <para><filename>/etc/network/interfaces</filename>
                        <programlisting># The loopback network interface
auto lo
iface lo inet loopback

auto eth0
iface eth0 inet dhcp

auto eth1
iface eth1 inet dhcp                </programlisting></para>
                </note></para>
            <note>
                <para>If the Virtual Network Service Neutron is
                    installed, it is possible to specify the networks
                    to attach to the respective interfaces by using
                    the <literal>--nic</literal> flag when invoking
                    the <literal>nova</literal> command:
                    <screen><prompt>$</prompt> <userinput>nova boot --image ed8b2a37-5535-4a5f-a615-443513036d71 --flavor 1 --nic net-id= &lt;id of first network&gt;  --nic net-id= &lt;id of first network&gt;  test-vm1</userinput></screen>
                </para>
            </note>
        </section>
    </section>
    <section xml:id="existing-ha-networking-options">
        <title>Existing High Availability Options for
            Networking</title>
        <para>Based off a blog post by <link
                xlink:href="http://unchainyourbrain.com/openstack/13-networking-in-nova"
                >Vish Ishaya</link></para>

        <para>As illustrated in the Flat DHCP diagram in Section <link
                xlink:href="#configuring-flat-dhcp-networking"
                >Configuring Flat DHCP Networking</link> titled <link
                linkend="flat-dhcp-diagram">Flat DHCP network,
                multiple interfaces, multiple servers</link>, traffic
            from the VM to the public internet has to go through the
            host running nova network. DHCP is handled by nova-network
            as well, listening on the gateway address of the
            fixed_range network. The compute hosts can optionally have
            their own public IPs, or they can use the network host as
            their gateway. This mode is pretty simple and it works in
            the majority of situations, but it has one major drawback:
            the network host is a single point of failure! If the
            network host goes down for any reason, it is impossible to
            communicate with the VMs. Here are some options for
            avoiding the single point of failure.</para>
        <simplesect>
            <title>HA Option 1: Multi-host</title>
            <para>To eliminate the network host as a single point of
                failure, Compute can be configured to allow each
                compute host to do all of the networking jobs for its
                own VMs. Each compute host does NAT, DHCP, and acts as
                a gateway for all of its own VMs. While there is still
                a single point of failure in this scenario, it is the
                same point of failure that applies to all virtualized
                systems.</para>

            <para>This setup requires adding an IP on the VM network
                to each host in the system, and it implies a little
                more overhead on the compute hosts. It is also
                possible to combine this with option 4 (HW Gateway) to
                remove the need for your compute hosts to gateway. In
                that hybrid version they would no longer gateway for
                the VMs and their responsibilities would only be DHCP
                and NAT.</para>
            <para>The resulting layout for the new HA networking
                option looks the following diagram:</para>
            <para><figure>
                    <title>High Availability Networking Option</title>
                    <mediaobject>
                        <imageobject>
                            <imagedata scale="50"
                                fileref="figures/ha-net.jpg"/>
                        </imageobject>
                    </mediaobject>
                </figure></para>
            <para>In contrast with the earlier diagram, all the hosts
                in the system are running the <systemitem
                    class="service">nova-compute</systemitem>,
                nova-network and nova-api services. Each host does
                DHCP and does NAT for public traffic for the VMs
                running on that particular host. In this model every
                compute host requires a connection to the public
                internet and each host is also assigned an address
                from the VM network where it listens for DHCP traffic.
                The <systemitem class="service">nova-api</systemitem> service
is needed so that it can act as a
                metadata server for the instances.</para>
            <para>To run in HA mode, each compute host must run the
                following services:<itemizedlist>
                    <listitem>
                        <para><command>nova-compute</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-network</command></para>
                    </listitem>
                    <listitem>
                        <para><command>nova-api-metadata</command> or
                                <systemitem class="service">nova-api</systemitem></para>
                    </listitem>
                </itemizedlist></para>
            <para>If the compute host is not an API endpoint, use the
                    <command>nova-api-metadata</command> service. The
                    <filename>nova.conf</filename> file should
                contain:</para>
            <programlisting>multi_host=True
send_arp_for_ha=true         </programlisting>
            <para>The <literal>send_arp_for_ha</literal> option
                facilitates sending of gratuitous arp messages to
                ensure the arp caches on compute hosts are up to
                date.</para>
            <para>If a compute host is also an API endpoint, use the
                    <systemitem class="service">nova-api</systemitem> service. Your
                    <literal>enabled_apis</literal> option will need
                to contain <literal>metadata</literal>, as well as
                additional options depending on the API services. For
                example, if it supports compute requests, volume
                requests, and EC2 compatibility, the
                    <filename>nova.conf</filename> file should
                contain:
                <programlisting>multi_host=True
send_arp_for_ha=true
enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>

            <para>The <literal>multi_host</literal> option must be in
                place when you create the network and nova-network
                must be run on every compute host. These created multi
                hosts networks will send all network related commands
                to the host that the specific VM is on. You need to
                edit the configuration option
                    <literal>enabled_apis</literal> such that it
                includes <literal>metadata</literal> in the list of
                enabled APIs. Other options become available when you
                configure <literal>multi_host</literal> nova
                networking. </para>
            <!--<para>See <link linkend="list-of-compute-config-options">Configuration: nova.conf</link>.</para>-->
            <note>
                <para>You must specify the
                        <literal>multi_host</literal> option on the
                    command line when creating fixed networks. For
                    example:
                    <screen><prompt>#</prompt> <userinput> nova network-create test --fixed-range-v4=192.168.0.0/24 --multi-host=T</userinput>            </screen></para>
            </note>
        </simplesect>

        <simplesect>
            <title>HA Option 2: Fail over</title>
            <para>The folks at NTT labs came up with a ha-linux
                configuration that allows for a 4 second fail over to
                a hot backup of the network host. Details on their
                approach can be found in the following post to the
                openstack mailing list: <link
                    xlink:href="https://lists.launchpad.net/openstack/msg02099.html"
                    >https://lists.launchpad.net/openstack/msg02099.html</link></para>
            <para>This solution is definitely an option, although it
                requires a second host that essentially does nothing
                unless there is a failure. Also four seconds can be
                too long for some real-time applications.</para>
            <para>To enable this HA option, your
                    <filename>nova.conf</filename> file must contain
                the following
                option:<programlisting>send_arp_for_ha=True</programlisting></para>
            <para>See <link
                    xlink:href="https://bugs.launchpad.net/nova/+bug/782364"
                    >https://bugs.launchpad.net/nova/+bug/782364</link>
                for details on why this option is required when
                configuring for fail over.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 3: Multi-nic</title>
            <para>Recently, nova gained support for multi-nic. This
                allows us to bridge a given VM into multiple networks.
                This gives us some more options for high availability.
                It is possible to set up two networks on separate
                vlans (or even separate ethernet devices on the host)
                and give the VMs a NIC and an IP on each network. Each
                of these networks could have its own network host
                acting as the gateway.</para>
            <para>In this case, the VM has two possible routes out. If
                one of them fails, it has the option of using the
                other one. The disadvantage of this approach is it
                offloads management of failure scenarios to the guest.
                The guest needs to be aware of multiple networks and
                have a strategy for switching between them. It also
                doesn't help with floating IPs. One would have to set
                up a floating IP associated with each of the IPs on
                private the private networks to achieve some type of
                redundancy.</para>
        </simplesect>
        <simplesect>
            <title>HA Option 4: Hardware gateway</title>
            <para>The <systemitem class="service">dnsmasq</systemitem>
                service can be configured to use an external gateway
                instead of acting as the gateway for the VMs. This
                offloads HA to standard switching hardware and it has
                some strong benefits. Unfortunately, the <systemitem
                    class="service">nova-network</systemitem> service
                is still responsible for floating IP natting and DHCP,
                so some fail over strategy needs to be employed for
                those options. To configure for hardware gateway:<orderedlist>
                    <listitem>
                        <para>Create a <systemitem class="service"
                                >dnsmasq</systemitem> configuration
                            file (e.g.,
                                <filename>/etc/dnsmasq-nova.conf</filename>)
                            that contains the IP address of the
                            external gateway. If running in FlatDHCP
                            mode, assuming the IP address of the
                            hardware gateway was 172.16.100.1, the
                            file would contain the
                            line:<programlisting>dhcp-option=option:router,<replaceable>172.16.100.1</replaceable></programlisting></para>
                        <para>If running in VLAN mode, a separate
                            router must be specified for each network.
                            The networks are identified by the first
                            argument when calling <command>nova
                                network-create</command> to create the
                            networks as documented in the <link
                                linkend="configuring-vlan-networking"
                                >Configuring VLAN Networking
                                subsection</link>. Assuming you have
                            three VLANs, that are labeled
                                <literal>red</literal>,
                                <literal>green</literal>, and
                                <literal>blue</literal>, with
                            corresponding hardware routers at
                                <literal>172.16.100.1</literal>,
                                <literal>172.16.101.1</literal> and
                                <literal>172.16.102.1</literal>, the
                                <systemitem class="service"
                                >dnsmasq</systemitem>configuration
                            file (e.g.,
                                <filename>/etc/dnsmasq-nova.conf</filename>)
                            would contain the
                            following:<programlisting>dhcp-option=tag:'red',option:router,172.16.100.1
dhcp-option=tag:'green',option:router,172.16.101.1
dhcp-option=tag:'blue',option:router,172.16.102.1</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Edit
                                <filename>/etc/nova/nova.conf</filename>
                            to specify the location of the <systemitem
                                class="service">dnsmasq</systemitem>
                            configuration
                            file:<programlisting>dnsmasq_config_file=/etc/dnsmasq-nova.conf</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Configure the hardware gateway to
                            forward metadata requests to a host that's
                            running the <systemitem class="service"
                                >nova-api</systemitem> service with
                            the metadata API enabled.</para>
                        <para>The virtual machine instances access the
                            metadata service at
                                <literal>169.254.169.254</literal>
                            port <literal>80</literal>. The hardware
                            gateway should forward these requests to a
                            host running the <systemitem
                                class="service">nova-api</systemitem>
                            service on the port specified as the
                                <literal>metadata_host</literal>
                            config option in
                                <filename>/etc/nova/nova.conf</filename>,
                            which defaults to
                            <literal>8775</literal>.</para>
                        <para>Make sure that the list in the
                                <literal>enabled_apis</literal>
                            configuration option
                                <filename>/etc/nova/nova.conf</filename>
                            contains <literal>metadata</literal> in
                            addition to the other APIs. An example
                            that contains the EC2 API, the OpenStack
                            compute API, the OpenStack volume API, and
                            the metadata service would look like:
                            <programlisting>enabled_apis=ec2,osapi_compute,osapi_volume,metadata</programlisting></para>
                    </listitem>
                    <listitem>
                        <para>Ensure you have set up routes properly
                            so that the subnet that you use for
                            virtual machines is routable.</para>
                    </listitem>
                </orderedlist></para>
        </simplesect>

    </section>
    <section xml:id="network-troubleshooting">
        <title>Troubleshooting Networking</title>
        <simplesect>
            <title>Can't reach floating IPs</title>
            <para>If you aren't able to reach your instances through
                the floating IP address, make sure the default
                security group allows ICMP (ping) and SSH (port 22),
                so that you can reach the instances:</para>
            <screen><prompt>$</prompt> <userinput>nova secgroup-list-rules default</userinput>
<computeroutput>+-------------+-----------+---------+-----------+--------------+
| IP Protocol | From Port | To Port |  IP Range | Source Group |
+-------------+-----------+---------+-----------+--------------+
| icmp        | -1        | -1      | 0.0.0.0/0 |              |
| tcp         | 22        | 22      | 0.0.0.0/0 |              |
+-------------+-----------+---------+-----------+--------------+</computeroutput>           </screen>
            <para>Ensure the NAT rules have been added to iptables on
                the node that nova-network is running on, as
                root:</para>
            <screen><prompt>#</prompt> <userinput>iptables -L -nv</userinput>
<computeroutput>
 -A nova-network-OUTPUT -d 68.99.26.170/32 -j DNAT --to-destination 10.0.0.3
</computeroutput>
<prompt>#</prompt> <userinput>iptables -L -nv -t nat</userinput>
<computeroutput>
-A nova-network-PREROUTING -d 68.99.26.170/32 -j DNAT --to-destination10.0.0.3
-A nova-network-floating-snat -s 10.0.0.3/32 -j SNAT --to-source 68.99.26.170</computeroutput>           </screen>
            <para>Check that the public address, in this example
                "68.99.26.170", has been added to your public
                interface: You should see the address in the listing
                when you enter "ip addr" at the command prompt.</para>
            <screen>
<prompt>$</prompt> <userinput>ip addr</userinput>
<computeroutput>
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
link/ether xx:xx:xx:17:4b:c2 brd ff:ff:ff:ff:ff:ff
inet 13.22.194.80/24 brd 13.22.194.255 scope global eth0
inet 68.99.26.170/32 scope global eth0
inet6 fe80::82b:2bf:fe1:4b2/64 scope link
valid_lft forever preferred_lft forever
</computeroutput>
            </screen>

            <para>Note that you cannot SSH to an instance with a
                public IP from within the same server as the routing
                configuration won't allow it. </para>
            <para>You can use <command>tcpdump</command> to identify
                if packets are being routed to the inbound interface
                on the compute host. If the packets are reaching the
                compute hosts but the connection is failing, the issue
                may be that the packet is being dropped by reverse
                path filtering. Try disabling reverse path filtering
                on the inbound interface. For example, if the inbound
                interface is <literal>eth2</literal>, as
                root:<screen><prompt>#</prompt> <userinput>sysctl -w net.ipv4.conf.<replaceable>eth2</replaceable>.rp_filter=0</userinput></screen></para>
            <para>If this solves your issue, add the following line to
                    <filename>/etc/sysctl.conf</filename> so that the
                reverse path filter will be disabled the next time the
                compute host
                reboots:<programlisting>net.ipv4.conf.rp_filter=0</programlisting></para>
        </simplesect>
        <simplesect>
            <title>Disabling firewall</title>
            <para>To help debug networking issues with reaching VMs,
                you can disable the firewall by setting the following
                option in
                <filename>/etc/nova/nova.conf</filename>:<programlisting>firewall_driver=nova.virt.firewall.NoopFirewallDriver</programlisting></para>
            <para>We strongly recommend you remove the above line to
                re-enable the firewall once your networking issues
                have been resolved.</para>
        </simplesect>
        <simplesect>
            <title>Packet loss from instances to nova-network server
                (VLANManager mode)</title>
            <para>If you can SSH to your instances but you find that
                the network interactions to your instance is slow, or
                if you find that running certain operations are slower
                than they should be (e.g., <command>sudo</command>),
                then there may be packet loss occurring on the
                connection to the instance.</para>
            <para>Packet loss can be caused by Linux networking
                configuration settings related to bridges. Certain
                settings can cause packets to be dropped between the
                VLAN interface (e.g., <literal>vlan100</literal>) and
                the associated bridge interface (e.g.,
                    <literal>br100</literal>) on the host running the
                nova-network service.</para>
            <para>One way to check if this is the issue in your setup
                is to open up three terminals and run the following
                commands:</para>
            <para>In the first terminal, on the host running
                nova-network, use <command>tcpdump</command> to
                monitor DNS-related traffic (UDP, port 53) on the VLAN
                interface. As
                root:<screen><prompt>#</prompt> <userinput>tcpdump -K -p -i vlan100 -v -vv udp port 53</userinput></screen></para>
            <para>In the second terminal, also on the host running
                nova-network, use <command>tcpdump</command> to
                monitor DNS-related traffic on the bridge interface.
                As
                root:<screen><prompt>#</prompt> <userinput>tcpdump -K -p -i br100 -v -vv udp port 53</userinput></screen></para>
            <para>In the third terminal, SSH inside of the instance
                and generate DNS requests by using the
                    <command>nslookup</command>
                command:<screen><prompt>$</prompt> <userinput>nslookup www.google.com</userinput></screen></para>
            <para>The symptoms may be intermittent, so try running
                    <command>nslookup</command> multiple times. If the
                network configuration is correct, the command should
                return immediately each time. If it is not functioning
                properly, the command will hang for several
                seconds.</para>
            <para>If the <command>nslookup</command> command sometimes
                hangs, and there are packets that appear in the first
                terminal but not the second, then the problem may be
                due to filtering done on the bridges. Try to disable
                filtering, as
                root:<screen><prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-arptables=0</userinput>
<prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-iptables=0</userinput>
<prompt>#</prompt> <userinput>sysctl -w net.bridge.bridge-nf-call-ip6tables=0</userinput></screen></para>
            <para>If this solves your issue, add the following line to
                    <filename>/etc/sysctl.conf</filename> so that
                these changes will take effect the next time the host
                reboots:<programlisting>net.bridge.bridge-nf-call-arptables=0
net.bridge.bridge-nf-call-iptables=0
net.bridge.bridge-nf-call-ip6tables=0</programlisting></para>
        </simplesect>
        <simplesect>
            <title>KVM: Network connectivity works initially, then
                fails</title>
            <para>Some administrators have observed an issue with the
                KVM hypervisor where instances running Ubuntu 12.04
                will sometimes lose network connectivity after
                functioning properly for a period of time. Some users
                have reported success with loading the vhost_net
                kernel module as a workaround for this issue (see
                    <link
                    xlink:href="https://bugs.launchpad.net/ubuntu/+source/libvirt/+bug/997978/"
                    >bug #997978</link>) . This kernel module may also
                    <link
                    xlink:href="http://www.linux-kvm.org/page/VhostNet"
                    >improve network performance on KVM</link>. To
                load the kernel module, as
                root:<screen><prompt>#</prompt> <userinput>modprobe vhost_net</userinput></screen></para>
            <para>Note that loading the module has no effect on
                running instances.</para>
        </simplesect>
    </section>
</chapter>
