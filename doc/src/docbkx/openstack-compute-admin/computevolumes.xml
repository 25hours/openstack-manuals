<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
    xml:id="ch_volumes">
    <title>Volumes</title>
    <section xml:id="managing-volumes">
        <title>Managing Volumes</title>
        <para>The Cinder project provides the service that allows you
            to give extra block level storage to your OpenStack
            Compute instances. You may recognize this as a similar
            offering from Amazon EC2 known as Elastic Block Storage
            (EBS). However, OpenStack Block Storage is not the same
            implementation that EC2 uses today. This is an iSCSI
            solution that employs the use of Logical Volume Manager
            (LVM) for Linux. Note that a volume may only be attached
            to one instance at a time. This is not a ‘shared storage’
            solution like a SAN of NFS on which multiple servers can
            attach to.</para>
        <para>Before going any further; let's discuss the block
            storage implementation in OpenStack: </para>
        <para>The cinder service uses iSCSI-exposed LVM volumes to the
            compute nodes which run instances. Thus, there are two
            components involved: </para>
        <para>
            <orderedlist>
                <listitem>
                    <para>lvm2, which works with a VG called
                            <literal>cinder-volumes</literal> or
                        another named Volume Group (Refer to <link
                            xlink:href="http://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)"
                            >http://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)</link>
                        for further details)</para>
                </listitem>
                <listitem>
                    <para><literal>open-iscsi</literal>, the iSCSI
                        implementation which manages iSCSI sessions on
                        the compute nodes </para>
                </listitem>
            </orderedlist>
        </para>
        <para>Here is what happens from the volume creation to its
            attachment: </para>
        <orderedlist>
            <listitem>
                <para>The volume is created via <command>nova
                        volume-create</command>; which creates an LV
                    into the volume group (VG)
                        <literal>cinder-volumes</literal>
                </para>
            </listitem>
            <listitem>
                <para>The volume is attached to an instance via
                        <command>nova volume-attach</command>; which
                    creates a unique iSCSI IQN that will be exposed to
                    the compute node </para>
            </listitem>
            <listitem>
                <para>The compute node which run the concerned
                    instance has now an active ISCSI session; and a
                    new local storage (usually a
                        <filename>/dev/sdX</filename> disk) </para>
            </listitem>
            <listitem>
                <para>libvirt uses that local storage as a storage for
                    the instance; the instance get a new disk (usually
                    a <filename>/dev/vdX</filename> disk) </para>
            </listitem>
        </orderedlist>
        <para>For this particular walk through, there is one cloud
            controller running <literal>nova-api</literal>,
                <literal>nova-scheduler</literal>,
                <literal>nova-objectstore</literal>,
                <literal>nova-network</literal> and
                <literal>cinder-*</literal> services. There are two
            additional compute nodes running
                <literal>nova-compute</literal>. The walk through uses
            a custom partitioning scheme that carves out 60GB of space
            and labels it as LVM. The network uses
                <literal>FlatManger</literal> is the
                <literal>NetworkManager</literal> setting for
            OpenStack Compute (Nova). </para>
        <para>Please note that the network mode doesn't interfere at
            all with the way cinder works, but networking must be set
            up for cinder to work. Please refer to <link
                linkend="ch_networking">Networking</link> for more
            details.</para>
        <para>To set up Compute to use volumes, ensure that Block
            Storage is installed along with lvm2. The guide will be
            split in four parts : </para>
        <para>
            <itemizedlist>
                <listitem>
                    <para>Installing the Block Storage service on the
                        cloud controller.</para>
                </listitem>
                <listitem>
                    <para>Configuring the
                            <literal>cinder-volumes</literal> volume
                        group on the compute nodes.</para>
                </listitem>
                <listitem>
                    <para>Troubleshooting your installation.</para>
                </listitem>
                <listitem>
                    <para>Backup your nova volumes.</para>
                </listitem>
            </itemizedlist>
        </para>
        <xi:include href="../openstack-install/cinder-install.xml"/>
        <xi:include href="backup-block-storage-disks.xml"/>
    </section>
    <section xml:id="volume-drivers">
        <title>Volume drivers</title>
        <para>The default behaviour can be altered by
            using different volume drivers that are included in the Compute (Nova)
            code base. To set a volume driver, use
                <literal>volume_driver</literal> flag. The default is
            as follows:</para>
        <programlisting>
volume_driver=nova.volume.driver.ISCSIDriver
iscsi_helper=tgtadm
        </programlisting>
        <para>Refer to the <link xlink:href="../openstack-block-storage/admin/content/">OpenStack Block Storage Admin Manual</link> for information about configuring drivers.</para>
    </section>
    <xi:include href="../openstack-install/adding-block-storage.xml" />
    <section xml:id="boot-from-volume">
        <title>Boot From Volume</title>
        <para>In some cases, instances can be stored and run from inside volumes. This is explained in further detail in the <link 
            linkend="boot_from_volume">Boot From Volume</link> 
            section.</para>
    </section>
</chapter>
