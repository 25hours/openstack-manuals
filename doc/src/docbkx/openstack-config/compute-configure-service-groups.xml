<section xml:id="configuring-compute-service-groups"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:ns5="http://www.w3.org/1999/xhtml"
         xmlns:ns4="http://www.w3.org/2000/svg"
         xmlns:ns3="http://www.w3.org/1998/Math/MathML"
         xmlns:ns="http://docbook.org/ns/docbook"
         version="5.0">
  <title>Configuring Compute Service Groups</title>
  <para> To effectively manage and utilize compute nodes, Nova needs
  to know the status of them. For example, when a user launches a
  new VM, the Nova scheduler should send the request to a live node
  (with enough capacity too, of course). From the Grizzly release
  onward, Nova queries the ServiceGroup API to get the node
  liveness information.</para>

  <para>When a compute worker (running the nova-compute daemon) starts,
  it calls the join API to join the compute group, so that every
  service that is interested in the information (e.g. the scheduler)
  will be able to query the group membership, or the status of a
  particular node. Under the hood, the ServiceGroup client driver
  will automatically update the compute worker status.</para>
 
  <para>Currently there are two drivers implemented: database and
  ZooKeeper. Further drivers are in review or development, such as
  memcache.</para>
 
  <section xml:id="database-servicegroup-driver">
   <title>Database ServiceGroup driver</title>
   <para>The database driver is how Compute has been tracking node
   liveness since the first release, and it's the default driver. 
   In a compute worker, it periodically sends a db update command
   to the database, saying "I'm OK" with a timestamp. A pre-defined
   timeout (<literal>service_down_time</literal>) is used to
   determine if a node is dead.</para>

   <para> The driver has two limitations, which may or may not be an
   issue for you, depending on your setup. First, the more compute
   worker nodes you have, the more pressure you put on the database.
   Second, the timeout is by default 60 seconds. So, it might take a
   while to detect node failures. Of course you could try to reduce
   the timeout value, but you would also need to make the DB update
   more frequently, which again increases the DB workload.</para>

   <para>The fundamental issue is, the data to describe whether the
   node is alive is "transient" --- it's basically useless after a
   couple of seconds. Other data in the database, such as the entries
   to describe which users own what VMs, are persistent. However, they
   are treated the same way since they are stored in the same
   database. This is also part of the motivation where for the
   ServiceGroup abstraction so that there is the a chance to treat
   them separately.</para>
  </section>
  <section xml:id="zookeeper-servicegroup-driver">
   <title>ZooKeeper ServiceGroup driver</title>
   <para>The ZooKeeper ServiceGroup driver works by using ZooKeeper
   ephemeral nodes. ZooKeeper in contrary to databases is a
   distributed system, and its load is divided among several servers.
   At a compute worker node, after establishing a ZooKeeper session, 
   it creates an ephemeral znode in the group directory. Ephemeral
   znodes has the same lifespan as the session. If the worker node
   or the nova-compute daemon crashes, or there is a network 
   partition between the worker and the ZooKeeper server quorums,
   then the ephemeral znodes are removed automatically. The driver
   gets the group membership by doing a "ls" in group directory.
   </para>
   <para>To use the ZooKeeper driver, of course you need to have
   both ZooKeeper servers and client libraries installed. Setting
   up ZooKeeper servers is outside the scope of this article. 
   For the rest of the article, let's assume you have them installed,
   and their addresses / ports are 192.168.2.1:2181, 192.168.2.2:2181,
   192.168.2.3:2181.
   </para>
   <para>To use ZooKeeper, you'll need two client-side Python
   libraries on every nova node: <literal>python-zookeeper</literal>
   - the official Zookeeper Python binding
   and <literal>evzookeeper</literal> - the library to make the
   binding work with the eventlet threading model.
   </para>
   <para>
   The relevent configuration snippet in /etc/nova/nova.conf on every node is:
<programlisting>
servicegroup_driver="zk"

[zookeeper]
address="192.168.2.1:2181,192.168.2.2:2181,192.168.2.3:2181"
</programlisting>
   </para>
   <xi:include href="../common/tables/nova-zookeeper.xml"/>
  </section>
 </section>
