<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="assumptions"
    xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
    <title>Installation Assumptions</title>
    <para>OpenStack Compute has a large number of configuration
        options. To simplify this installation guide, we make
        a number of assumptions about the target installation.</para>
        <itemizedlist>
            <listitem><para>You have a collection of compute nodes, each installed with a
                    supported Linux distribution: Fedora 17, Ubuntu
                    Server 12.04, RHEL 6.2, Scientific Linux 6.1 or
                    CentOS 6 + CR distributions (continuous release ( CR ) repository).</para>
            <note><para>Debian, openSUSE, and SLES also have OpenStack support, but are not documented here.</para></note></listitem>
            <listitem><para>You have designated one of the nodes as the Cloud Controller,
                    which will run all of the services (RabbitMQ or
                    Qpid, MySQL, Identity, Image, nova-api,
                    nova-network, nova-scheduler, nova-volume) except
                    for nova-compute.</para></listitem>
            <listitem><para>The disk partitions on your cloud controller are being managed
                    by the <link
                        xlink:href="http://docs.openstack.org/trunk/openstack-compute/admin/content/managing-volumes.html"
                        >Logical Volume Manager</link> (LVM).</para></listitem>
        <listitem>
            <para>Your Cloud Controller has an LVM volume group named
                "nova-volumes" to provide persistent storage to guest
                VMs. Either create this during the installation or
                leave some free space to create it prior to installing
                nova services.</para>
        </listitem>

            <listitem>
                <para>Ensure that the server can resolve its own hostname,
                    otherwise you may have problems if you are using RabbitMQ
                    as the messaging backend (RabbitMQ is the default messaging
                    back-end on Ubuntu, on Fedora the default backend is Qpid).</para>
            </listitem>
            <listitem>
                <para><literal>192.168.206.130</literal> is the primary IP for our host
                    on <literal>eth0</literal>.</para>
            </listitem>
            <listitem>
                <para><literal>192.168.100.0/24</literal> as the fixed range for our
                    guest VMs, connected to the host via <literal>br100</literal>.</para>
            </listitem>
            <listitem>
                <para>FlatDHCP with a single network interface.</para>
            </listitem>
            <listitem>
                <para>KVM or Xen (XenServer or XCP) as the hypervisor.</para>
            </listitem>
            <listitem>
                <para>Ensure the operating system is up-to-date by
                    running <command>yum update</command> or
                        <command>apt-get update</command> and
                        <command>apt-get upgrade</command> prior to
                    the installation.</para>
            </listitem>
            <listitem os="rhel;fedora;centos"><para>On RHEL (and derivatives) enable the EPEL repository by running:</para>
            <screen><prompt>$</prompt> <userinput>sudo rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-5.noarch.rpm</userinput></screen></listitem>
        </itemizedlist>
    <para>This installation process walks through installing a cloud
        controller node and a compute node using a set of packages
        that are known to work with each other. The cloud controller
        node contains all the nova- services including the API server
        and the database server. The compute node needs to run only
        the nova-compute service. You only need one nova-network
        service running in a multi-node install, though if high 
        availability for networks is required, there are additional options. </para>
</chapter>
